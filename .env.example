# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
# NOTE: This .env file is gitignored and won't be pushed to HuggingFace Spaces.
# For HF Spaces deployment, set these as Repository Secrets:
#   Settings → Repository secrets → New secret
# ============================================================================

# ===== LLM BACKEND =====
# Options: 'huggingface' | 'openai' | 'ollama' | 'auto'
# For HF Spaces: set to 'huggingface'
# For local with Ollama: set to 'ollama'
LLM_BACKEND=huggingface

# HuggingFace Inference API (recommended for HF Spaces - FREE!)
# Get token: https://huggingface.co/settings/tokens
HF_TOKEN=your_hf_token_here
HF_LLM_MODEL=HuggingFaceTB/SmolLM2-1.7B-Instruct

# OpenAI (alternative - requires paid API key)
# OPENAI_API_KEY=your_openai_key_here
# OPENAI_MODEL=gpt-4o-mini

# Ollama (local development only - requires running Ollama server)
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# OLLAMA_MODEL=smollm2:360m
# OLLAMA_MODEL_CLOUD=gpt-oss:20b-cloud
# OLLAMA_API_KEY=your_ollama_cloud_key

# ===== EMBEDDINGS =====
# Options: 'fastembed' (local, free) | 'huggingface' | 'openai'
EMBEDDING_BACKEND=fastembed
FASTEMBED_MODEL=BAAI/bge-small-en-v1.5

# ===== DOCUMENT CONFIGURATION =====
# For Docker deployment, documents are uploaded via Gradio UI
# Optional: Default document path (used if no upload provided)
SAMPLE_DOCUMENT_PATH=~/Documents/Books/Atomic_Habits_James_Clear.pdf
SAVE_DIR=./rag_data

# ===== CHUNKING CONFIGURATION =====
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=5

# ===== TOKEN LIMITS FOR LLM =====
TOKEN_LIMIT_BASE=512
TOKEN_LIMIT_PER_SOURCE=200
TOKEN_LIMIT_MAX=2048

# ===== GRADIO UI CONFIGURATION =====
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false
