# ===== HUGGINGFACE SPACES CONFIGURATION =====
# Optimized for HF Spaces deployment (cloud-based, no local Ollama)

# ===== EMBEDDING CONFIGURATION =====
# Use HuggingFace Inference API (free tier available)
EMBEDDING_BACKEND=fastembed
FASTEMBED_MODEL=BAAI/bge-small-en-v1.5

# Optional: For cloud embeddings (requires token)
# EMBEDDING_METHOD=huggingface
# HF_TOKEN=your_token_here
# HF_EMBEDDING_MODEL=BAAI/bge-base-en-v1.5

# ===== LLM CONFIGURATION =====
# Use HuggingFace Inference API for generation (recommended for HF Spaces)
LLM_BACKEND=huggingface  # Options: huggingface, openai, ollama, auto, none
HF_TOKEN=your_hf_token_here
HF_LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# Alternative: Use OpenAI
# LLM_BACKEND=openai
# OPENAI_API_KEY=your_openai_key_here
# OPENAI_MODEL=gpt-4o-mini

# Alternative: Use Ollama (requires local server)
# LLM_BACKEND=ollama
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# OLLAMA_MODEL=smollm2:360m

# ===== DOCUMENT CONFIGURATION =====
SAVE_DIR=./rag_data
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=5

# ===== TOKEN LIMITS FOR LLM =====
TOKEN_LIMIT_BASE=512
TOKEN_LIMIT_PER_SOURCE=200
TOKEN_LIMIT_MAX=2048

# ===== GRADIO UI CONFIGURATION =====
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false

# ===== OPTIONAL: Web Search =====
# Disabled by default on HF Spaces due to dependency issues
ENABLE_WEB_SEARCH=false
