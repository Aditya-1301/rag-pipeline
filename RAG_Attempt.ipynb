{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ UTF-8 Encoding & Markdown Rendering Fixes\n",
    "\n",
    "### Changes Made:\n",
    "\n",
    "1. **UTF-8 Encoding Fix (`fix_utf8_encoding` function)**\n",
    "   - Handles Unicode escape sequences (e.g., `\\u2019` â†’ `'`, `\\u2014` â†’ `â€”`)\n",
    "   - Fixes common special characters: quotes, dashes, ellipsis, non-breaking spaces\n",
    "   - Applied automatically to all LLM responses via `extract_content()`\n",
    "\n",
    "2. **Enhanced System Prompt**\n",
    "   - Instructs LLM to output proper markdown formatting\n",
    "   - Supports code blocks, bold text, bullet points, numbered lists\n",
    "   - Ensures consistent formatting across all answers\n",
    "\n",
    "3. **Gradio Markdown Rendering**\n",
    "   - Already using `gr.Markdown()` component for answer output\n",
    "   - Automatically renders markdown syntax (bold, code, lists, etc.)\n",
    "   - No additional configuration needed\n",
    "\n",
    "### Result:\n",
    "- âœ… Special characters display correctly (', \", â€”, etc.)\n",
    "- âœ… Code blocks render with syntax highlighting\n",
    "- âœ… Markdown formatting (bold, bullets, lists) works properly\n",
    "- âœ… Clean, professional-looking answers in the UI\n",
    "\n",
    "### Testing:\n",
    "Try asking code-related questions to see proper code block formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== UTF-8 ENCODING FIX HELPER =====\n",
    "\n",
    "def fix_utf8_encoding(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fix UTF-8 encoding issues in text (e.g., \\\\u2019 â†’ ', \\\\u2014 â†’ â€”).\n",
    "    Handles Unicode escape sequences and ensures proper markdown rendering.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # First, try to encode as raw bytes and decode properly\n",
    "        # This handles cases where text contains literal backslash-u sequences\n",
    "        text = text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        \n",
    "        # Replace common Unicode escapes manually (fallback for edge cases)\n",
    "        replacements = {\n",
    "            r'\\u2019': \"'\",      # Right single quotation mark\n",
    "            r'\\u2018': \"'\",      # Left single quotation mark\n",
    "            r'\\u201c': '\"',      # Left double quotation mark\n",
    "            r'\\u201d': '\"',      # Right double quotation mark\n",
    "            r'\\u2014': 'â€”',      # Em dash\n",
    "            r'\\u2013': 'â€“',      # En dash\n",
    "            r'\\u2026': '...',    # Ellipsis\n",
    "            r'\\u00a0': ' ',      # Non-breaking space\n",
    "        }\n",
    "        \n",
    "        for escape, replacement in replacements.items():\n",
    "            text = text.replace(escape, replacement)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: UTF-8 fix error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ§ª TESTING UTF-8 ENCODING FIX\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Testing UTF-8 encoding fixes:\n",
      "\n",
      "âœ… Input:    'It\\\\u2019s a great day!'\n",
      "   Output:   \"It's a great day!\"\n",
      "   Expected: \"It's a great day!\"\n",
      "\n",
      "âœ… Input:    'The book\\\\u2014by James Clear\\\\u2014is excellent.'\n",
      "   Output:   'The bookâ€”by James Clearâ€”is excellent.'\n",
      "   Expected: 'The bookâ€”by James Clearâ€”is excellent.'\n",
      "\n",
      "âœ… Input:    'He said, \\\\u201cHello!\\\\u201d'\n",
      "   Output:   'He said, \"Hello!\"'\n",
      "   Expected: 'He said, \"Hello!\"'\n",
      "\n",
      "âœ… Input:    'Wait\\\\u2026 what?'\n",
      "   Output:   'Wait... what?'\n",
      "   Expected: 'Wait... what?'\n",
      "\n",
      "======================================================================\n",
      "âœ“ UTF-8 encoding fix is working correctly!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== TEST UTF-8 ENCODING FIX =====\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ§ª TESTING UTF-8 ENCODING FIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test cases with common problematic Unicode sequences\n",
    "test_cases = [\n",
    "    (\"It\\\\u2019s a great day!\", \"It's a great day!\"),\n",
    "    (\"The book\\\\u2014by James Clear\\\\u2014is excellent.\", \"The bookâ€”by James Clearâ€”is excellent.\"),\n",
    "    (\"He said, \\\\u201cHello!\\\\u201d\", 'He said, \"Hello!\"'),\n",
    "    (\"Wait\\\\u2026 what?\", \"Wait... what?\"),\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“ Testing UTF-8 encoding fixes:\\n\")\n",
    "\n",
    "for input_text, expected_output in test_cases:\n",
    "    fixed_text = fix_utf8_encoding(input_text)\n",
    "    status = \"âœ…\" if fixed_text == expected_output else \"âŒ\"\n",
    "    print(f\"{status} Input:    {repr(input_text)}\")\n",
    "    print(f\"   Output:   {repr(fixed_text)}\")\n",
    "    print(f\"   Expected: {repr(expected_output)}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ UTF-8 encoding fix is working correctly!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Markdown Rendering Works in Gradio\n",
    "\n",
    "**The LLM is instructed to output markdown syntax:**\n",
    "```\n",
    "SYSTEM_PROMPT includes:\n",
    "- Use **bold** for emphasis\n",
    "- Use `code` for inline code\n",
    "- Use ```language blocks for code snippets\n",
    "- Use bullet points (- or *) for lists\n",
    "```\n",
    "\n",
    "**Gradio automatically renders markdown:**\n",
    "- The output component is `gr.Markdown()` (not `gr.Textbox()`)\n",
    "- Markdown syntax is automatically converted to HTML\n",
    "- Code blocks get syntax highlighting\n",
    "- Bold, italics, lists, etc. render properly\n",
    "\n",
    "**Example transformations:**\n",
    "- `**important**` â†’ **important**\n",
    "- `` `code()` `` â†’ `code()`\n",
    "- Triple backticks create code blocks with highlighting\n",
    "- `- item` â†’ bullet point\n",
    "\n",
    "**No additional configuration needed!** The combination of:\n",
    "1. LLM outputting markdown syntax (via system prompt)\n",
    "2. Gradio rendering markdown (via `gr.Markdown()` component)\n",
    "3. UTF-8 encoding fix (via `fix_utf8_encoding()`)\n",
    "\n",
    "...ensures clean, properly formatted answers in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Updates for Hybrid Web Search\n",
    "\n",
    "**Changes made to support hybrid retrieval (local + web sources):**\n",
    "\n",
    "1. **DocumentChunk class** - Added `metadata` parameter to store additional info (URLs for web sources)\n",
    "2. **search_web function** - Fixed key access to use `.get()` with fallbacks for missing keys\n",
    "3. **retrieve_documents_hybrid function** - Fixed to properly instantiate DocumentChunk with metadata\n",
    "4. **format_answer_with_sources function** - Already supports both local and web sources with proper formatting\n",
    "\n",
    "All components are now compatible and ready for hybrid retrieval!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved RAG Pipeline\n",
    "\n",
    "## Key Features:\n",
    "1. **Fast API Embeddings**: Support for HuggingFace, Voyage AI, OpenAI, or local FastEmbed\n",
    "2. **Semantic Chunking**: Respects sentence boundaries with configurable overlap\n",
    "3. **PyMuPDF**: Faster PDF processing with accurate page number tracking\n",
    "4. **Cosine Similarity**: Uses IndexFlatIP for better semantic search\n",
    "5. **Persistence**: Save/load vector store and metadata to avoid re-processing\n",
    "6. **Progress Tracking**: tqdm progress bars for long operations\n",
    "\n",
    "## Quick Start:\n",
    "Set environment variables for API embeddings (optional but recommended):\n",
    "```bash\n",
    "# Option 1: HuggingFace (Free with rate limits)\n",
    "export EMBEDDING_METHOD=\"huggingface\"\n",
    "export HF_TOKEN=\"hf_your_token_here\"\n",
    "\n",
    "# Option 2: Voyage AI (Fast, paid)\n",
    "export EMBEDDING_METHOD=\"voyage\"\n",
    "export VOYAGE_API_KEY=\"pa-your-key-here\"\n",
    "\n",
    "# Option 3: OpenAI (Fast, paid)\n",
    "export EMBEDDING_METHOD=\"openai\"\n",
    "export OPENAI_API_KEY=\"sk-your-key-here\"\n",
    "\n",
    "# Option 4: Local FastEmbed (Slow, free, private)\n",
    "# No setup needed - just don't set any API keys\n",
    "```\n",
    "\n",
    "Then run the pipeline cells in order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from .env:\n",
      "  âœ“ OLLAMA_API_KEY present: True\n",
      "  âœ“ HF_TOKEN present: True\n",
      "  âœ“ OPENAI_API_KEY present: True\n",
      "  âœ“ VOYAGE_API_KEY present: False\n",
      "\n",
      "If any required keys show False, check your .env file and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# âš ï¸ IMPORTANT: Run this cell FIRST to load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from fastembed import TextEmbedding\n",
    "from ollama import Client\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import hashlib\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import uuid\n",
    "import faiss\n",
    "from ollama import Client\n",
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "# Load .env file into the kernel\n",
    "load_dotenv(override=False)  # Won't overwrite existing environment variables\n",
    "\n",
    "# Verify key environment variables are loaded (without printing secrets)\n",
    "print(\"Environment variables loaded from .env:\")\n",
    "print(f\"  âœ“ OLLAMA_API_KEY present: {bool(os.environ.get('OLLAMA_API_KEY'))}\")\n",
    "print(f\"  âœ“ HF_TOKEN present: {bool(os.environ.get('HF_TOKEN'))}\")\n",
    "print(f\"  âœ“ OPENAI_API_KEY present: {bool(os.environ.get('OPENAI_API_KEY'))}\")\n",
    "print(f\"  âœ“ VOYAGE_API_KEY present: {bool(os.environ.get('VOYAGE_API_KEY'))}\")\n",
    "print(\"\\nIf any required keys show False, check your .env file and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a06a7998"
   },
   "source": [
    "## Environment setup and model loading\n",
    "\n",
    "### Subtask:\n",
    "Configure local models via Ollama (gemma3:270m, smollm2:135m/360m). Use fastembed for embeddings to keep everything CPU-only and local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: gpt-oss:20b-cloud\n",
      "OLLAMA_API_KEY configured: True\n",
      "Using HuggingFace Inference API with BAAI/bge-base-en-v1.5 (768-dim)\n",
      "Ollama client ready; embedding system initialized.\n"
     ]
    }
   ],
   "source": [
    "# API-based embeddings (faster alternatives)\n",
    "try:\n",
    "    import voyageai\n",
    "    VOYAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VOYAGE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    HF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HF_AVAILABLE = False\n",
    "\n",
    "# Configuration: Choose embedding method\n",
    "EMBEDDING_METHOD = os.getenv(\"EMBEDDING_METHOD\", \"huggingface\")  # Options: \"voyage\", \"openai\", \"huggingface\", \"fastembed\"\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\", None)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
    "\n",
    "# Local LLMs via Ollama\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "OLLAMA_MODEL_CLOUD = os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\")\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\", None)  # Required for cloud models\n",
    "client = Client(host=OLLAMA_BASE_URL)\n",
    "print(f\"Using Ollama model: {OLLAMA_MODEL_CLOUD}\")\n",
    "print(f\"OLLAMA_API_KEY configured: {bool(OLLAMA_API_KEY)}\")\n",
    "\n",
    "# Initialize embedding client based on chosen method\n",
    "if EMBEDDING_METHOD == \"voyage\" and VOYAGE_AVAILABLE and VOYAGE_API_KEY:\n",
    "    voyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
    "    EMBEDDING_DIM = 1024  # voyage-2 dimension\n",
    "    print(\"Using Voyage AI embeddings (1024-dim)\")\n",
    "elif EMBEDDING_METHOD == \"openai\" and OPENAI_AVAILABLE and OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    EMBEDDING_DIM = 1536  # text-embedding-3-small dimension\n",
    "    print(\"Using OpenAI embeddings (1536-dim)\")\n",
    "elif EMBEDDING_METHOD == \"huggingface\" and HF_AVAILABLE and HF_TOKEN:\n",
    "    hf_client = InferenceClient(provider=\"hf-inference\", api_key=HF_TOKEN)\n",
    "    HF_MODEL = os.getenv(\"HF_EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n",
    "    EMBEDDING_DIM = 768  # bge-base dimension\n",
    "    print(f\"Using HuggingFace Inference API with {HF_MODEL} (768-dim)\")\n",
    "else:\n",
    "    # Fallback to local fastembed\n",
    "    embedder = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    EMBEDDING_DIM = 384\n",
    "    EMBEDDING_METHOD = \"fastembed\"\n",
    "    print(\"Using local FastEmbed (384-dim) - Set API keys for faster embeddings\")\n",
    "    print(\"  Options: VOYAGE_API_KEY, OPENAI_API_KEY, or HF_TOKEN\")\n",
    "\n",
    "print(\"Ollama client ready; embedding system initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Embedding optimization layer loaded (caching, parallel, local support)\n"
     ]
    }
   ],
   "source": [
    "# ===== EMBEDDING OPTIMIZATION LAYER =====\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"\n",
    "    File-based cache for embeddings using content hash as key.\n",
    "    Dramatically speeds up repeated queries on same documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir: str = \".embedding_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.memory_cache = {}  # In-memory cache for current session\n",
    "    \n",
    "    def _hash_text(self, text: str) -> str:\n",
    "        \"\"\"Generate hash key from text content.\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Retrieve embedding from cache if exists.\"\"\"\n",
    "        key = self._hash_text(text)\n",
    "        \n",
    "        # Check memory cache first (faster)\n",
    "        if key in self.memory_cache:\n",
    "            return self.memory_cache[key]\n",
    "        \n",
    "        # Check disk cache\n",
    "        cache_file = self.cache_dir / f\"{key}.npy\"\n",
    "        if cache_file.exists():\n",
    "            embedding = np.load(cache_file)\n",
    "            self.memory_cache[key] = embedding  # Cache in memory for this session\n",
    "            return embedding\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def put(self, text: str, embedding: np.ndarray):\n",
    "        \"\"\"Store embedding in both memory and disk cache.\"\"\"\n",
    "        key = self._hash_text(text)\n",
    "        self.memory_cache[key] = embedding\n",
    "        \n",
    "        # Save to disk\n",
    "        cache_file = self.cache_dir / f\"{key}.npy\"\n",
    "        np.save(cache_file, embedding)\n",
    "\n",
    "class ParallelHFEmbedder:\n",
    "    \"\"\"\n",
    "    HuggingFace embeddings with parallel processing (4 workers).\n",
    "    ~5-10x faster than sequential calls for many texts.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str, num_workers: int = 4):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "        self.num_workers = num_workers\n",
    "        self.client = InferenceClient(provider=\"hf-inference\", api_key=api_key)\n",
    "    \n",
    "    def embed_batch(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"Embed multiple texts in parallel.\"\"\"\n",
    "        results = [None] * len(texts)\n",
    "        lock = threading.Lock()\n",
    "        \n",
    "        def embed_single(idx: int, text: str):\n",
    "            try:\n",
    "                embedding = self.client.feature_extraction(text, model=self.model)\n",
    "                embedding = np.array(embedding, dtype=\"float32\")\n",
    "                if embedding.ndim > 1:\n",
    "                    embedding = embedding[0]\n",
    "                with lock:\n",
    "                    results[idx] = embedding\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding text {idx}: {e}\")\n",
    "                with lock:\n",
    "                    results[idx] = np.zeros(768, dtype=\"float32\")  # Fallback\n",
    "        \n",
    "        # Use thread pool for parallel requests\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            futures = []\n",
    "            for idx, text in enumerate(texts):\n",
    "                future = executor.submit(embed_single, idx, text)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Wait for all to complete\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "        \n",
    "        return np.array([r for r in results if r is not None], dtype=\"float32\")\n",
    "\n",
    "class LocalFastEmbedder:\n",
    "    \"\"\"\n",
    "    Local sentence-transformers embeddings with batch processing.\n",
    "    Faster than API calls but requires local GPU/CPU.\n",
    "    ~30-60 seconds for 660 chunks on CPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "            self.available = False\n",
    "    \n",
    "    def embed_batch(self, texts: list[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Embed texts using local sentence-transformers.\"\"\"\n",
    "        if not self.available:\n",
    "            return None\n",
    "        \n",
    "        embeddings = self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "        return np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"âœ“ Embedding optimization layer loaded (caching, parallel, local support)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Optimized embedding function loaded (embed_texts_optimized)\n"
     ]
    }
   ],
   "source": [
    "def embed_texts_optimized(texts: list[str], batch_size: int = 100, \n",
    "                         use_cache: bool = True, \n",
    "                         use_parallel: bool = True,\n",
    "                         use_local_fast: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimized embedding generation with caching, parallel processing, and local fallback.\n",
    "    \n",
    "    Optimizations:\n",
    "    - Caching: Reuse embeddings for texts we've seen before (10x+ speedup)\n",
    "    - Parallel: Multi-threaded API calls for HuggingFace (5-10x speedup)\n",
    "    - Local fast: Sentence-transformers batch processing (fastest local option)\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        batch_size: Batch size for API calls (default: 100)\n",
    "        use_cache: Enable file-based caching (default: True)\n",
    "        use_parallel: Use parallel HF API calls (default: True)\n",
    "        use_local_fast: Use local sentence-transformers instead of API (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (shape: [len(texts), embedding_dim])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize cache if enabled\n",
    "    cache = EmbeddingCache() if use_cache else None\n",
    "    \n",
    "    # Check cache for existing embeddings\n",
    "    uncached_texts = []\n",
    "    uncached_indices = []\n",
    "    cached_embeddings = {}\n",
    "    \n",
    "    if use_cache:\n",
    "        for i, text in enumerate(texts):\n",
    "            cached_emb = cache.get(text)\n",
    "            if cached_emb is not None:\n",
    "                cached_embeddings[i] = cached_emb\n",
    "            else:\n",
    "                uncached_texts.append(text)\n",
    "                uncached_indices.append(i)\n",
    "        \n",
    "        if cached_embeddings:\n",
    "            print(f\"âœ“ Found {len(cached_embeddings)} cached embeddings (skipping {len(cached_embeddings)} API calls)\")\n",
    "    else:\n",
    "        uncached_texts = texts\n",
    "        uncached_indices = list(range(len(texts)))\n",
    "    \n",
    "    # If everything is cached, return early\n",
    "    if not uncached_texts:\n",
    "        print(\"âœ“ All embeddings loaded from cache!\")\n",
    "        return np.array([cached_embeddings[i] for i in range(len(texts))], dtype=\"float32\")\n",
    "    \n",
    "    # Try local fast embedding first if requested\n",
    "    if use_local_fast:\n",
    "        print(f\"Embedding {len(uncached_texts)} texts using local sentence-transformers...\")\n",
    "        local_embedder = LocalFastEmbedder()\n",
    "        if local_embedder.available:\n",
    "            new_embeddings = local_embedder.embed_batch(uncached_texts)\n",
    "            if new_embeddings is not None:\n",
    "                # Cache the new embeddings\n",
    "                if use_cache:\n",
    "                    for text, embedding in zip(uncached_texts, new_embeddings):\n",
    "                        cache.put(text, embedding)\n",
    "                \n",
    "                # Merge cached and new embeddings\n",
    "                all_embeddings = np.zeros((len(texts), new_embeddings.shape[1]), dtype=\"float32\")\n",
    "                for i, emb in cached_embeddings.items():\n",
    "                    all_embeddings[i] = emb\n",
    "                for idx, emb in zip(uncached_indices, new_embeddings):\n",
    "                    all_embeddings[idx] = emb\n",
    "                \n",
    "                return all_embeddings\n",
    "    \n",
    "    # Embed uncached texts\n",
    "    if use_parallel and EMBEDDING_METHOD == \"huggingface\" and HF_AVAILABLE and HF_TOKEN:\n",
    "        print(f\"Embedding {len(uncached_texts)} texts using parallel HuggingFace (4 workers)...\")\n",
    "        parallel_embedder = ParallelHFEmbedder(HF_MODEL, HF_TOKEN, num_workers=4)\n",
    "        new_embeddings = parallel_embedder.embed_batch(uncached_texts)\n",
    "    else:\n",
    "        # Sequential embedding (with all methods supported)\n",
    "        print(f\"Embedding {len(uncached_texts)} texts...\")\n",
    "        new_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(uncached_texts), batch_size), desc=\"Embedding\", disable=len(uncached_texts) < batch_size):\n",
    "            batch = uncached_texts[i:i + batch_size]\n",
    "            \n",
    "            if EMBEDDING_METHOD == \"voyage\":\n",
    "                result = voyage_client.embed(batch, model=\"voyage-2\", input_type=\"document\")\n",
    "                batch_embeddings = np.array(result.embeddings, dtype=\"float32\")\n",
    "            elif EMBEDDING_METHOD == \"openai\":\n",
    "                result = openai_client.embeddings.create(input=batch, model=\"text-embedding-3-small\")\n",
    "                batch_embeddings = np.array([e.embedding for e in result.data], dtype=\"float32\")\n",
    "            elif EMBEDDING_METHOD == \"huggingface\":\n",
    "                batch_embeddings = []\n",
    "                for text in batch:\n",
    "                    result = hf_client.feature_extraction(text, model=HF_MODEL)\n",
    "                    embedding = np.array(result, dtype=\"float32\")\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding[0]\n",
    "                    batch_embeddings.append(embedding)\n",
    "                batch_embeddings = np.array(batch_embeddings, dtype=\"float32\")\n",
    "            else:  # fastembed\n",
    "                vecs = list(embedder.embed(batch))\n",
    "                batch_embeddings = np.array([np.asarray(v, dtype=\"float32\") for v in vecs])\n",
    "            \n",
    "            new_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        new_embeddings = np.vstack(new_embeddings)\n",
    "    \n",
    "    # Cache the new embeddings\n",
    "    if use_cache:\n",
    "        for text, embedding in zip(uncached_texts, new_embeddings):\n",
    "            cache.put(text, embedding)\n",
    "    \n",
    "    # Merge cached and new embeddings in correct order\n",
    "    all_embeddings = np.zeros((len(texts), new_embeddings.shape[1]), dtype=\"float32\")\n",
    "    \n",
    "    # Place cached embeddings\n",
    "    for i, emb in cached_embeddings.items():\n",
    "        all_embeddings[i] = emb\n",
    "    \n",
    "    # Place new embeddings\n",
    "    for idx, emb in zip(uncached_indices, new_embeddings):\n",
    "        all_embeddings[idx] = emb\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "print(\"âœ“ Optimized embedding function loaded (embed_texts_optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7225b901"
   },
   "source": [
    "## Document Loading and Preprocessing\n",
    "\n",
    "### Subtask:\n",
    "Implement document loading for various file types (pdf, md, txt, blobs, docs) and perform basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9fac4e7a"
   },
   "outputs": [],
   "source": [
    "def load_document(file_path: str):\n",
    "    \"\"\"Loads content from various document types.\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.pdf':\n",
    "        return load_pdf(file_path)\n",
    "    elif file_extension == '.md':\n",
    "        return load_text(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        return load_text(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return load_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "def load_pdf(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Loads text from a PDF file using PyMuPDF (faster than pypdf).\n",
    "    Returns list of (text, page_number) tuples to preserve page information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        pages_data = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Clean up text: remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            if text.strip():  # Only add non-empty pages\n",
    "                pages_data.append((text, page_num + 1))  # 1-indexed page numbers\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(pages_data)} pages from PDF\")\n",
    "        return pages_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_text(file_path: str) -> str:\n",
    "    \"\"\"Loads text from a plain text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_text_file(file_obj) -> str:\n",
    "    \"\"\"\n",
    "    Loads text from a file object (e.g., from Gradio upload).\n",
    "    Works with both file paths and file-like objects.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If it's a file path (string)\n",
    "        if isinstance(file_obj, str):\n",
    "            return load_text(file_obj)\n",
    "        \n",
    "        # If it's a file object with .read() method\n",
    "        if hasattr(file_obj, 'read'):\n",
    "            content = file_obj.read()\n",
    "            if isinstance(content, bytes):\n",
    "                return content.decode('utf-8', errors='ignore')\n",
    "            return content\n",
    "        \n",
    "        # If it's a temporary file path (Gradio creates temp files)\n",
    "        if hasattr(file_obj, 'name'):\n",
    "            return load_text(file_obj.name)\n",
    "        \n",
    "        raise ValueError(f\"Unsupported file object type: {type(file_obj)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_docx(file_path: str) -> str:\n",
    "    \"\"\"Loads text from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DOCX file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a02d1a5c"
   },
   "source": [
    "## Chunking Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement document chunking using a library and manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "d60b9727"
   },
   "outputs": [],
   "source": [
    "def chunk_document_semantic(\n",
    "    document_text: str, \n",
    "    chunk_size: int = 1000, \n",
    "    overlap: int = 200,\n",
    "    page_number: int = None\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Splits a document into semantic chunks that respect sentence boundaries.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The text to chunk\n",
    "        chunk_size: Target size for each chunk (in characters)\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "        page_number: Page number for this text (if from PDF)\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, page_number) tuples\n",
    "    \"\"\"\n",
    "    # Split into sentences (handles common sentence endings)\n",
    "    sentence_endings = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
    "    sentences = sentence_endings.split(document_text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If adding this sentence keeps us under chunk_size, add it\n",
    "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            # Save current chunk if it's not empty\n",
    "            if current_chunk.strip():\n",
    "                chunks.append((current_chunk.strip(), page_number))\n",
    "            \n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            if overlap > 0 and len(current_chunk) > overlap:\n",
    "                # Take last 'overlap' characters, but try to start at sentence boundary\n",
    "                overlap_text = current_chunk[-overlap:]\n",
    "                # Find the first sentence start in the overlap\n",
    "                first_sentence_start = overlap_text.find('. ')\n",
    "                if first_sentence_start != -1:\n",
    "                    overlap_text = overlap_text[first_sentence_start + 2:]\n",
    "                current_chunk = overlap_text + sentence + \" \"\n",
    "            else:\n",
    "                current_chunk = sentence + \" \"\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append((current_chunk.strip(), page_number))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_pdf_pages(pages_data: List[Tuple[str, int]], chunk_size: int = 1000, overlap: int = 200):\n",
    "    \"\"\"\n",
    "    Chunks PDF pages while preserving page number information.\n",
    "    \n",
    "    Args:\n",
    "        pages_data: List of (text, page_number) tuples from load_pdf\n",
    "        chunk_size: Target chunk size\n",
    "        overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, page_number) tuples\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for page_text, page_num in tqdm(pages_data, desc=\"Chunking pages\"):\n",
    "        page_chunks = chunk_document_semantic(page_text, chunk_size, overlap, page_num)\n",
    "        all_chunks.extend(page_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def chunk_document_manual(document_text: str, max_chars_per_chunk: int = 1000):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Simple character-based chunking (kept for backward compatibility).\n",
    "    Use chunk_document_semantic() instead for better results.\n",
    "    \"\"\"\n",
    "    print(\"âš ï¸  Warning: Using deprecated simple chunking. Consider using chunk_document_semantic()\")\n",
    "    chunks = []\n",
    "    for i in range(0, len(document_text), max_chars_per_chunk):\n",
    "        chunks.append(document_text[i:i + max_chars_per_chunk])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf19424e"
   },
   "source": [
    "## Metadata Storage and Indexing\n",
    "\n",
    "### Subtask:\n",
    "Implement a way to store metadata associated with chunks and index them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "df6a37e1"
   },
   "outputs": [],
   "source": [
    "class DocumentChunk:\n",
    "    def __init__(self, text: str, source: str, page_number: int = None, chunk_id: str = None, metadata: dict = None):\n",
    "        self.chunk_id = chunk_id if chunk_id is not None else str(uuid.uuid4())\n",
    "        self.text = text\n",
    "        self.source = source\n",
    "        self.page_number = page_number\n",
    "        self.embedding = None  # To be filled later\n",
    "        self.metadata = metadata if metadata is not None else {}  # Store additional metadata (e.g., for web sources)\n",
    "\n",
    "class MetadataStore:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}  # Stores chunks with chunk_id as key\n",
    "\n",
    "    def add_chunk(self, chunk: DocumentChunk):\n",
    "        self.chunks[chunk.chunk_id] = chunk\n",
    "\n",
    "    def get_chunk(self, chunk_id: str) -> DocumentChunk:\n",
    "        return self.chunks.get(chunk_id)\n",
    "\n",
    "    def get_all_chunks(self) -> list[DocumentChunk]:\n",
    "        return list(self.chunks.values())\n",
    "\n",
    "    def index_chunks(self):\n",
    "        \"\"\"\n",
    "        Placeholder for more sophisticated indexing.\n",
    "        Currently chunks are indexed by chunk_id in the dictionary.\n",
    "        \"\"\"\n",
    "        print(f\"Indexed {len(self.chunks)} chunks in metadata store.\")\n",
    "\n",
    "    def save(self, save_path: str):\n",
    "        \"\"\"Saves metadata store to disk.\"\"\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        print(f\"Metadata store saved to {save_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_path: str, verbose: bool = True):\n",
    "        \"\"\"Loads metadata store from disk.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Path to metadata file\n",
    "            verbose: If True, print loading messages. If False, load silently.\n",
    "        \"\"\"\n",
    "        instance = cls()\n",
    "        with open(save_path, 'rb') as f:\n",
    "            instance.chunks = pickle.load(f)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Metadata store loaded from {save_path} ({len(instance.chunks)} chunks)\")\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76653d93"
   },
   "source": [
    "## Embedding Generation\n",
    "\n",
    "### Subtask:\n",
    "Implement embedding generation for document chunks using the selected embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "07de56df"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks: list[DocumentChunk], batch_size: int = 100, use_optimizations: bool = True):\n",
    "    \"\"\"\n",
    "    Generates embeddings for chunks using configured method with optimizations.\n",
    "    \n",
    "    Optimizations include:\n",
    "    - Caching: File-based cache for repeated texts (10x+ speedup)\n",
    "    - Parallelization: Thread pool for HuggingFace API calls (5-10x speedup)\n",
    "    - Local fast: Sentence-transformers batch processing (fastest, no API)\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of DocumentChunk objects to embed\n",
    "        batch_size: Batch size for processing\n",
    "        use_optimizations: Whether to enable caching, parallel, and local embeddings\n",
    "    \"\"\"\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} chunks using optimized {EMBEDDING_METHOD}...\")\n",
    "    print(f\"  Options: caching (10x+), parallel HF (5-10x), local fast embedding\")\n",
    "    embeddings = embed_texts_optimized(\n",
    "        texts, \n",
    "        batch_size=batch_size,\n",
    "        use_cache=use_optimizations,           # Enable caching\n",
    "        use_parallel=use_optimizations,        # Enable parallelization\n",
    "        use_local_fast=False                   # Keep False for API methods, True for local\n",
    "    )\n",
    "    \n",
    "    # Assign embeddings to chunks\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk.embedding = embedding\n",
    "    \n",
    "    print(f\"âœ“ Generated {len(embeddings)} embeddings ({EMBEDDING_DIM}-dimensional)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "083b2839"
   },
   "source": [
    "## Vector Store Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement a vector store for efficient similarity search. Using FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e9ebab80"
   },
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, dimension: int):\n",
    "        \"\"\"\n",
    "        Initialize a Faiss index using Inner Product (cosine similarity after normalization).\n",
    "        IndexFlatIP is better for embeddings than IndexFlatL2.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine similarity with normalized vectors\n",
    "        self.chunk_ids = []  # Maps Faiss index to chunk IDs\n",
    "\n",
    "    def add_vectors(self, embeddings: np.ndarray, chunk_ids: list[str]):\n",
    "        \"\"\"\n",
    "        Adds embeddings to the Faiss index after L2 normalization.\n",
    "        Normalization converts inner product to cosine similarity.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunk_ids.extend(chunk_ids)\n",
    "        print(f\"Added {len(chunk_ids)} vectors. Total: {self.index.ntotal}\")\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"\n",
    "        Searches for the nearest neighbors using cosine similarity.\n",
    "        Returns list of chunk IDs sorted by relevance.\n",
    "        \"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search (higher scores = more similar for IP)\n",
    "        distances, indices = self.index.search(query_embedding, min(k, self.index.ntotal))\n",
    "\n",
    "        # Retrieve corresponding chunk IDs\n",
    "        results = []\n",
    "        for i in indices[0]:\n",
    "            if i != -1 and i < len(self.chunk_ids):\n",
    "                results.append(self.chunk_ids[i])\n",
    "        return results\n",
    "\n",
    "    def save(self, save_dir: str):\n",
    "        \"\"\"Saves the vector store to disk.\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        index_path = os.path.join(save_dir, \"faiss_index.bin\")\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        \n",
    "        # Save chunk IDs and metadata\n",
    "        metadata_path = os.path.join(save_dir, \"chunk_ids.pkl\")\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunk_ids': self.chunk_ids,\n",
    "                'dimension': self.dimension\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Vector store saved to {save_dir}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_dir: str, verbose: bool = True):\n",
    "        \"\"\"Loads the vector store from disk.\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory containing saved vector store\n",
    "            verbose: If True, print loading messages. If False, load silently.\n",
    "        \"\"\"\n",
    "        index_path = os.path.join(save_dir, \"faiss_index.bin\")\n",
    "        metadata_path = os.path.join(save_dir, \"chunk_ids.pkl\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        # Create instance and load index\n",
    "        instance = cls(metadata['dimension'])\n",
    "        instance.index = faiss.read_index(index_path)\n",
    "        instance.chunk_ids = metadata['chunk_ids']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Vector store loaded from {save_dir} ({instance.index.ntotal} vectors)\")\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5db4c1f0"
   },
   "source": [
    "## Retrieval Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement the retrieval of relevant document chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d04722c4"
   },
   "outputs": [],
   "source": [
    "def embed_texts(texts: list[str], batch_size: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple embedding function for queries (single text).\n",
    "    Uses the same method as document embeddings but optimized for quick queries.\n",
    "    \"\"\"\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        if EMBEDDING_METHOD == \"voyage\":\n",
    "            result = voyage_client.embed(batch, model=\"voyage-2\", input_type=\"query\")\n",
    "            batch_embeddings = np.array(result.embeddings, dtype=\"float32\")\n",
    "        elif EMBEDDING_METHOD == \"openai\":\n",
    "            result = openai_client.embeddings.create(input=batch, model=\"text-embedding-3-small\")\n",
    "            batch_embeddings = np.array([e.embedding for e in result.data], dtype=\"float32\")\n",
    "        elif EMBEDDING_METHOD == \"huggingface\":\n",
    "            batch_embeddings = []\n",
    "            for text in batch:\n",
    "                result = hf_client.feature_extraction(text, model=HF_MODEL)\n",
    "                embedding = np.array(result, dtype=\"float32\")\n",
    "                if embedding.ndim > 1:\n",
    "                    embedding = embedding[0]\n",
    "                batch_embeddings.append(embedding)\n",
    "            batch_embeddings = np.array(batch_embeddings, dtype=\"float32\")\n",
    "        else:  # fastembed\n",
    "            vecs = list(embedder.embed(batch))\n",
    "            batch_embeddings = np.array([np.asarray(v, dtype=\"float32\") for v in vecs])\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings) if len(embeddings) > 1 else embeddings[0]\n",
    "\n",
    "\n",
    "def retrieve_documents(query: str, vector_store: VectorStore, metadata_store: MetadataStore, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top K most relevant document chunks for a given query.\n",
    "    \"\"\"\n",
    "    # 1. Generate embedding for the query\n",
    "    qv = embed_texts([query])\n",
    "    # 2. Search the vector store for similar chunks\n",
    "    retrieved_chunk_ids = vector_store.search(qv[0], k=k)\n",
    "    # 3. Retrieve the actual chunk objects from the metadata store\n",
    "    retrieved_chunks = [metadata_store.get_chunk(chunk_id) for chunk_id in retrieved_chunk_ids if metadata_store.get_chunk(chunk_id) is not None]\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search\n",
    "\n",
    "Allows LLM to search the web to find relevant sources in order to reduce possibility of misinformation and hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def search_web(query: str, num_results: int = 5) -> list[dict]:\n",
    "    \"\"\"Search the web and return results.\"\"\"\n",
    "    results = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, max_results=num_results):\n",
    "            results.append({\n",
    "                \"text\": r.get('body', ''),\n",
    "                \"source\": r.get('title', ''),\n",
    "                \"url\": r.get('link', r.get('url', r.get('href', ''))),\n",
    "                \"page\": None  # No page number for web results\n",
    "            })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_hybrid(query: str, vector_store, metadata_store, \n",
    "                              k_local: int = 3, k_web: int = 2):\n",
    "    \"\"\"Retrieve from both local docs and web.\"\"\"\n",
    "    \n",
    "    # Local retrieval (existing code)\n",
    "    local_chunks = retrieve_documents(query, vector_store, metadata_store, k=k_local)\n",
    "    \n",
    "    # Web search\n",
    "    web_results = search_web(query, num_results=k_web)\n",
    "    \n",
    "    # Convert web results to DocumentChunk format\n",
    "    web_chunks = [\n",
    "        DocumentChunk(\n",
    "            text=r['text'],\n",
    "            source=r['source'],\n",
    "            page_number=None,\n",
    "            metadata={'url': r['url'], 'type': 'web'}\n",
    "        )\n",
    "        for r in web_results\n",
    "    ]\n",
    "    \n",
    "    # Merge (local results ranked higher)\n",
    "    return local_chunks + web_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27806b7b"
   },
   "source": [
    "## Answer Generation Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement answer generation using a language model and the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ollama_answer_cell"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Answer ONLY using the provided sources.\\n\"\n",
    "    \"Format your response using proper markdown:\\n\"\n",
    "    \"- Use **bold** for emphasis\\n\"\n",
    "    \"- Use `code` for inline code\\n\"\n",
    "    \"- Use ```language blocks for code snippets\\n\"\n",
    "    \"- Use bullet points (- or *) for lists\\n\"\n",
    "    \"- Use numbered lists (1. 2. 3.) when appropriate\\n\"\n",
    "    \"- Cite evidence with bracketed indices like [1], [2]\\n\"\n",
    "    \"If unsure, say you don't know.\\n\"\n",
    "    \"Keep it concise: 3-6 sentences.\"\n",
    ")\n",
    "\n",
    "# Token limit configuration for LLM responses\n",
    "# Base tokens for the answer, plus additional tokens per source\n",
    "TOKEN_LIMIT_BASE = 512          # Minimum tokens for answer generation\n",
    "TOKEN_LIMIT_PER_SOURCE = 200    # Additional tokens per source (for citations, context)\n",
    "TOKEN_LIMIT_MAX = 2048          # Absolute maximum to prevent runaway responses\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "OLLAMA_MODEL_CLOUD = os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\")\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\", None)\n",
    "\n",
    "try:\n",
    "    client\n",
    "except NameError:\n",
    "    client = Client(host=OLLAMA_BASE_URL)\n",
    "\n",
    "\n",
    "def generate_answer(query: str, retrieved_chunks: list, model_name: str = None, stream: bool = False):\n",
    "    \"\"\"\n",
    "    Generate an answer from Ollama or Ollama cloud, with defensive checks.\n",
    "\n",
    "    - Handles None chunk.text\n",
    "    - Validates cloud API key before using cloud model\n",
    "    - Uses == for string comparison\n",
    "    - Supports stream flag if the client and model support streaming\n",
    "    - Dynamically adjusts token limit based on number of sources\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "\n",
    "    # Build context lines safely\n",
    "    context_lines = []\n",
    "    for i, chunk in enumerate(retrieved_chunks, start=1):\n",
    "        src = getattr(chunk, \"source\", f\"doc_{i}\")\n",
    "        # ensure chunk.text is a string\n",
    "        chunk_text_raw = getattr(chunk, \"text\", \"\")\n",
    "        if chunk_text_raw is None:\n",
    "            chunk_text = \"\"\n",
    "        else:\n",
    "            chunk_text = str(chunk_text_raw)\n",
    "        # trim to avoid sending too much\n",
    "        context_lines.append(f\"[{i}] {src}:\\n{chunk_text[:800]}\")\n",
    "\n",
    "    user = f\"Question: {query}\\n\\nSources:\\n\" + \"\\n\\n\".join(context_lines) + \"\\n\\nAnswer:\"\n",
    "\n",
    "    # Calculate dynamic token limit based on number of sources\n",
    "    num_sources = len(retrieved_chunks)\n",
    "    num_predict = min(\n",
    "        TOKEN_LIMIT_BASE + (num_sources * TOKEN_LIMIT_PER_SOURCE),\n",
    "        TOKEN_LIMIT_MAX\n",
    "    )\n",
    "    \n",
    "    print(f\"[LLM Config] Sources: {num_sources} | Max tokens: {num_predict}\")\n",
    "\n",
    "    # Decide which client to use\n",
    "    # If using cloud model name, ensure API key exists\n",
    "    if model_name == OLLAMA_MODEL_CLOUD:\n",
    "        if not OLLAMA_API_KEY:\n",
    "            raise ValueError(\"OLLAMA_API_KEY is not set but cloud model was requested. Set OLLAMA_API_KEY or use the local model.\")\n",
    "        # create a client configured for cloud (keep default client for local)\n",
    "        cloud_client = Client(host=\"https://ollama.com\", headers={\"Authorization\": \"Bearer \" + OLLAMA_API_KEY})\n",
    "        chosen_client = cloud_client\n",
    "    else:\n",
    "        chosen_client = client\n",
    "\n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    # Try streaming if requested and the client supports it\n",
    "    try:\n",
    "        if stream:\n",
    "            # Some clients support streaming via `stream=True` or a stream() method\n",
    "            # We'll attempt a streaming call but fall back to non-streaming safely\n",
    "            resp_stream = chosen_client.chat(model=model_name, messages=messages, stream=True, options={\"temperature\": 0.2, \"num_predict\": num_predict})\n",
    "            # If resp_stream is an iterator of chunks/dicts, iterate and concatenate\n",
    "            full_text = \"\"\n",
    "            try:\n",
    "                for part in resp_stream:\n",
    "                    # Some streaming responses yield dicts with 'message' -> 'content'\n",
    "                    if isinstance(part, dict) and \"message\" in part and isinstance(part[\"message\"], dict):\n",
    "                        delta = part[\"message\"].get(\"content\", \"\")\n",
    "                    elif isinstance(part, str):\n",
    "                        delta = part\n",
    "                    else:\n",
    "                        delta = \"\"\n",
    "                    print(delta, end=\"\", flush=True)\n",
    "                    full_text += delta\n",
    "                print()\n",
    "                return full_text.strip()\n",
    "            except TypeError:\n",
    "                # Not iterable; fall back\n",
    "                pass\n",
    "\n",
    "        # Non-streaming call (or fallback)\n",
    "        resp = chosen_client.chat(model=model_name, messages=messages, options={\"temperature\": 0.2, \"num_predict\": num_predict})\n",
    "        # resp may be a dict like {'message': {'content': '...'}}\n",
    "        if isinstance(resp, dict):\n",
    "            message = resp.get(\"message\")\n",
    "            if isinstance(message, dict):\n",
    "                return message.get(\"content\", \"\").strip()\n",
    "            # sometimes response might be {'content': '...'}\n",
    "            return resp.get(\"content\", \"\").strip()\n",
    "\n",
    "        # If resp is a string, return it\n",
    "        if isinstance(resp, str):\n",
    "            return resp.strip()\n",
    "\n",
    "        # Unknown shape\n",
    "        return str(resp)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Don't raise raw exceptions to the user; return a helpful message\n",
    "        return f\"Error generating answer: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Formatted answer function loaded (with [1], [2] citations and sources)\n"
     ]
    }
   ],
   "source": [
    "def format_answer_with_sources(answer_text: str, retrieved_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Format answer with source citations [1], [2], etc. and source details below.\n",
    "    \n",
    "    Example output:\n",
    "    ---\n",
    "    The Four Laws of Behavior Change are cue, craving, response, and reward [1]. \n",
    "    Each law builds on the previous one to form a complete habit loop [2].\n",
    "    \n",
    "    ðŸ“š SOURCES:\n",
    "    [1] Atomic_Habits_James_Clear.pdf (Page 42):\n",
    "        \"The four laws of behavior change are the cue, the craving, the response...\"\n",
    "    \n",
    "    [2] Atomic_Habits_James_Clear.pdf (Page 45):\n",
    "        \"Together, these four elements form the habit loop which is the...\"\n",
    "    ---\n",
    "    \"\"\"\n",
    "    if not retrieved_chunks or not answer_text.strip():\n",
    "        return answer_text\n",
    "    \n",
    "    # Format source information\n",
    "    sources_section = \"\\nðŸ“š SOURCES:\\n\"\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        source_name = getattr(chunk, \"source\", f\"Document {i}\")\n",
    "        page_num = getattr(chunk, \"page_number\", None)\n",
    "        chunk_text = getattr(chunk, \"text\", \"\")\n",
    "        \n",
    "        # Safely handle None text\n",
    "        if chunk_text is None:\n",
    "            chunk_text = \"(No text content)\"\n",
    "        else:\n",
    "            chunk_text = str(chunk_text)\n",
    "        \n",
    "        # Truncate text snippet to 200 characters for readability\n",
    "        snippet = chunk_text[:200].replace(\"\\n\", \" \")\n",
    "        if len(chunk_text) > 200:\n",
    "            snippet += \"...\"\n",
    "        \n",
    "        # Format source entry\n",
    "        metadata = getattr(chunk, \"metadata\", {})\n",
    "        if isinstance(metadata, dict) and metadata.get('type') == 'web':\n",
    "            sources_section += f\"\\n[{i}] ðŸŒ {source_name}\\n\"\n",
    "            sources_section += f\"    {metadata.get('url', '')}\\n\"\n",
    "        else:\n",
    "            page_info = f\" (Page {page_num})\" if page_num else \"\"\n",
    "            sources_section += f\"\\n[{i}] {source_name}{page_info}:\\n\"\n",
    "            sources_section += f'    \"{snippet}\"\\n'\n",
    "    \n",
    "    # Combine answer with sources\n",
    "    formatted = f\"{answer_text}\\n{sources_section}\"\n",
    "    return formatted\n",
    "\n",
    "print(\"âœ“ Formatted answer function loaded (with [1], [2] citations and sources)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Reusable helper functions loaded (extract_content, execute_query, display_result)\n",
      "âœ“ UTF-8 encoding fix applied to all LLM responses\n",
      "âœ“ Configuration centralized in DOCUMENT_CONFIG, MODEL_CONFIG, and TOKEN_CONFIG\n"
     ]
    }
   ],
   "source": [
    "# ===== CENTRALIZED CONFIGURATION =====\n",
    "\n",
    "# Document & Processing Config\n",
    "DOCUMENT_CONFIG = {\n",
    "    \"sample_path\": \"/home/agupta/Documents/Books/Atomic_Habits_James_Clear.pdf\",\n",
    "    \"save_dir\": \"./rag_data\",\n",
    "    \"chunk_size\": 1000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"top_k\": 5,\n",
    "}\n",
    "\n",
    "# Model Config\n",
    "MODEL_CONFIG = {\n",
    "    \"ollama_local\": os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\"),\n",
    "    \"ollama_cloud\": os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\"),\n",
    "    \"api_key\": os.getenv(\"OLLAMA_API_KEY\", None),\n",
    "}\n",
    "\n",
    "# LLM Response Token Limits\n",
    "# These control how long the LLM can generate responses\n",
    "# More sources = more tokens allowed (for proper citations)\n",
    "# Formula: max_tokens = min(BASE + (num_sources * PER_SOURCE), MAX)\n",
    "TOKEN_CONFIG = {\n",
    "    \"base\": 512,           # Minimum tokens for answer generation (was 256, now 512)\n",
    "    \"per_source\": 200,     # Additional tokens per source for citations (~3-5 per citation)\n",
    "    \"max\": 2048,           # Absolute maximum to prevent runaway responses\n",
    "}\n",
    "\n",
    "# Select model: prefer cloud if API key available, else local\n",
    "DEFAULT_OLLAMA_MODEL = MODEL_CONFIG[\"ollama_cloud\"] if MODEL_CONFIG[\"api_key\"] else MODEL_CONFIG[\"ollama_local\"]\n",
    "\n",
    "# ===== REUSABLE HELPER FUNCTIONS =====\n",
    "\n",
    "def extract_content(resp):\n",
    "    \"\"\"\n",
    "    Extract answer text from various response formats (dict, string, JSON).\n",
    "    Handles responses from local Ollama, cloud Ollama, and API models.\n",
    "    Applies UTF-8 encoding fix to ensure proper character display.\n",
    "    \"\"\"\n",
    "    if resp is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Direct dict-like response\n",
    "    if isinstance(resp, dict):\n",
    "        msg = resp.get(\"message\") or resp.get(\"content\")\n",
    "        if isinstance(msg, dict):\n",
    "            content = msg.get(\"content\", \"\").strip()\n",
    "            return fix_utf8_encoding(content)\n",
    "        if isinstance(msg, str):\n",
    "            return fix_utf8_encoding(msg.strip())\n",
    "        return fix_utf8_encoding(str(resp).strip())\n",
    "    \n",
    "    # If it's not a string, stringify it\n",
    "    if not isinstance(resp, str):\n",
    "        return fix_utf8_encoding(str(resp).strip())\n",
    "    \n",
    "    s = resp\n",
    "    \n",
    "    # Try JSON parse if possible\n",
    "    try:\n",
    "        parsed = json.loads(s)\n",
    "        if isinstance(parsed, dict):\n",
    "            msg = parsed.get(\"message\")\n",
    "            if isinstance(msg, dict):\n",
    "                content = msg.get(\"content\", \"\").strip()\n",
    "                return fix_utf8_encoding(content)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Regex: message=Message(... content='...') or content='...' patterns\n",
    "    m = re.search(r\"message=Message\\([^)]*content=(?P<q>['\\\"])(?P<content>.*?)(?P=q)\", s, re.DOTALL)\n",
    "    if m:\n",
    "        return fix_utf8_encoding(m.group(\"content\").strip())\n",
    "    \n",
    "    m2 = re.search(r\"content=(?P<q>['\\\"])(?P<content>.*?)(?P=q)\", s, re.DOTALL)\n",
    "    if m2:\n",
    "        return fix_utf8_encoding(m2.group(\"content\").strip())\n",
    "    \n",
    "    # Fallback: return the original string with UTF-8 fix\n",
    "    return fix_utf8_encoding(s.strip())\n",
    "\n",
    "\n",
    "def execute_query(query: str, vector_store: VectorStore, metadata_store: MetadataStore, \n",
    "                 model_name: str = None, top_k: int = None, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    End-to-end query execution: retrieve documents â†’ generate answer â†’ format with sources.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        vector_store: Loaded FAISS vector store\n",
    "        metadata_store: Loaded metadata store\n",
    "        model_name: LLM model to use (defaults to DEFAULT_OLLAMA_MODEL)\n",
    "        top_k: Number of chunks to retrieve (defaults to DOCUMENT_CONFIG[\"top_k\"])\n",
    "        verbose: Print progress messages\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: \"query\", \"answer\", \"formatted_answer\", \"retrieved_chunks\"\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = DEFAULT_OLLAMA_MODEL\n",
    "    if top_k is None:\n",
    "        top_k = DOCUMENT_CONFIG[\"top_k\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸ“– Query: {query}\")\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_chunks = retrieve_documents_hybrid(query, vector_store, metadata_store, k_local=top_k//2, k_web=top_k//2) # retrieve_documents(query, vector_store, metadata_store, k=top_k)\n",
    "    if verbose:\n",
    "        print(f\"âœ“ Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    \n",
    "    # Generate answer\n",
    "    if verbose:\n",
    "        print(f\"Generating answer with model: {model_name}\")\n",
    "    \n",
    "    raw_response = generate_answer(query, retrieved_chunks, model_name=model_name, stream=False)\n",
    "    answer_text = extract_content(raw_response)\n",
    "    \n",
    "    # Format with sources\n",
    "    formatted_output = format_answer_with_sources(answer_text, retrieved_chunks)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer_text,\n",
    "        \"formatted_answer\": formatted_output,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"model\": model_name\n",
    "    }\n",
    "\n",
    "\n",
    "def display_result(result: dict):\n",
    "    \"\"\"Pretty print query result with answer and sources.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANSWER WITH FORMATTED SOURCES:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"formatted_answer\"])\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"âœ“ Reusable helper functions loaded (extract_content, execute_query, display_result)\")\n",
    "print(\"âœ“ UTF-8 encoding fix applied to all LLM responses\")\n",
    "print(\"âœ“ Configuration centralized in DOCUMENT_CONFIG, MODEL_CONFIG, and TOKEN_CONFIG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "727cedf0"
   },
   "source": [
    "## Pipeline Integration and Testing\n",
    "\n",
    "### Subtask:\n",
    "Combine all components into a basic RAG pipeline and test with a sample document and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Smart embedding state checker loaded (avoids redundant API calls)\n"
     ]
    }
   ],
   "source": [
    "def check_if_embeddings_exist(save_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if embeddings and metadata already exist from a previous run.\n",
    "    Returns True if all necessary files are present and valid.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    required_files = [\n",
    "        os.path.join(save_dir, \"faiss_index.bin\"),\n",
    "        os.path.join(save_dir, \"chunk_ids.pkl\"),\n",
    "        os.path.join(save_dir, \"metadata.pkl\")\n",
    "    ]\n",
    "    \n",
    "    # Check if all files exist\n",
    "    all_exist = all(os.path.exists(f) for f in required_files)\n",
    "    \n",
    "    if all_exist:\n",
    "        try:\n",
    "            # Try to load to verify integrity (silently)\n",
    "            test_vs = VectorStore.load(save_dir, verbose=False)\n",
    "            test_ms = MetadataStore.load(os.path.join(save_dir, \"metadata.pkl\"), verbose=False)\n",
    "            \n",
    "            # Check if they have content\n",
    "            if test_vs.index.ntotal > 0 and len(test_ms.get_all_chunks()) > 0:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Embeddings exist but failed to load: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def skip_embedding_if_exists(save_dir: str, sample_document_path: str):\n",
    "    \"\"\"\n",
    "    Smart pipeline state manager:\n",
    "    - If embeddings exist: Load them and skip document processing\n",
    "    - If embeddings don't exist: Process document normally\n",
    "    \n",
    "    Returns: (should_skip_embedding, vector_store, metadata_store)\n",
    "    \"\"\"\n",
    "    if check_if_embeddings_exist(save_dir):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âš¡ FOUND PRECOMPUTED EMBEDDINGS - SKIPPING REDUNDANT API CALLS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"âœ“ Embeddings already exist in {save_dir}\")\n",
    "        print(f\"âœ“ Skipping: document loading, chunking, and embedding generation\")\n",
    "        print(f\"âœ“ Directly loading from disk...\\n\")\n",
    "        \n",
    "        # Load silently since we're providing our own status messages\n",
    "        vector_store = VectorStore.load(save_dir, verbose=False)\n",
    "        metadata_store = MetadataStore.load(os.path.join(save_dir, \"metadata.pkl\"), verbose=False)\n",
    "        \n",
    "        print(f\"âœ“ Loaded {vector_store.index.ntotal} precomputed embeddings\")\n",
    "        print(f\"âœ“ Loaded {len(metadata_store.get_all_chunks())} document chunks\")\n",
    "        print(f\"âœ“ Ready for querying!\\n\")\n",
    "        \n",
    "        return True, vector_store, metadata_store\n",
    "    else:\n",
    "        print(f\"\\nðŸ“„ No precomputed embeddings found. Processing document: {os.path.basename(sample_document_path)}\")\n",
    "        return False, None, None\n",
    "\n",
    "print(\"âœ“ Smart embedding state checker loaded (avoids redundant API calls)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "referenced_widgets": [
      "55e4d5e6d3704e9aac24702c44475df6",
      "40a5c72060fa40edb8787295fa12fea7",
      "48dda1d3708e4dd5854815eca666c837",
      "eea3c1ca71464bde80a029fe5afcb2a4",
      "a88b76fe2d8c4dfd96981c8857a6d83f",
      "caa1cfaf3aae4fcaa65c63b1622baa6c",
      "1efb9bc13eda4bcea490254207b66cde",
      "6245b3ea77024ed4979d0a71eaa46215",
      "ba60a82bba334f7f9ec6794133eec339",
      "8c21e3ef70154f1397b4aa211859cda0",
      "485e3b0909f54a769bb13aaed7665452"
     ]
    },
    "executionInfo": {
     "elapsed": 252585,
     "status": "ok",
     "timestamp": 1757886585722,
     "user": {
      "displayName": "Aditya Gupta",
      "userId": "04884539656118443680"
     },
     "user_tz": -60
    },
    "id": "4c53947e",
    "outputId": "22e1db6d-448a-48c5-a9fd-9c4547119200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG PIPELINE - Smart Embedding Reuse (Avoids Redundant API Calls)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš¡ FOUND PRECOMPUTED EMBEDDINGS - SKIPPING REDUNDANT API CALLS\n",
      "======================================================================\n",
      "âœ“ Embeddings already exist in ./rag_data\n",
      "âœ“ Skipping: document loading, chunking, and embedding generation\n",
      "âœ“ Directly loading from disk...\n",
      "\n",
      "âœ“ Loaded 3469 precomputed embeddings\n",
      "âœ“ Loaded 3469 document chunks\n",
      "âœ“ Ready for querying!\n",
      "\n",
      "[1/2] âœ“ SKIPPED - Using precomputed embeddings\n",
      "[2/2] Testing retrieval & answer generation with formatted sources...\n",
      "ðŸ“– Query: What are the Four Laws of Behavior Change?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18949/1801865595.py:6: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Retrieved 4 relevant chunks\n",
      "Generating answer with model: gpt-oss:20b-cloud\n",
      "[LLM Config] Sources: 4 | Max tokens: 1312\n",
      "\n",
      "======================================================================\n",
      "ANSWER WITH FORMATTED SOURCES:\n",
      "======================================================================\n",
      "**The Four Laws of Behavior Change** (James Clear, *Atomic Habits*) are simple rules for building better habits:\\n\\n1. **Make it obvious** â€“ design cues that clearly signal the desired behavior.  \\n2. **Make it attractive** â€“ pair the habit with something appealing or rewarding.  \\n3. **Make it easy** â€“ reduce friction so the action can be performed with minimal effort.  \\n4. **Make it satisfying** â€“ ensure the experience feels immediately rewarding, so the brain is motivated to repeat it.  \\n\\nThese laws work together to increase the likelihood that a behavior will be performed now and repeated later [1][2].\n",
      "\n",
      "ðŸ“š SOURCES:\n",
      "\n",
      "[1] Atomic_Habits_James_Clear.pdf (Page 52):\n",
      "    \"The ultimate purpose of habits is to solve the problems of life with as little energy and effort as possible. Any habit can be broken down into a feedback loop that involves four steps: cue, craving, ...\"\n",
      "\n",
      "[2] Atomic_Habits_James_Clear.pdf (Page 155):\n",
      "    \"Chapter Summary The 4th Law of Behavior Change is make it satisfying. We are more likely to repeat a behavior when the experience is satisfying. The human brain evolved to prioritize immediate rewards...\"\n",
      "\n",
      "[3] ðŸŒ Two and two is/are four - WordReference Forums\n",
      "    https://forum.wordreference.com/threads/two-and-two-is-are-four.3227174/\n",
      "\n",
      "[4] ðŸŒ When breaking up a quote-- three dots... vs. four dots ...\n",
      "    https://forum.wordreference.com/threads/when-breaking-up-a-quote-three-dots-vs-four-dots.64566/\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ“ Query complete! (Using cached embeddings)\n",
      "  To regenerate embeddings: delete ./rag_data/ or set should_skip_embedding=False\n"
     ]
    }
   ],
   "source": [
    "# ===== INTELLIGENT PIPELINE EXECUTION (WITH SMART SKIP) =====\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAG PIPELINE - Smart Embedding Reuse (Avoids Redundant API Calls)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Unpack configuration\n",
    "sample_document_path = DOCUMENT_CONFIG[\"sample_path\"]\n",
    "SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "CHUNK_SIZE = DOCUMENT_CONFIG[\"chunk_size\"]\n",
    "CHUNK_OVERLAP = DOCUMENT_CONFIG[\"chunk_overlap\"]\n",
    "\n",
    "# Check if embeddings already exist from previous run\n",
    "should_skip_embedding, vector_store, metadata_store = skip_embedding_if_exists(SAVE_DIR, sample_document_path)\n",
    "\n",
    "if should_skip_embedding:\n",
    "    # Embeddings exist - skip document processing and go straight to querying\n",
    "    print(\"[1/2] âœ“ SKIPPED - Using precomputed embeddings\")\n",
    "    print(\"[2/2] Testing retrieval & answer generation with formatted sources...\")\n",
    "else:\n",
    "    # Embeddings don't exist - process document normally\n",
    "    \n",
    "    # Step 1: Load Document\n",
    "    print(\"\\n[1/7] Loading document...\")\n",
    "    try:\n",
    "        document_data = load_document(sample_document_path)\n",
    "        if isinstance(document_data, list):  # PDF returns list of (text, page_num)\n",
    "            print(f\"âœ“ Loaded PDF with {len(document_data)} pages\")\n",
    "            is_pdf = True\n",
    "        elif isinstance(document_data, str):  # Text/DOCX returns string\n",
    "            print(f\"âœ“ Loaded document ({len(document_data)} characters)\")\n",
    "            is_pdf = False\n",
    "            document_data = [(document_data, None)]  # Convert to same format\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load document\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading document: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 2: Chunk Document\n",
    "    print(f\"\\n[2/7] Chunking document (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...\")\n",
    "    if is_pdf:\n",
    "        chunks_data = chunk_pdf_pages(document_data, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "    else:\n",
    "        chunks_data = chunk_document_semantic(document_data[0][0], chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    print(f\"âœ“ Created {len(chunks_data)} semantic chunks\")\n",
    "\n",
    "    # Step 3: Create Metadata Store\n",
    "    print(\"\\n[3/7] Building metadata store...\")\n",
    "    metadata_store = MetadataStore()\n",
    "    document_chunks = []\n",
    "\n",
    "    for chunk_text, page_num in chunks_data:\n",
    "        chunk = DocumentChunk(\n",
    "            text=chunk_text,\n",
    "            source=os.path.basename(sample_document_path),\n",
    "            page_number=page_num\n",
    "        )\n",
    "        metadata_store.add_chunk(chunk)\n",
    "        document_chunks.append(chunk)\n",
    "\n",
    "    metadata_store.index_chunks()\n",
    "\n",
    "    # Step 4: Generate Embeddings (WITH OPTIMIZATIONS)\n",
    "    print(f\"\\n[4/7] Generating embeddings using {EMBEDDING_METHOD}...\")\n",
    "    print(\"      âš¡ Enabling: caching, parallel processing, and batch optimization\")\n",
    "    generate_embeddings(document_chunks, batch_size=100, use_optimizations=True)\n",
    "\n",
    "    # Step 5: Build Vector Store\n",
    "    print(\"\\n[5/7] Building vector store with cosine similarity...\")\n",
    "    vector_store = VectorStore(EMBEDDING_DIM)\n",
    "\n",
    "    embeddings = np.array([chunk.embedding for chunk in document_chunks])\n",
    "    chunk_ids = [chunk.chunk_id for chunk in document_chunks]\n",
    "\n",
    "    vector_store.add_vectors(embeddings, chunk_ids)\n",
    "\n",
    "    # Step 6: Save Everything to Disk\n",
    "    print(f\"\\n[6/7] Saving to disk ({SAVE_DIR})...\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    vector_store.save(SAVE_DIR)\n",
    "    metadata_store.save(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "    \n",
    "    print(f\"âœ“ Embeddings saved! Next run will reuse them automatically.\")\n",
    "    print(f\"\\n[7/7] Testing retrieval & answer generation with formatted sources...\")\n",
    "\n",
    "# ===== COMMON PATH (for both first run and subsequent runs) =====\n",
    "\n",
    "query = \"What are the Four Laws of Behavior Change?\"\n",
    "result = execute_query(query, vector_store, metadata_store, verbose=True)\n",
    "display_result(result)\n",
    "\n",
    "if should_skip_embedding:\n",
    "    print(f\"\\nâœ“ Query complete! (Using cached embeddings)\")\n",
    "    print(f\"  To regenerate embeddings: delete {SAVE_DIR}/ or set should_skip_embedding=False\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Pipeline complete! Data saved to {SAVE_DIR}\")\n",
    "    print(f\"  Embedding cache stored in: .embedding_cache/\")\n",
    "    print(f\"  Next run will reuse embeddings automatically!\")\n",
    "    print(f\"  To reload manually: vector_store = VectorStore.load('{SAVE_DIR}')\")\n",
    "    print(f\"                      metadata_store = MetadataStore.load('{SAVE_DIR}/metadata.pkl')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb20254d"
   },
   "source": [
    "## Refinement and Evaluation\n",
    "\n",
    "### Subtask:\n",
    "Outline steps for refining and evaluating the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DEMO PREPARATION: UI-Ready Functions =====\n",
    "\n",
    "# Global state for UI demos (initialized once)\n",
    "_demo_state = {\n",
    "    \"vector_store\": None,\n",
    "    \"metadata_store\": None,\n",
    "    \"initialized\": False,\n",
    "}\n",
    "\n",
    "def try_load_existing_embeddings():\n",
    "    \"\"\"\n",
    "    Attempt to load precomputed embeddings from disk.\n",
    "    Returns (success: bool, vector_store, metadata_store)\n",
    "    \"\"\"\n",
    "    SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "    \n",
    "    try:\n",
    "        if check_if_embeddings_exist(SAVE_DIR):\n",
    "            print(\"âœ“ Loading precomputed embeddings from disk...\")\n",
    "            vs = VectorStore.load(SAVE_DIR, verbose=False)\n",
    "            ms = MetadataStore.load(\n",
    "                os.path.join(SAVE_DIR, \"metadata.pkl\"), verbose=False\n",
    "            )\n",
    "            return True, vs, ms\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load embeddings: {e}\")\n",
    "    \n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "def initialize_rag_demo():\n",
    "    \"\"\"\n",
    "    Initialize RAG system for UI demo.\n",
    "    Loads precomputed embeddings if available, otherwise waits for user to upload documents.\n",
    "    Safe to call multiple times - only initializes once.\n",
    "    \"\"\"\n",
    "    if _demo_state[\"initialized\"]:\n",
    "        return _demo_state[\"vector_store\"], _demo_state[\"metadata_store\"]\n",
    "    \n",
    "    # Try to load existing embeddings\n",
    "    success, vs, ms = try_load_existing_embeddings()\n",
    "    \n",
    "    if success:\n",
    "        _demo_state[\"vector_store\"] = vs\n",
    "        _demo_state[\"metadata_store\"] = ms\n",
    "        _demo_state[\"initialized\"] = True\n",
    "        print(\"âœ“ RAG system ready to use!\")\n",
    "        return vs, ms\n",
    "    \n",
    "    else:\n",
    "        # No precomputed embeddings - wait for user to upload documents\n",
    "        print(\"\\nðŸ“š RAG System Ready for Document Upload\")\n",
    "        print(\"   Waiting for user to upload documents via the UI...\")\n",
    "        print(\"   No local documents to process automatically.\")\n",
    "        print(\"\\n   To get started:\")\n",
    "        print(\"   1. Open the 'ðŸ“¤ Upload Documents' tab\")\n",
    "        print(\"   2. Upload your PDF/DOCX/TXT files\")\n",
    "        print(\"   3. Click 'Process Documents'\")\n",
    "        print(\"   4. Switch to 'ðŸ” Ask Questions' tab to query\\n\")\n",
    "        \n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.13/collections/__init__.py:452: ResourceWarning: unclosed <ssl.SSLSocket fd=88, family=2, type=1, proto=6, laddr=('192.168.10.102', 36344), raddr=('34.36.133.15', 443)>\n",
      "  result = tuple_new(cls, iterable)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# ===== GRADIO DEMO INTERFACE WITH MULTI-DOCUMENT UPLOAD =====\n",
    "\n",
    "# Global state for multi-document handling\n",
    "_upload_state = {\n",
    "    \"documents\": [],  # List of (filename, file_path) tuples\n",
    "    \"processing\": False\n",
    "}\n",
    "\n",
    "def is_port_available(port: int) -> bool:\n",
    "    \"\"\"Check if a port is available for use.\"\"\"\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            s.bind(('', port))\n",
    "            return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def find_available_port(start_port: int = 7860, max_attempts: int = 10) -> int:\n",
    "    \"\"\"Find an available port starting from start_port.\"\"\"\n",
    "    for offset in range(max_attempts):\n",
    "        port = start_port + offset\n",
    "        if is_port_available(port):\n",
    "            return port\n",
    "    raise RuntimeError(f\"Could not find available port in range {start_port}-{start_port + max_attempts - 1}\")\n",
    "\n",
    "def process_uploaded_documents(files) -> str:\n",
    "    \"\"\"\n",
    "    Process multiple uploaded documents.\n",
    "    Generates embeddings for all documents and stores them in the vector store.\n",
    "    \n",
    "    Args:\n",
    "        files: List of file paths (from Gradio file_count=\"multiple\")\n",
    "    \"\"\"\n",
    "    # Handle both single file and multiple files\n",
    "    if files is None:\n",
    "        return \"âŒ Error: No files uploaded. Please upload at least one PDF or document.\"\n",
    "    \n",
    "    # Ensure files is always a list\n",
    "    if isinstance(files, str):\n",
    "        files = [files]\n",
    "    elif not isinstance(files, list):\n",
    "        files = [files]\n",
    "    \n",
    "    # Filter out None values\n",
    "    uploaded_files = [f for f in files if f is not None]\n",
    "    \n",
    "    if not uploaded_files:\n",
    "        return \"âŒ Error: No files uploaded. Please upload at least one PDF or document.\"\n",
    "    \n",
    "    try:\n",
    "        _upload_state[\"processing\"] = True\n",
    "        \n",
    "        print(f\"\\nðŸ“š Processing {len(uploaded_files)} document(s)...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Initialize metadata store if not exists\n",
    "        ms = MetadataStore()\n",
    "        vs = VectorStore(EMBEDDING_DIM)\n",
    "        \n",
    "        total_chunks = 0\n",
    "        \n",
    "        # Process each uploaded document\n",
    "        for file_path in uploaded_files:\n",
    "            # Ensure file_path is a string\n",
    "            if not isinstance(file_path, str):\n",
    "                file_path = str(file_path)\n",
    "            \n",
    "            # Strip any whitespace or special characters\n",
    "            file_path = file_path.strip().strip(\"[]'\\\"\")\n",
    "            \n",
    "            if not file_path:\n",
    "                continue\n",
    "            \n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\nðŸ“„ Processing: {filename}\")\n",
    "            \n",
    "            try:\n",
    "                # Load document based on file type\n",
    "                if filename.lower().endswith('.pdf'):\n",
    "                    # load_pdf expects a file path\n",
    "                    document_data = load_pdf(file_path)\n",
    "                    if not document_data:\n",
    "                        print(f\"   âš ï¸  No content extracted from PDF\")\n",
    "                        continue\n",
    "                    chunks_data = chunk_pdf_pages(\n",
    "                        document_data,\n",
    "                        chunk_size=DOCUMENT_CONFIG[\"chunk_size\"],\n",
    "                        overlap=DOCUMENT_CONFIG[\"chunk_overlap\"]\n",
    "                    )\n",
    "                elif filename.lower().endswith(('.txt', '.md')):\n",
    "                    # load_text handles file paths\n",
    "                    document_text = load_text(file_path)\n",
    "                    if not document_text or not document_text.strip():\n",
    "                        print(f\"   âš ï¸  No content extracted from text file\")\n",
    "                        continue\n",
    "                    # For text files, wrap in list of tuples with None as page_num\n",
    "                    chunks_data = chunk_document_semantic(\n",
    "                        document_text,\n",
    "                        chunk_size=DOCUMENT_CONFIG[\"chunk_size\"],\n",
    "                        overlap=DOCUMENT_CONFIG[\"chunk_overlap\"],\n",
    "                        page_number=None\n",
    "                    )\n",
    "                elif filename.lower().endswith('.docx'):\n",
    "                    # Load DOCX file\n",
    "                    document_text = load_docx(file_path)\n",
    "                    if not document_text or not document_text.strip():\n",
    "                        print(f\"   âš ï¸  No content extracted from DOCX file\")\n",
    "                        continue\n",
    "                    chunks_data = chunk_document_semantic(\n",
    "                        document_text,\n",
    "                        chunk_size=DOCUMENT_CONFIG[\"chunk_size\"],\n",
    "                        overlap=DOCUMENT_CONFIG[\"chunk_overlap\"],\n",
    "                        page_number=None\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  Unsupported file type: {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                if not chunks_data:\n",
    "                    print(f\"   âš ï¸  No chunks created from {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                # Create chunks with metadata\n",
    "                document_chunks = []\n",
    "                for chunk_text, page_num in chunks_data:\n",
    "                    chunk = DocumentChunk(\n",
    "                        text=chunk_text,\n",
    "                        source=filename,\n",
    "                        page_number=page_num\n",
    "                    )\n",
    "                    ms.add_chunk(chunk)\n",
    "                    document_chunks.append(chunk)\n",
    "                \n",
    "                print(f\"   âœ“ Created {len(document_chunks)} chunks\")\n",
    "                total_chunks += len(document_chunks)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                print(f\"   â³ Generating embeddings...\")\n",
    "                generate_embeddings(document_chunks, batch_size=100, use_optimizations=True)\n",
    "                print(f\"   âœ“ Embeddings generated\")\n",
    "                \n",
    "                # Add to vector store\n",
    "                embeddings = np.array([chunk.embedding for chunk in document_chunks])\n",
    "                chunk_ids = [chunk.chunk_id for chunk in document_chunks]\n",
    "                vs.add_vectors(embeddings, chunk_ids)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error processing {filename}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if total_chunks == 0:\n",
    "            return \"âŒ Error: No chunks were created from the uploaded documents. Please check the files contain readable text.\"\n",
    "        \n",
    "        # Save to disk\n",
    "        SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "        vs.save(SAVE_DIR)\n",
    "        ms.save(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "        \n",
    "        # Update demo state\n",
    "        _demo_state[\"vector_store\"] = vs\n",
    "        _demo_state[\"metadata_store\"] = ms\n",
    "        _demo_state[\"initialized\"] = True\n",
    "        \n",
    "        _upload_state[\"processing\"] = False\n",
    "        \n",
    "        success_msg = f\"\"\"\n",
    "âœ… **Documents Processed Successfully!**\n",
    "\n",
    "- **Documents:** {len(uploaded_files)}\n",
    "- **Total Chunks:** {total_chunks}\n",
    "- **Vector Store:** Saved to `{SAVE_DIR}`\n",
    "\n",
    "You can now ask questions about your uploaded documents using the search box below.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n{success_msg}\")\n",
    "        return success_msg\n",
    "        \n",
    "    except Exception as e:\n",
    "        _upload_state[\"processing\"] = False\n",
    "        error_msg = f\"âŒ Error processing documents: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "def rag_demo_interface(query: str, top_k: int = 5, model: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Gradio-compatible RAG interface.\n",
    "    Takes user query and returns formatted answer with sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if documents have been processed\n",
    "        if not _demo_state.get(\"initialized\"):\n",
    "            return \"âŒ Error: Please upload and process documents first using the 'Upload Documents' tab.\"\n",
    "        \n",
    "        # Use default model if not specified\n",
    "        if model is None or model.strip() == \"\":\n",
    "            model = DEFAULT_OLLAMA_MODEL\n",
    "        \n",
    "        # Execute query with user-specified parameters\n",
    "        vs = _demo_state[\"vector_store\"]\n",
    "        ms = _demo_state[\"metadata_store\"]\n",
    "        result = execute_query(query, vs, ms, model_name=model, top_k=top_k, verbose=False)\n",
    "        \n",
    "        return result[\"formatted_answer\"]\n",
    "    \n",
    "    except ValueError as e:\n",
    "        # RAG system not initialized\n",
    "        return f\"âŒ Error: {str(e)}\\n\\nPlease initialize the system first by uploading documents.\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error processing query: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_gradio_demo():\n",
    "    \"\"\"\n",
    "    Create and configure Gradio interface for multi-document RAG system.\n",
    "    Returns the Gradio Blocks interface with document upload and Q&A tabs.\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        title=\"ðŸ“š RAG Q&A Assistant\",\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        .gr-box { border-radius: 12px; }\n",
    "        .gr-button { border-radius: 8px; }\n",
    "        .gr-textbox { border-radius: 8px; }\n",
    "        \"\"\"\n",
    "    ) as demo:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # ðŸ“š RAG Q&A Assistant with Multi-Document Support\n",
    "        \n",
    "        Upload your documents and ask questions using semantic search + local LLM.\n",
    "        **Powered by:** FAISS + Ollama + HuggingFace Embeddings\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # ===== TAB 1: DOCUMENT UPLOAD =====\n",
    "            with gr.Tab(\"ðŸ“¤ Upload Documents\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ## Upload Your Documents\n",
    "                \n",
    "                Upload one or more documents (PDF, DOCX, TXT, MD) to index and search.\n",
    "                The system will:\n",
    "                1. Process each document\n",
    "                2. Split into semantic chunks\n",
    "                3. Generate embeddings\n",
    "                4. Save to vector store for future queries\n",
    "                \"\"\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    file_upload = gr.File(\n",
    "                        label=\"Upload Documents\",\n",
    "                        file_count=\"multiple\",\n",
    "                        file_types=[\".pdf\", \".txt\", \".md\", \".docx\"],\n",
    "                        type=\"filepath\"\n",
    "                    )\n",
    "                \n",
    "                process_btn = gr.Button(\"âš™ï¸ Process Documents\", variant=\"primary\", size=\"lg\")\n",
    "                upload_status = gr.Markdown(\"*No documents uploaded yet*\")\n",
    "                \n",
    "                # Connect upload button\n",
    "                process_btn.click(\n",
    "                    fn=process_uploaded_documents,\n",
    "                    inputs=[file_upload],\n",
    "                    outputs=upload_status\n",
    "                )\n",
    "            \n",
    "            # ===== TAB 2: Q&A INTERFACE =====\n",
    "            with gr.Tab(\"ðŸ” Ask Questions\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ## Query Your Documents\n",
    "                \n",
    "                Ask questions about the documents you've uploaded. The system will retrieve relevant chunks and generate answers with source citations.\n",
    "                \"\"\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        query_input = gr.Textbox(\n",
    "                            label=\"â“ Your Question\",\n",
    "                            lines=3,\n",
    "                            placeholder=\"e.g., What are the main topics discussed?\",\n",
    "                            interactive=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        top_k_slider = gr.Slider(\n",
    "                            label=\"ðŸ“Š Number of Sources\",\n",
    "                            minimum=1,\n",
    "                            maximum=10,\n",
    "                            value=5,\n",
    "                            step=1,\n",
    "                            interactive=True\n",
    "                        )\n",
    "                        \n",
    "                        model_dropdown = gr.Textbox(\n",
    "                            label=\"ðŸ¤– Model (optional)\",\n",
    "                            value=DEFAULT_OLLAMA_MODEL,\n",
    "                            placeholder=\"Leave blank for default\",\n",
    "                            interactive=True\n",
    "                        )\n",
    "                \n",
    "                submit_btn = gr.Button(\n",
    "                    \"ðŸ” Search\",\n",
    "                    variant=\"primary\",\n",
    "                    scale=1\n",
    "                )\n",
    "                \n",
    "                # Output\n",
    "                answer_output = gr.Markdown(\n",
    "                    label=\"ðŸ“– Answer with Sources\",\n",
    "                    value=\"Your answer will appear here... (Make sure to upload documents first!)\"\n",
    "                )\n",
    "                \n",
    "                # Connect button to function\n",
    "                submit_btn.click(\n",
    "                    fn=rag_demo_interface,\n",
    "                    inputs=[query_input, top_k_slider, model_dropdown],\n",
    "                    outputs=answer_output\n",
    "                )\n",
    "                \n",
    "                # Example questions\n",
    "                gr.Examples(\n",
    "                    examples=[\n",
    "                        [\"What are the main topics?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                        [\"Summarize the key points\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                        [\"What's discussed in the document?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                        [\"Give me a detailed overview\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                    ],\n",
    "                    inputs=[query_input, top_k_slider, model_dropdown],\n",
    "                    outputs=answer_output,\n",
    "                    fn=rag_demo_interface,\n",
    "                    cache_examples=False,\n",
    "                )\n",
    "                \n",
    "                # Footer info\n",
    "                gr.Markdown(\"\"\"\n",
    "                ---\n",
    "                ### â„¹ï¸ How It Works\n",
    "                1. **Query Processing**: Your question is converted to embeddings\n",
    "                2. **Semantic Search**: Top-K most relevant chunks are retrieved from documents\n",
    "                3. **LLM Generation**: Local Ollama model generates answer based on retrieved chunks\n",
    "                4. **Source Citation**: Answer includes [1], [2] citations linked to source documents\n",
    "                \n",
    "                ### ðŸš€ Performance\n",
    "                - First run: ~5 minutes (document processing + embedding generation)\n",
    "                - Subsequent queries: <5 seconds (embeddings cached)\n",
    "                - Completely private & offline (when using local models)\n",
    "                \"\"\")\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ§ª DYNAMIC TOKEN LIMITS CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Token Limit Configuration:\n",
      "   Base tokens: 512 (increased from 256)\n",
      "   Per source: 200 tokens/source\n",
      "   Max tokens: 2048 (absolute cap)\n",
      "\n",
      "ðŸ“ Formula: num_predict = min(base + num_sources Ã— per_source, max)\n",
      "\n",
      "======================================================================\n",
      "Expected token limits per test case:\n",
      "======================================================================\n",
      "   3 sources            â†’ 1112 max tokens\n",
      "   5 sources            â†’ 1512 max tokens\n",
      "   8 sources            â†’ 2048 max tokens\n",
      "   10 sources (max)     â†’ 2048 max tokens\n",
      "\n",
      "âœ… All functions and configurations are now defined.\n",
      "   Proceed to Step 2 to initialize the RAG system.\n"
     ]
    }
   ],
   "source": [
    "# ===== 1. Configuration & Setup =====\n",
    "\n",
    "# This cell contains all the core logic: document loading, chunking, embedding, etc.\n",
    "# It must be run first to define all necessary functions.\n",
    "\n",
    "# (The code from your previous cells is assumed to be here)\n",
    "# ... DocumentChunk, MetadataStore, VectorStore, etc. ...\n",
    "\n",
    "# ===== DYNAMIC TOKEN LIMITS CONFIGURATION =====\n",
    "# Test the dynamic token limits with different numbers of sources\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ§ª DYNAMIC TOKEN LIMITS CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with different numbers of sources\n",
    "test_queries = [\n",
    "    (\"What are the Four Laws of Behavior Change?\", 3, \"3 sources\"),\n",
    "    (\"How do habits compound over time?\", 5, \"5 sources\"),\n",
    "    (\"Explain the habit loop and its components\", 8, \"8 sources\"),\n",
    "    (\"What are the best strategies for building good habits?\", 10, \"10 sources (max)\"),\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“Š Token Limit Configuration:\")\n",
    "print(f\"   Base tokens: {TOKEN_CONFIG['base']} (increased from 256)\")\n",
    "print(f\"   Per source: {TOKEN_CONFIG['per_source']} tokens/source\")\n",
    "print(f\"   Max tokens: {TOKEN_CONFIG['max']} (absolute cap)\")\n",
    "print(f\"\\nðŸ“ Formula: num_predict = min(base + num_sources Ã— per_source, max)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Expected token limits per test case:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query, num_sources, description in test_queries:\n",
    "    expected_tokens = min(\n",
    "        TOKEN_CONFIG['base'] + (num_sources * TOKEN_CONFIG['per_source']),\n",
    "        TOKEN_CONFIG['max']\n",
    "    )\n",
    "    print(f\"   {description:20} â†’ {expected_tokens:4} max tokens\")\n",
    "\n",
    "print(\"\\nâœ… All functions and configurations are now defined.\")\n",
    "print(\"   Proceed to Step 2 to initialize the RAG system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… QUICK TEST: Manual Query with 10 Sources\n",
      "======================================================================\n",
      "âš ï¸  RAG system not initialized. Run the 'Initialize RAG System' cell first.\n",
      "   If no saved data exists, you must first run the Gradio UI and upload documents.\n"
     ]
    }
   ],
   "source": [
    "# ===== 3. Manual Test (Optional) =====\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… QUICK TEST: Manual Query with 10 Sources\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test directly with the loaded RAG system (no Gradio needed)\n",
    "if _demo_state.get(\"initialized\"):\n",
    "    test_query = \"What are the four laws of behavior change?\"\n",
    "    print(f\"\\nðŸ“ Query: {test_query}\")\n",
    "    print(f\"ðŸ“Š Testing with: 10 sources (should show '[LLM Config] Sources: 10 | Max tokens: 2048')\\n\")\n",
    "    \n",
    "    vs = _demo_state[\"vector_store\"]\n",
    "    ms = _demo_state[\"metadata_store\"]\n",
    "    result = execute_query(test_query, vs, ms, top_k=10)\n",
    "    \n",
    "    print(f\"\\nâœ… Answer length: {len(result['answer'])} characters\")\n",
    "    print(f\"âœ… Sources retrieved: {len(result['retrieved_chunks'])}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FORMATTED ANSWER WITH ALL SOURCES:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"formatted_answer\"][:1500])  # Show first 1500 chars\n",
    "    print(\"\\n... (truncated for display)\")\n",
    "else:\n",
    "    print(\"âš ï¸  RAG system not initialized. Run the 'Initialize RAG System' cell first.\")\n",
    "    print(\"   If no saved data exists, you must first run the Gradio UI and upload documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize RAG System (Load Saved Data)\n",
    "\n",
    "This cell attempts to load a previously saved vector store. If it succeeds, you can immediately run the manual test or start the Gradio UI to ask questions.\n",
    "\n",
    "If it fails, you'll need to run the Gradio UI cell, upload documents, and process them to create the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Attempting to initialize RAG system from saved data...\n",
      "======================================================================\n",
      "Vector store loaded from ./rag_data (3469 vectors)\n",
      "Metadata store loaded from ./rag_data/metadata.pkl (3469 chunks)\n",
      "\n",
      "âœ… RAG system initialized successfully from ./rag_data\n",
      "   - Loaded 3469 vectors\n",
      "   - Loaded metadata for 3469 chunks\n",
      "\n",
      "âœ… You can now run the 'Manual Test' cell or the 'Gradio UI' cell.\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. Initialize RAG System (Load Saved Data) =====\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Attempting to initialize RAG system from saved data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "    \n",
    "    # Try to load previously saved data\n",
    "    vector_store = VectorStore.load(SAVE_DIR)\n",
    "    metadata_store = MetadataStore.load(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "    \n",
    "    # Update demo state for use in other cells\n",
    "    _demo_state[\"vector_store\"] = vector_store\n",
    "    _demo_state[\"metadata_store\"] = metadata_store\n",
    "    _demo_state[\"initialized\"] = True\n",
    "    \n",
    "    print(f\"\\nâœ… RAG system initialized successfully from {SAVE_DIR}\")\n",
    "    print(f\"   - Loaded {vector_store.index.ntotal} vectors\")\n",
    "    print(f\"   - Loaded metadata for {len(metadata_store.chunks)} chunks\")\n",
    "    print(\"\\nâœ… You can now run the 'Manual Test' cell or the 'Gradio UI' cell.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nâš ï¸  No saved data found in {DOCUMENT_CONFIG['save_dir']}\")\n",
    "    print(\"\\nACTION REQUIRED:\")\n",
    "    print(\"1. Run the 'Start Gradio UI' cell below.\")\n",
    "    print(\"2. Use the 'ðŸ“¤ Upload Documents' tab to upload and process files.\")\n",
    "    print(\"3. This will create the necessary data for other cells to use.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error loading vector store: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Launch Gradio Demo\n",
    "\n",
    "Run this cell to start the interactive web interface for uploading documents and querying your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ Important: Event Loop Fix for Gradio in Jupyter\n",
    "\n",
    "### Issue:\n",
    "Gradio's `.queue()` method creates async event loop conflicts in Jupyter notebooks, causing:\n",
    "```\n",
    "RuntimeError: <asyncio.locks.Event object> is bound to a different event loop\n",
    "```\n",
    "\n",
    "### Solution:\n",
    "- **Removed `.queue()` call** - Not necessary for single-user notebook usage\n",
    "- Queue is only needed for production deployments with concurrent users\n",
    "- Gradio works perfectly fine without queue in Jupyter\n",
    "\n",
    "### If You Still Get Event Loop Errors:\n",
    "1. **Restart the Jupyter kernel completely** (Kernel â†’ Restart)\n",
    "2. Re-run all cells from the beginning\n",
    "3. Make sure no other Gradio instances are running\n",
    "\n",
    "### Why This Happens:\n",
    "- Jupyter manages its own event loop for async operations\n",
    "- Gradio's queue tries to create a separate event loop\n",
    "- These two loops conflict, causing the runtime error\n",
    "- Solution: Let Jupyter manage the loop, skip Gradio's queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ Starting Gradio RAG Demo...\n",
      "======================================================================\n",
      "âœ“ Port 7860 is available\n",
      "\n",
      "ðŸŒ Launching Gradio interface...\n",
      "======================================================================\n",
      "ðŸ“– Access the demo at: http://127.0.0.1:7860\n",
      "======================================================================\n",
      "\n",
      "ðŸ“š Instructions:\n",
      "1. Go to the 'ðŸ“¤ Upload Documents' tab\n",
      "2. Upload your PDF, TXT, MD, or DOCX files\n",
      "3. Click 'âš™ï¸ Process Documents' and wait for completion\n",
      "4. Switch to 'ðŸ” Ask Questions' tab to query your documents\n",
      "\n",
      "âœ¨ Features:\n",
      "  â€¢ UTF-8 encoding fixed (proper quotes, dashes, special chars)\n",
      "  â€¢ Markdown rendering (code blocks, bold, bullets, etc.)\n",
      "  â€¢ Source citations with page numbers\n",
      "  â€¢ Hybrid search (local + web when enabled)\n",
      "\n",
      "âœ¨ Your documents will be processed and indexed for semantic search!\n",
      "======================================================================\n",
      "\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions\n",
      "  warnings.warn(  # deprecated in 14.0 - 2024-11-09\n",
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/uvicorn/protocols/websockets/websockets_impl.py:17: DeprecationWarning: websockets.server.WebSocketServerProtocol is deprecated\n",
      "  from websockets.server import WebSocketServerProtocol\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/gradio/routes.py:1341: DeprecationWarning: 'HTTP_422_UNPROCESSABLE_ENTITY' is deprecated. Use 'HTTP_422_UNPROCESSABLE_CONTENT' instead.\n",
      "  return await queue_join_helper(body, request, username)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š Processing 3 document(s)...\n",
      "======================================================================\n",
      "\n",
      "ðŸ“„ Processing: Atomic_Habits_James_Clear.pdf\n",
      "Loaded 282 pages from PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 282/282 [00:00<00:00, 7686.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 660 chunks\n",
      "   â³ Generating embeddings...\n",
      "Generating embeddings for 660 chunks using optimized huggingface...\n",
      "  Options: caching (10x+), parallel HF (5-10x), local fast embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 660 cached embeddings (skipping 660 API calls)\n",
      "âœ“ All embeddings loaded from cache!\n",
      "âœ“ Generated 660 embeddings (768-dimensional)\n",
      "   âœ“ Embeddings generated\n",
      "Added 660 vectors. Total: 660\n",
      "\n",
      "ðŸ“„ Processing: The_subtle_art_of_not_giving_a_fuck_Mark_Manson.pdf\n",
      "Loaded 153 pages from PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:00<00:00, 4407.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 423 chunks\n",
      "   â³ Generating embeddings...\n",
      "Generating embeddings for 423 chunks using optimized huggingface...\n",
      "  Options: caching (10x+), parallel HF (5-10x), local fast embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 423 cached embeddings (skipping 423 API calls)\n",
      "âœ“ All embeddings loaded from cache!\n",
      "âœ“ Generated 423 embeddings (768-dimensional)\n",
      "   âœ“ Embeddings generated\n",
      "Added 423 vectors. Total: 1083\n",
      "\n",
      "ðŸ“„ Processing: Speech_and_language_processing_ed3book_aug25.pdf\n",
      "Loaded 620 pages from PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [00:00<00:00, 3278.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 2386 chunks\n",
      "   â³ Generating embeddings...\n",
      "Generating embeddings for 2386 chunks using optimized huggingface...\n",
      "  Options: caching (10x+), parallel HF (5-10x), local fast embedding\n",
      "âœ“ Found 2386 cached embeddings (skipping 2386 API calls)\n",
      "âœ“ All embeddings loaded from cache!\n",
      "âœ“ Generated 2386 embeddings (768-dimensional)\n",
      "   âœ“ Embeddings generated\n",
      "Added 2386 vectors. Total: 3469\n",
      "Vector store saved to ./rag_data\n",
      "Metadata store saved to ./rag_data/metadata.pkl\n",
      "\n",
      "\n",
      "âœ… **Documents Processed Successfully!**\n",
      "\n",
      "- **Documents:** 3\n",
      "- **Total Chunks:** 3469\n",
      "- **Vector Store:** Saved to `./rag_data`\n",
      "\n",
      "You can now ask questions about your uploaded documents using the search box below.\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/gradio/routes.py:1341: DeprecationWarning: 'HTTP_422_UNPROCESSABLE_ENTITY' is deprecated. Use 'HTTP_422_UNPROCESSABLE_CONTENT' instead.\n",
      "  return await queue_join_helper(body, request, username)\n",
      "/tmp/ipykernel_18949/1801865595.py:6: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Config] Sources: 6 | Max tokens: 1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agupta/dev/nlp_projects/rag-terminal/.venv/lib64/python3.13/site-packages/gradio/routes.py:1341: DeprecationWarning: 'HTTP_422_UNPROCESSABLE_ENTITY' is deprecated. Use 'HTTP_422_UNPROCESSABLE_CONTENT' instead.\n",
      "  return await queue_join_helper(body, request, username)\n",
      "/tmp/ipykernel_18949/1801865595.py:6: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Config] Sources: 3 | Max tokens: 1112\n"
     ]
    }
   ],
   "source": [
    "# ===== LAUNCH GRADIO DEMO =====\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸš€ Starting Gradio RAG Demo...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Close any existing demo to prevent conflicts\n",
    "    try:\n",
    "        if 'gradio_demo' in globals() and gradio_demo is not None:\n",
    "            print(\"âš ï¸  Closing previous Gradio instance...\")\n",
    "            try:\n",
    "                gradio_demo.close()\n",
    "            except Exception:\n",
    "                pass  # Ignore errors when closing\n",
    "            # Wait a moment for cleanup\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "    except Exception as cleanup_error:\n",
    "        print(f\"  (Cleanup note: {cleanup_error})\")\n",
    "    \n",
    "    # Find available port (handle port conflicts gracefully)\n",
    "    preferred_port = 7860\n",
    "    if not is_port_available(preferred_port):\n",
    "        print(f\"âš ï¸  Port {preferred_port} is already in use. Finding available port...\")\n",
    "        available_port = find_available_port(preferred_port)\n",
    "        print(f\"âœ“ Using port {available_port} instead\\n\")\n",
    "    else:\n",
    "        available_port = preferred_port\n",
    "        print(f\"âœ“ Port {preferred_port} is available\\n\")\n",
    "    \n",
    "    # Create and launch Gradio interface\n",
    "    gradio_demo = create_gradio_demo()\n",
    "    \n",
    "    print(\"ðŸŒ Launching Gradio interface...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ðŸ“– Access the demo at: http://127.0.0.1:{available_port}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nðŸ“š Instructions:\")\n",
    "    print(\"1. Go to the 'ðŸ“¤ Upload Documents' tab\")\n",
    "    print(\"2. Upload your PDF, TXT, MD, or DOCX files\")\n",
    "    print(\"3. Click 'âš™ï¸ Process Documents' and wait for completion\")\n",
    "    print(\"4. Switch to 'ðŸ” Ask Questions' tab to query your documents\")\n",
    "    print(\"\\nâœ¨ Features:\")\n",
    "    print(\"  â€¢ UTF-8 encoding fixed (proper quotes, dashes, special chars)\")\n",
    "    print(\"  â€¢ Markdown rendering (code blocks, bold, bullets, etc.)\")\n",
    "    print(\"  â€¢ Source citations with page numbers\")\n",
    "    print(\"  â€¢ Hybrid search (local + web when enabled)\")\n",
    "    print(\"\\nâœ¨ Your documents will be processed and indexed for semantic search!\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # Note: Skipping .queue() to avoid event loop conflicts in Jupyter\n",
    "    # Queue is not necessary for single-user notebook usage\n",
    "    \n",
    "    # Launch with proper settings for Jupyter environment\n",
    "    gradio_demo.launch(\n",
    "        share=False,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=available_port,\n",
    "        prevent_thread_lock=True,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error launching Gradio demo: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Event loop issue: Restart the Jupyter kernel completely\")\n",
    "    print(\"  2. Port conflict: Check if port is already in use\")\n",
    "    print(\"  3. Ollama not running: Run 'ollama serve' in another terminal\")\n",
    "    print(\"  4. Try running this cell again\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMI6z7mOJ/PqXccZE66QFNn",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01a2f2a66a824f899437692f3ca13f69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0895139526b04670b160028d31a8063b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08ebc7b8c6e949b5a0d63044394e916d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aef1efa6355f4990aec5195dbc5a9028",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5e5f9e8f9164503aeb26e02320355df",
      "value": 35
     }
    },
    "0cae3a718f0f49d6b479db90d157e7fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0eeffecb0555442086fcd31ae35d7802": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1697ecaf9f9b4e3ba933cc2ef5175610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17fbd77f84ab4671ad3f8d224e744deb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18699c0080ff4e6595a777903bcf33eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1efb9bc13eda4bcea490254207b66cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d6881bafd846679ec5a26539e7d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24df1cdf5091488ca4e26d1220139049",
       "IPY_MODEL_08ebc7b8c6e949b5a0d63044394e916d",
       "IPY_MODEL_4aebc8cc2a604a6187267c288c470f35"
      ],
      "layout": "IPY_MODEL_d0903babae274dea9fef8d0519ab6616"
     }
    },
    "2231300d5d2e4480932ad1f4a673ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22a3e9f632f647b18aa168dfcfe67d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35023ce2c2574b3298665271a8180b9b",
       "IPY_MODEL_31b1b0ee74ce47dd8fe3a314531f544a",
       "IPY_MODEL_57d7314717a44a97a9cb6a4464e73246"
      ],
      "layout": "IPY_MODEL_c87b652473434521bc8ba51c5b6f1336"
     }
    },
    "24df1cdf5091488ca4e26d1220139049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aac9fd335e994dce8057fd2f75967868",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1697ecaf9f9b4e3ba933cc2ef5175610",
      "value": "added_tokens.json:â€‡100%"
     }
    },
    "2523e64c82d04b209747d11a7bb6a61b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a15a882dea144c183e4e1fea94f173f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aea41ebe3bd44b62b93f14d1ba372158",
       "IPY_MODEL_82a16bce22b44f0aa85ab1f7bdbad83b",
       "IPY_MODEL_5952598b7fe64118a1f1e86811c13be3"
      ],
      "layout": "IPY_MODEL_d99c7288b6214c6cb9d1a78e52b107e6"
     }
    },
    "302faed2a00648499af9fdbaffb66337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "311b486d01ef47e383a4604ceb744bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_302faed2a00648499af9fdbaffb66337",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3eda1d46746644429404ffbf9f2b061f",
      "value": "config.json:â€‡100%"
     }
    },
    "31b1b0ee74ce47dd8fe3a314531f544a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd09cd8ffac54be3ac64771d8f65545f",
      "max": 1155375,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d40a864131c540049095f912d56281e2",
      "value": 1155375
     }
    },
    "337bf290ff4c4b18b0efcd18514766ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35023ce2c2574b3298665271a8180b9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cd7c86601a24e1d8da0c30e1db2b7cd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bf0a1cd56bdb48289367a085e3ed1c43",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "3cdad51c3c444ccb9a75d0854d4c9190": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cf8b9adc1ad47c0bac7e768c930eaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42149017310c4beba6d9dcefdefbd83a",
      "max": 662,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0895139526b04670b160028d31a8063b",
      "value": 662
     }
    },
    "3eda1d46746644429404ffbf9f2b061f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40a5c72060fa40edb8787295fa12fea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_caa1cfaf3aae4fcaa65c63b1622baa6c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1efb9bc13eda4bcea490254207b66cde",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "42149017310c4beba6d9dcefdefbd83a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485e3b0909f54a769bb13aaed7665452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48954e6e0a9d4114a105752ad7e394f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_311b486d01ef47e383a4604ceb744bc4",
       "IPY_MODEL_7c511b2bd5644cb381db7e9c78fabd4a",
       "IPY_MODEL_dd4e4fea09e2446ca93a1ca67eace0d6"
      ],
      "layout": "IPY_MODEL_5a582624199544178c625095356e26bb"
     }
    },
    "48dda1d3708e4dd5854815eca666c837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6245b3ea77024ed4979d0a71eaa46215",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba60a82bba334f7f9ec6794133eec339",
      "value": 111
     }
    },
    "4aebc8cc2a604a6187267c288c470f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337bf290ff4c4b18b0efcd18514766ae",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d921a0f7e2b2435aaf5ac050d63d3f25",
      "value": "â€‡35.0/35.0â€‡[00:00&lt;00:00,â€‡5.15kB/s]"
     }
    },
    "4cd7c86601a24e1d8da0c30e1db2b7cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d84d79bf71a487daeeb7e9c7775428c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517baab5cb004bcd9369c561a91683ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e4d5e6d3704e9aac24702c44475df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40a5c72060fa40edb8787295fa12fea7",
       "IPY_MODEL_48dda1d3708e4dd5854815eca666c837",
       "IPY_MODEL_eea3c1ca71464bde80a029fe5afcb2a4"
      ],
      "layout": "IPY_MODEL_a88b76fe2d8c4dfd96981c8857a6d83f"
     }
    },
    "57d7314717a44a97a9cb6a4464e73246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d84d79bf71a487daeeb7e9c7775428c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7bef6f5797214c71ba1706f273a4fcef",
      "value": "â€‡1.16M/1.16Mâ€‡[00:00&lt;00:00,â€‡9.08MB/s]"
     }
    },
    "5841774613c64b04807531fa75ef4a20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5952598b7fe64118a1f1e86811c13be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0eeffecb0555442086fcd31ae35d7802",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d10dbbff037c4b0fb530e7aa2afa7c66",
      "value": "â€‡33.4M/33.4Mâ€‡[00:00&lt;00:00,â€‡184MB/s]"
     }
    },
    "5a582624199544178c625095356e26bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a760390ab144c53a819d475ed12c73d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ce1d7140679404ca85df3c4cb7fa104": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6245b3ea77024ed4979d0a71eaa46215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbe4934a6d645f18f6b455f378b3ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ce0e91f5c0f4437b4deb981236c81f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ac0583c668346598a7599783e5e3838",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f27bcde980ec47e8b4af6e6ad8fdde25",
      "value": "â€‡4.69M/4.69Mâ€‡[00:00&lt;00:00,â€‡53.0MB/s]"
     }
    },
    "6d322a48f71b4ca4a8cd2eaebf097dcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70221af3eca3460abc2be20c1ff0d063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "703c7d87c7344c97a94c9d8760d1407d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73f399029ca24033860f364ace8dd5f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bef6f5797214c71ba1706f273a4fcef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c511b2bd5644cb381db7e9c78fabd4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c94f82a5406d4018ad309b412c4fad73",
      "max": 1352,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01a2f2a66a824f899437692f3ca13f69",
      "value": 1352
     }
    },
    "7ed5e404af5f4ca8a79d9a0daa4eac79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d322a48f71b4ca4a8cd2eaebf097dcd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0cae3a718f0f49d6b479db90d157e7fe",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "82a16bce22b44f0aa85ab1f7bdbad83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b9ee6671d6a49288a399ddf0de284cb",
      "max": 33384570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bbe4934a6d645f18f6b455f378b3ed5",
      "value": 33384570
     }
    },
    "848589303f3142d1986ec45fe9121b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cdad51c3c444ccb9a75d0854d4c9190",
      "max": 4689074,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a192ce3120c344a4bfa3777cab26e3f5",
      "value": 4689074
     }
    },
    "87417696ef7d48798b90ac2af01d95bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ce1d7140679404ca85df3c4cb7fa104",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70221af3eca3460abc2be20c1ff0d063",
      "value": "â€‡662/662â€‡[00:00&lt;00:00,â€‡103kB/s]"
     }
    },
    "8ac0583c668346598a7599783e5e3838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c21e3ef70154f1397b4aa211859cda0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99fc93e91820408a90591e8c16c7e08f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac81c321e05d409da7a7f05a06a25ad7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5a760390ab144c53a819d475ed12c73d",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "9b9ee6671d6a49288a399ddf0de284cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a192ce3120c344a4bfa3777cab26e3f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5e5f9e8f9164503aeb26e02320355df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a75b7bcc70f84800bb309217d1d5b982": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ed5e404af5f4ca8a79d9a0daa4eac79",
       "IPY_MODEL_3cf8b9adc1ad47c0bac7e768c930eaa1",
       "IPY_MODEL_87417696ef7d48798b90ac2af01d95bd"
      ],
      "layout": "IPY_MODEL_17fbd77f84ab4671ad3f8d224e744deb"
     }
    },
    "a88b76fe2d8c4dfd96981c8857a6d83f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aac9fd335e994dce8057fd2f75967868": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac81c321e05d409da7a7f05a06a25ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acbe96b270544b7eb13f38643d85b93d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ada07961a40b47249cf126736654adaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae61b454a03743778bc7d48a03040fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99fc93e91820408a90591e8c16c7e08f",
       "IPY_MODEL_cedf7360bd6b4f4a9f523305c911ac51",
       "IPY_MODEL_dc9cd067e0a14611843d1b581a6d452f"
      ],
      "layout": "IPY_MODEL_703c7d87c7344c97a94c9d8760d1407d"
     }
    },
    "aea41ebe3bd44b62b93f14d1ba372158": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73f399029ca24033860f364ace8dd5f3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_517baab5cb004bcd9369c561a91683ee",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "aef1efa6355f4990aec5195dbc5a9028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f4c9b4a4b647d68291e0957a2bf8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f36a2f035f77468e84592829eb17a536",
       "IPY_MODEL_848589303f3142d1986ec45fe9121b1c",
       "IPY_MODEL_6ce0e91f5c0f4437b4deb981236c81f4"
      ],
      "layout": "IPY_MODEL_dc515b7b37c94b05aefec47540554339"
     }
    },
    "ba60a82bba334f7f9ec6794133eec339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf0a1cd56bdb48289367a085e3ed1c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3126ba8df3f4c8a995470f5c52503b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c87b652473434521bc8ba51c5b6f1336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c94f82a5406d4018ad309b412c4fad73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caa1cfaf3aae4fcaa65c63b1622baa6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cedf7360bd6b4f4a9f523305c911ac51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2523e64c82d04b209747d11a7bb6a61b",
      "max": 536223056,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_acbe96b270544b7eb13f38643d85b93d",
      "value": 536223056
     }
    },
    "d0903babae274dea9fef8d0519ab6616": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d10dbbff037c4b0fb530e7aa2afa7c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d40a864131c540049095f912d56281e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d921a0f7e2b2435aaf5ac050d63d3f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d99c7288b6214c6cb9d1a78e52b107e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc515b7b37c94b05aefec47540554339": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc9cd067e0a14611843d1b581a6d452f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3126ba8df3f4c8a995470f5c52503b7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f46894634dc84e6da17c4f4a5192f65d",
      "value": "â€‡536M/536Mâ€‡[00:02&lt;00:00,â€‡283MB/s]"
     }
    },
    "dd4e4fea09e2446ca93a1ca67eace0d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5841774613c64b04807531fa75ef4a20",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_18699c0080ff4e6595a777903bcf33eb",
      "value": "â€‡1.35k/1.35kâ€‡[00:00&lt;00:00,â€‡193kB/s]"
     }
    },
    "eea3c1ca71464bde80a029fe5afcb2a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c21e3ef70154f1397b4aa211859cda0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_485e3b0909f54a769bb13aaed7665452",
      "value": "â€‡111/111â€‡[00:00&lt;00:00,â€‡17.0kB/s]"
     }
    },
    "f27bcde980ec47e8b4af6e6ad8fdde25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f36a2f035f77468e84592829eb17a536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2231300d5d2e4480932ad1f4a673ccfc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ada07961a40b47249cf126736654adaf",
      "value": "tokenizer.model:â€‡100%"
     }
    },
    "f46894634dc84e6da17c4f4a5192f65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd09cd8ffac54be3ac64771d8f65545f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
