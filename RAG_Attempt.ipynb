{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved RAG Pipeline\n",
    "\n",
    "## Key Features:\n",
    "1. **Fast API Embeddings**: Support for HuggingFace, Voyage AI, OpenAI, or local FastEmbed\n",
    "2. **Semantic Chunking**: Respects sentence boundaries with configurable overlap\n",
    "3. **PyMuPDF**: Faster PDF processing with accurate page number tracking\n",
    "4. **Cosine Similarity**: Uses IndexFlatIP for better semantic search\n",
    "5. **Persistence**: Save/load vector store and metadata to avoid re-processing\n",
    "6. **Progress Tracking**: tqdm progress bars for long operations\n",
    "\n",
    "## Quick Start:\n",
    "Set environment variables for API embeddings (optional but recommended):\n",
    "```bash\n",
    "# Option 1: HuggingFace (Free with rate limits)\n",
    "export EMBEDDING_METHOD=\"huggingface\"\n",
    "export HF_TOKEN=\"hf_your_token_here\"\n",
    "\n",
    "# Option 2: Voyage AI (Fast, paid)\n",
    "export EMBEDDING_METHOD=\"voyage\"\n",
    "export VOYAGE_API_KEY=\"pa-your-key-here\"\n",
    "\n",
    "# Option 3: OpenAI (Fast, paid)\n",
    "export EMBEDDING_METHOD=\"openai\"\n",
    "export OPENAI_API_KEY=\"sk-your-key-here\"\n",
    "\n",
    "# Option 4: Local FastEmbed (Slow, free, private)\n",
    "# No setup needed - just don't set any API keys\n",
    "```\n",
    "\n",
    "Then run the pipeline cells in order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from .env:\n",
      "  ✓ OLLAMA_API_KEY present: True\n",
      "  ✓ HF_TOKEN present: True\n",
      "  ✓ OPENAI_API_KEY present: True\n",
      "  ✓ VOYAGE_API_KEY present: False\n",
      "\n",
      "If any required keys show False, check your .env file and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ IMPORTANT: Run this cell FIRST to load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file into the kernel\n",
    "load_dotenv(override=False)  # Won't overwrite existing environment variables\n",
    "\n",
    "# Verify key environment variables are loaded (without printing secrets)\n",
    "print(\"Environment variables loaded from .env:\")\n",
    "print(f\"  ✓ OLLAMA_API_KEY present: {bool(os.environ.get('OLLAMA_API_KEY'))}\")\n",
    "print(f\"  ✓ HF_TOKEN present: {bool(os.environ.get('HF_TOKEN'))}\")\n",
    "print(f\"  ✓ OPENAI_API_KEY present: {bool(os.environ.get('OPENAI_API_KEY'))}\")\n",
    "print(f\"  ✓ VOYAGE_API_KEY present: {bool(os.environ.get('VOYAGE_API_KEY'))}\")\n",
    "print(\"\\nIf any required keys show False, check your .env file and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757884427770,
     "user": {
      "displayName": "Aditya Gupta",
      "userId": "04884539656118443680"
     },
     "user_tz": -60
    },
    "id": "nOhF9lXU4Xr1",
    "outputId": "c59e3d1f-4600-4592-8523-c0069cb73559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBasic RAG pipeline Implementation:\\n\\nDocuments -> {Chunking / Tokenization -> Convert to Embeddings -> Store Metadata + Chunks in Datastore}\\nQuery -> {Convert to Embedding -> Check for Top K similar documents based on cosine similarity (or other criteria) to query -> Pass Documents to LLM} -> Final Answer with inline citations based on the documents\\n\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG pipeline Implementation:\n",
    "\n",
    "Documents -> {Chunking / Tokenization -> Convert to Embeddings -> Store Metadata + Chunks in Datastore}\n",
    "Query -> {Convert to Embedding -> Check for Top K similar documents based on cosine similarity (or other criteria) to query -> Pass Documents to LLM} -> Final Answer with inline citations based on the documents\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8419fc1"
   },
   "source": [
    "# Task\n",
    "Generate a Python template for a RAG pipeline using local Ollama models (gemma3:270m, smollm2:135m, smollm2:360m) for answer generation and a local embedding model (fastembed). Implement the logic for document loading (pdf, md, txt, blobs, docs), chunking (library-based and manual), metadata storage and indexing, embedding generation, vector storage, retrieval, and answer generation. Include necessary library installations and imports, and a simple test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a06a7998"
   },
   "source": [
    "## Environment setup and model loading\n",
    "\n",
    "### Subtask:\n",
    "Install necessary libraries and configure local models via Ollama (gemma3:270m, smollm2:135m/360m). Use fastembed for embeddings to keep everything CPU-only and local.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99414757"
   },
   "source": [
    "**Reasoning**:\n",
    "Install the required libraries for the RAG pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: gpt-oss:20b-cloud\n",
      "OLLAMA_API_KEY configured: True\n",
      "Using HuggingFace Inference API with BAAI/bge-base-en-v1.5 (768-dim)\n",
      "Ollama client ready; embedding system initialized.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from fastembed import TextEmbedding\n",
    "from ollama import Client\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "# API-based embeddings (faster alternatives)\n",
    "try:\n",
    "    import voyageai\n",
    "    VOYAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VOYAGE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    HF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HF_AVAILABLE = False\n",
    "\n",
    "# Configuration: Choose embedding method\n",
    "EMBEDDING_METHOD = os.getenv(\"EMBEDDING_METHOD\", \"huggingface\")  # Options: \"voyage\", \"openai\", \"huggingface\", \"fastembed\"\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\", None)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
    "\n",
    "# Local LLMs via Ollama\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "OLLAMA_MODEL_CLOUD = os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\")\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\", None)  # Required for cloud models\n",
    "client = Client(host=OLLAMA_BASE_URL)\n",
    "print(f\"Using Ollama model: {OLLAMA_MODEL_CLOUD}\")\n",
    "print(f\"OLLAMA_API_KEY configured: {bool(OLLAMA_API_KEY)}\")\n",
    "\n",
    "# Initialize embedding client based on chosen method\n",
    "if EMBEDDING_METHOD == \"voyage\" and VOYAGE_AVAILABLE and VOYAGE_API_KEY:\n",
    "    voyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
    "    EMBEDDING_DIM = 1024  # voyage-2 dimension\n",
    "    print(\"Using Voyage AI embeddings (1024-dim)\")\n",
    "elif EMBEDDING_METHOD == \"openai\" and OPENAI_AVAILABLE and OPENAI_API_KEY:\n",
    "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    EMBEDDING_DIM = 1536  # text-embedding-3-small dimension\n",
    "    print(\"Using OpenAI embeddings (1536-dim)\")\n",
    "elif EMBEDDING_METHOD == \"huggingface\" and HF_AVAILABLE and HF_TOKEN:\n",
    "    hf_client = InferenceClient(provider=\"hf-inference\", api_key=HF_TOKEN)\n",
    "    HF_MODEL = os.getenv(\"HF_EMBEDDING_MODEL\", \"BAAI/bge-base-en-v1.5\")\n",
    "    EMBEDDING_DIM = 768  # bge-base dimension\n",
    "    print(f\"Using HuggingFace Inference API with {HF_MODEL} (768-dim)\")\n",
    "else:\n",
    "    # Fallback to local fastembed\n",
    "    embedder = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    EMBEDDING_DIM = 384\n",
    "    EMBEDDING_METHOD = \"fastembed\"\n",
    "    print(\"Using local FastEmbed (384-dim) - Set API keys for faster embeddings\")\n",
    "    print(\"  Options: VOYAGE_API_KEY, OPENAI_API_KEY, or HF_TOKEN\")\n",
    "\n",
    "print(\"Ollama client ready; embedding system initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding optimization layer loaded (caching, parallel, local support)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== EMBEDDING OPTIMIZATION LAYER =====\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"\n",
    "    File-based cache for embeddings using content hash as key.\n",
    "    Dramatically speeds up repeated queries on same documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir: str = \".embedding_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.memory_cache = {}  # In-memory cache for current session\n",
    "    \n",
    "    def _hash_text(self, text: str) -> str:\n",
    "        \"\"\"Generate hash key from text content.\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Retrieve embedding from cache if exists.\"\"\"\n",
    "        key = self._hash_text(text)\n",
    "        \n",
    "        # Check memory cache first (faster)\n",
    "        if key in self.memory_cache:\n",
    "            return self.memory_cache[key]\n",
    "        \n",
    "        # Check disk cache\n",
    "        cache_file = self.cache_dir / f\"{key}.npy\"\n",
    "        if cache_file.exists():\n",
    "            embedding = np.load(cache_file)\n",
    "            self.memory_cache[key] = embedding  # Cache in memory for this session\n",
    "            return embedding\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def put(self, text: str, embedding: np.ndarray):\n",
    "        \"\"\"Store embedding in both memory and disk cache.\"\"\"\n",
    "        key = self._hash_text(text)\n",
    "        self.memory_cache[key] = embedding\n",
    "        \n",
    "        # Save to disk\n",
    "        cache_file = self.cache_dir / f\"{key}.npy\"\n",
    "        np.save(cache_file, embedding)\n",
    "\n",
    "class ParallelHFEmbedder:\n",
    "    \"\"\"\n",
    "    HuggingFace embeddings with parallel processing (4 workers).\n",
    "    ~5-10x faster than sequential calls for many texts.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str, num_workers: int = 4):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "        self.num_workers = num_workers\n",
    "        self.client = InferenceClient(provider=\"hf-inference\", api_key=api_key)\n",
    "    \n",
    "    def embed_batch(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"Embed multiple texts in parallel.\"\"\"\n",
    "        results = [None] * len(texts)\n",
    "        lock = threading.Lock()\n",
    "        \n",
    "        def embed_single(idx: int, text: str):\n",
    "            try:\n",
    "                embedding = self.client.feature_extraction(text, model=self.model)\n",
    "                embedding = np.array(embedding, dtype=\"float32\")\n",
    "                if embedding.ndim > 1:\n",
    "                    embedding = embedding[0]\n",
    "                with lock:\n",
    "                    results[idx] = embedding\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding text {idx}: {e}\")\n",
    "                with lock:\n",
    "                    results[idx] = np.zeros(768, dtype=\"float32\")  # Fallback\n",
    "        \n",
    "        # Use thread pool for parallel requests\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            futures = []\n",
    "            for idx, text in enumerate(texts):\n",
    "                future = executor.submit(embed_single, idx, text)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Wait for all to complete\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "        \n",
    "        return np.array([r for r in results if r is not None], dtype=\"float32\")\n",
    "\n",
    "class LocalFastEmbedder:\n",
    "    \"\"\"\n",
    "    Local sentence-transformers embeddings with batch processing.\n",
    "    Faster than API calls but requires local GPU/CPU.\n",
    "    ~30-60 seconds for 660 chunks on CPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"⚠️  sentence-transformers not available. Install with: pip install sentence-transformers\")\n",
    "            self.available = False\n",
    "    \n",
    "    def embed_batch(self, texts: list[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Embed texts using local sentence-transformers.\"\"\"\n",
    "        if not self.available:\n",
    "            return None\n",
    "        \n",
    "        embeddings = self.model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "        return np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(\"✓ Embedding optimization layer loaded (caching, parallel, local support)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimized embedding function ready (caching, parallel, local support)\n"
     ]
    }
   ],
   "source": [
    "    # Embed uncached texts\n",
    "    if use_parallel and EMBEDDING_METHOD == \"huggingface\" and HF_AVAILABLE and HF_TOKEN:\n",
    "        print(f\"Embedding {len(uncached_texts)} texts using parallel HuggingFace (4 workers)...\")\n",
    "        parallel_embedder = ParallelHFEmbedder(HF_MODEL, HF_TOKEN, num_workers=4)\n",
    "        new_embeddings = parallel_embedder.embed_batch(uncached_texts)\n",
    "    else:\n",
    "        # Sequential embedding (with all methods supported)\n",
    "        print(f\"Embedding {len(uncached_texts)} texts...\")\n",
    "        new_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(uncached_texts), batch_size), desc=\"Embedding\", disable=len(uncached_texts) < batch_size):\n",
    "            batch = uncached_texts[i:i + batch_size]\n",
    "            \n",
    "            if EMBEDDING_METHOD == \"voyage\":\n",
    "                result = voyage_client.embed(batch, model=\"voyage-2\", input_type=\"document\")\n",
    "                batch_embeddings = np.array(result.embeddings, dtype=\"float32\")\n",
    "            elif EMBEDDING_METHOD == \"openai\":\n",
    "                result = openai_client.embeddings.create(input=batch, model=\"text-embedding-3-small\")\n",
    "                batch_embeddings = np.array([e.embedding for e in result.data], dtype=\"float32\")\n",
    "            elif EMBEDDING_METHOD == \"huggingface\":\n",
    "                batch_embeddings = []\n",
    "                for text in batch:\n",
    "                    result = hf_client.feature_extraction(text, model=HF_MODEL)\n",
    "                    embedding = np.array(result, dtype=\"float32\")\n",
    "                    if embedding.ndim > 1:\n",
    "                        embedding = embedding[0]\n",
    "                    batch_embeddings.append(embedding)\n",
    "                batch_embeddings = np.array(batch_embeddings, dtype=\"float32\")\n",
    "            else:  # fastembed\n",
    "                vecs = list(embedder.embed(batch))\n",
    "                batch_embeddings = np.array([np.asarray(v, dtype=\"float32\") for v in vecs])\n",
    "            \n",
    "            new_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        new_embeddings = np.vstack(new_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7225b901"
   },
   "source": [
    "## Document Loading and Preprocessing\n",
    "\n",
    "### Subtask:\n",
    "Implement document loading for various file types (pdf, md, txt, blobs, docs) and perform basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "9fac4e7a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_document(file_path: str):\n",
    "    \"\"\"Loads content from various document types.\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.pdf':\n",
    "        return load_pdf(file_path)\n",
    "    elif file_extension == '.md':\n",
    "        return load_text(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        return load_text(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return load_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "def load_pdf(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Loads text from a PDF file using PyMuPDF (faster than pypdf).\n",
    "    Returns list of (text, page_number) tuples to preserve page information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        pages_data = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Clean up text: remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            if text.strip():  # Only add non-empty pages\n",
    "                pages_data.append((text, page_num + 1))  # 1-indexed page numbers\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"Loaded {len(pages_data)} pages from PDF\")\n",
    "        return pages_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_text(file_path: str) -> str:\n",
    "    \"\"\"Loads text from a plain text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_docx(file_path: str) -> str:\n",
    "    \"\"\"Loads text from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DOCX file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a02d1a5c"
   },
   "source": [
    "## Chunking Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement document chunking using a library and manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "d60b9727"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def chunk_document_semantic(\n",
    "    document_text: str, \n",
    "    chunk_size: int = 1000, \n",
    "    overlap: int = 200,\n",
    "    page_number: int = None\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Splits a document into semantic chunks that respect sentence boundaries.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The text to chunk\n",
    "        chunk_size: Target size for each chunk (in characters)\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "        page_number: Page number for this text (if from PDF)\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, page_number) tuples\n",
    "    \"\"\"\n",
    "    # Split into sentences (handles common sentence endings)\n",
    "    sentence_endings = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
    "    sentences = sentence_endings.split(document_text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If adding this sentence keeps us under chunk_size, add it\n",
    "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            # Save current chunk if it's not empty\n",
    "            if current_chunk.strip():\n",
    "                chunks.append((current_chunk.strip(), page_number))\n",
    "            \n",
    "            # Start new chunk with overlap from previous chunk\n",
    "            if overlap > 0 and len(current_chunk) > overlap:\n",
    "                # Take last 'overlap' characters, but try to start at sentence boundary\n",
    "                overlap_text = current_chunk[-overlap:]\n",
    "                # Find the first sentence start in the overlap\n",
    "                first_sentence_start = overlap_text.find('. ')\n",
    "                if first_sentence_start != -1:\n",
    "                    overlap_text = overlap_text[first_sentence_start + 2:]\n",
    "                current_chunk = overlap_text + sentence + \" \"\n",
    "            else:\n",
    "                current_chunk = sentence + \" \"\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append((current_chunk.strip(), page_number))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_pdf_pages(pages_data: List[Tuple[str, int]], chunk_size: int = 1000, overlap: int = 200):\n",
    "    \"\"\"\n",
    "    Chunks PDF pages while preserving page number information.\n",
    "    \n",
    "    Args:\n",
    "        pages_data: List of (text, page_number) tuples from load_pdf\n",
    "        chunk_size: Target chunk size\n",
    "        overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunk_text, page_number) tuples\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for page_text, page_num in tqdm(pages_data, desc=\"Chunking pages\"):\n",
    "        page_chunks = chunk_document_semantic(page_text, chunk_size, overlap, page_num)\n",
    "        all_chunks.extend(page_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def chunk_document_manual(document_text: str, max_chars_per_chunk: int = 1000):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Simple character-based chunking (kept for backward compatibility).\n",
    "    Use chunk_document_semantic() instead for better results.\n",
    "    \"\"\"\n",
    "    print(\"⚠️  Warning: Using deprecated simple chunking. Consider using chunk_document_semantic()\")\n",
    "    chunks = []\n",
    "    for i in range(0, len(document_text), max_chars_per_chunk):\n",
    "        chunks.append(document_text[i:i + max_chars_per_chunk])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf19424e"
   },
   "source": [
    "## Metadata Storage and Indexing\n",
    "\n",
    "### Subtask:\n",
    "Implement a way to store metadata associated with chunks and index them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "df6a37e1"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class DocumentChunk:\n",
    "    def __init__(self, text: str, source: str, page_number: int = None, chunk_id: str = None):\n",
    "        self.chunk_id = chunk_id if chunk_id is not None else str(uuid.uuid4())\n",
    "        self.text = text\n",
    "        self.source = source\n",
    "        self.page_number = page_number\n",
    "        self.embedding = None  # To be filled later\n",
    "\n",
    "class MetadataStore:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}  # Stores chunks with chunk_id as key\n",
    "\n",
    "    def add_chunk(self, chunk: DocumentChunk):\n",
    "        self.chunks[chunk.chunk_id] = chunk\n",
    "\n",
    "    def get_chunk(self, chunk_id: str) -> DocumentChunk:\n",
    "        return self.chunks.get(chunk_id)\n",
    "\n",
    "    def get_all_chunks(self) -> list[DocumentChunk]:\n",
    "        return list(self.chunks.values())\n",
    "\n",
    "    def index_chunks(self):\n",
    "        \"\"\"\n",
    "        Placeholder for more sophisticated indexing.\n",
    "        Currently chunks are indexed by chunk_id in the dictionary.\n",
    "        \"\"\"\n",
    "        print(f\"Indexed {len(self.chunks)} chunks in metadata store.\")\n",
    "\n",
    "    def save(self, save_path: str):\n",
    "        \"\"\"Saves metadata store to disk.\"\"\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        print(f\"Metadata store saved to {save_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_path: str, verbose: bool = True):\n",
    "        \"\"\"Loads metadata store from disk.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Path to metadata file\n",
    "            verbose: If True, print loading messages. If False, load silently.\n",
    "        \"\"\"\n",
    "        instance = cls()\n",
    "        with open(save_path, 'rb') as f:\n",
    "            instance.chunks = pickle.load(f)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Metadata store loaded from {save_path} ({len(instance.chunks)} chunks)\")\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76653d93"
   },
   "source": [
    "## Embedding Generation\n",
    "\n",
    "### Subtask:\n",
    "Implement embedding generation for document chunks using the selected embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07de56df"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(chunks: list[DocumentChunk], batch_size: int = 100, use_optimizations: bool = True):\n",
    "    \"\"\"\n",
    "    Generates embeddings for chunks using configured method with optimizations.\n",
    "    \n",
    "    Optimizations include:\n",
    "    - Caching: File-based cache for repeated texts (10x+ speedup)\n",
    "    - Parallelization: Thread pool for HuggingFace API calls (5-10x speedup)\n",
    "    - Local fast: Sentence-transformers batch processing (fastest, no API)\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of DocumentChunk objects to embed\n",
    "        batch_size: Batch size for processing\n",
    "        use_optimizations: Whether to enable caching, parallel, and local embeddings\n",
    "    \"\"\"\n",
    "    texts = [chunk.text for chunk in chunks]\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} chunks using optimized {EMBEDDING_METHOD}...\")\n",
    "    print(f\"  Options: caching (10x+), parallel HF (5-10x), local fast embedding\")\n",
    "    embeddings = embed_texts_optimized(\n",
    "        texts, \n",
    "        batch_size=batch_size,\n",
    "        use_cache=use_optimizations,           # Enable caching\n",
    "        use_parallel=use_optimizations,        # Enable parallelization\n",
    "        use_local_fast=False                   # Keep False for API methods, True for local\n",
    "    )\n",
    "    \n",
    "    # Assign embeddings to chunks\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk.embedding = embedding\n",
    "    \n",
    "    print(f\"✓ Generated {len(embeddings)} embeddings ({EMBEDDING_DIM}-dimensional)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "083b2839"
   },
   "source": [
    "## Vector Store Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement a vector store for efficient similarity search. Using FAISS for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "e9ebab80"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, dimension: int):\n",
    "        \"\"\"\n",
    "        Initialize a Faiss index using Inner Product (cosine similarity after normalization).\n",
    "        IndexFlatIP is better for embeddings than IndexFlatL2.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine similarity with normalized vectors\n",
    "        self.chunk_ids = []  # Maps Faiss index to chunk IDs\n",
    "\n",
    "    def add_vectors(self, embeddings: np.ndarray, chunk_ids: list[str]):\n",
    "        \"\"\"\n",
    "        Adds embeddings to the Faiss index after L2 normalization.\n",
    "        Normalization converts inner product to cosine similarity.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunk_ids.extend(chunk_ids)\n",
    "        print(f\"Added {len(chunk_ids)} vectors. Total: {self.index.ntotal}\")\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5):\n",
    "        \"\"\"\n",
    "        Searches for the nearest neighbors using cosine similarity.\n",
    "        Returns list of chunk IDs sorted by relevance.\n",
    "        \"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search (higher scores = more similar for IP)\n",
    "        distances, indices = self.index.search(query_embedding, min(k, self.index.ntotal))\n",
    "\n",
    "        # Retrieve corresponding chunk IDs\n",
    "        results = []\n",
    "        for i in indices[0]:\n",
    "            if i != -1 and i < len(self.chunk_ids):\n",
    "                results.append(self.chunk_ids[i])\n",
    "        return results\n",
    "\n",
    "    def save(self, save_dir: str):\n",
    "        \"\"\"Saves the vector store to disk.\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        index_path = os.path.join(save_dir, \"faiss_index.bin\")\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        \n",
    "        # Save chunk IDs and metadata\n",
    "        metadata_path = os.path.join(save_dir, \"chunk_ids.pkl\")\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunk_ids': self.chunk_ids,\n",
    "                'dimension': self.dimension\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Vector store saved to {save_dir}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, save_dir: str, verbose: bool = True):\n",
    "        \"\"\"Loads the vector store from disk.\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory containing saved vector store\n",
    "            verbose: If True, print loading messages. If False, load silently.\n",
    "        \"\"\"\n",
    "        index_path = os.path.join(save_dir, \"faiss_index.bin\")\n",
    "        metadata_path = os.path.join(save_dir, \"chunk_ids.pkl\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        # Create instance and load index\n",
    "        instance = cls(metadata['dimension'])\n",
    "        instance.index = faiss.read_index(index_path)\n",
    "        instance.chunk_ids = metadata['chunk_ids']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Vector store loaded from {save_dir} ({instance.index.ntotal} vectors)\")\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5db4c1f0"
   },
   "source": [
    "## Retrieval Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement the retrieval of relevant document chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "d04722c4"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, vector_store: VectorStore, metadata_store: MetadataStore, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top K most relevant document chunks for a given query using fastembed for the query embedding.\n",
    "    \"\"\"\n",
    "    # 1. Generate embedding for the query (fastembed)\n",
    "    qv = embed_texts([query])\n",
    "    # 2. Search the vector store for similar chunks\n",
    "    retrieved_chunk_ids = vector_store.search(qv[0], k=k)\n",
    "    # 3. Retrieve the actual chunk objects from the metadata store\n",
    "    retrieved_chunks = [metadata_store.get_chunk(chunk_id) for chunk_id in retrieved_chunk_ids if metadata_store.get_chunk(chunk_id) is not None]\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27806b7b"
   },
   "source": [
    "## Answer Generation Implementation\n",
    "\n",
    "### Subtask:\n",
    "Implement answer generation using a language model and the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "ollama_answer_cell"
   },
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise assistant. Answer ONLY using the provided sources.\\n\"\n",
    "    \"Cite evidence with bracketed indices like [1], [2]. If unsure, say you don't know.\\n\"\n",
    "    \"Keep it concise: 3-6 sentences.\"\n",
    ")\n",
    "\n",
    "# Token limit configuration for LLM responses\n",
    "# Base tokens for the answer, plus additional tokens per source\n",
    "TOKEN_LIMIT_BASE = 512          # Minimum tokens for answer generation\n",
    "TOKEN_LIMIT_PER_SOURCE = 200    # Additional tokens per source (for citations, context)\n",
    "TOKEN_LIMIT_MAX = 2048          # Absolute maximum to prevent runaway responses\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://127.0.0.1:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "OLLAMA_MODEL_CLOUD = os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\")\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\", None)\n",
    "\n",
    "try:\n",
    "    client\n",
    "except NameError:\n",
    "    client = Client(host=OLLAMA_BASE_URL)\n",
    "\n",
    "\n",
    "def generate_answer(query: str, retrieved_chunks: list, model_name: str = None, stream: bool = False):\n",
    "    \"\"\"\n",
    "    Generate an answer from Ollama or Ollama cloud, with defensive checks.\n",
    "\n",
    "    - Handles None chunk.text\n",
    "    - Validates cloud API key before using cloud model\n",
    "    - Uses == for string comparison\n",
    "    - Supports stream flag if the client and model support streaming\n",
    "    - Dynamically adjusts token limit based on number of sources\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\")\n",
    "\n",
    "    # Build context lines safely\n",
    "    context_lines = []\n",
    "    for i, chunk in enumerate(retrieved_chunks, start=1):\n",
    "        src = getattr(chunk, \"source\", f\"doc_{i}\")\n",
    "        # ensure chunk.text is a string\n",
    "        chunk_text_raw = getattr(chunk, \"text\", \"\")\n",
    "        if chunk_text_raw is None:\n",
    "            chunk_text = \"\"\n",
    "        else:\n",
    "            chunk_text = str(chunk_text_raw)\n",
    "        # trim to avoid sending too much\n",
    "        context_lines.append(f\"[{i}] {src}:\\n{chunk_text[:800]}\")\n",
    "\n",
    "    user = f\"Question: {query}\\n\\nSources:\\n\" + \"\\n\\n\".join(context_lines) + \"\\n\\nAnswer:\"\n",
    "\n",
    "    # Calculate dynamic token limit based on number of sources\n",
    "    num_sources = len(retrieved_chunks)\n",
    "    num_predict = min(\n",
    "        TOKEN_LIMIT_BASE + (num_sources * TOKEN_LIMIT_PER_SOURCE),\n",
    "        TOKEN_LIMIT_MAX\n",
    "    )\n",
    "    \n",
    "    print(f\"[LLM Config] Sources: {num_sources} | Max tokens: {num_predict}\")\n",
    "\n",
    "    # Decide which client to use\n",
    "    # If using cloud model name, ensure API key exists\n",
    "    if model_name == OLLAMA_MODEL_CLOUD:\n",
    "        if not OLLAMA_API_KEY:\n",
    "            raise ValueError(\"OLLAMA_API_KEY is not set but cloud model was requested. Set OLLAMA_API_KEY or use the local model.\")\n",
    "        # create a client configured for cloud (keep default client for local)\n",
    "        cloud_client = Client(host=\"https://ollama.com\", headers={\"Authorization\": \"Bearer \" + OLLAMA_API_KEY})\n",
    "        chosen_client = cloud_client\n",
    "    else:\n",
    "        chosen_client = client\n",
    "\n",
    "    # Prepare messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "    # Try streaming if requested and the client supports it\n",
    "    try:\n",
    "        if stream:\n",
    "            # Some clients support streaming via `stream=True` or a stream() method\n",
    "            # We'll attempt a streaming call but fall back to non-streaming safely\n",
    "            resp_stream = chosen_client.chat(model=model_name, messages=messages, stream=True, options={\"temperature\": 0.2, \"num_predict\": num_predict})\n",
    "            # If resp_stream is an iterator of chunks/dicts, iterate and concatenate\n",
    "            full_text = \"\"\n",
    "            try:\n",
    "                for part in resp_stream:\n",
    "                    # Some streaming responses yield dicts with 'message' -> 'content'\n",
    "                    if isinstance(part, dict) and \"message\" in part and isinstance(part[\"message\"], dict):\n",
    "                        delta = part[\"message\"].get(\"content\", \"\")\n",
    "                    elif isinstance(part, str):\n",
    "                        delta = part\n",
    "                    else:\n",
    "                        delta = \"\"\n",
    "                    print(delta, end=\"\", flush=True)\n",
    "                    full_text += delta\n",
    "                print()\n",
    "                return full_text.strip()\n",
    "            except TypeError:\n",
    "                # Not iterable; fall back\n",
    "                pass\n",
    "\n",
    "        # Non-streaming call (or fallback)\n",
    "        resp = chosen_client.chat(model=model_name, messages=messages, options={\"temperature\": 0.2, \"num_predict\": num_predict})\n",
    "        # resp may be a dict like {'message': {'content': '...'}}\n",
    "        if isinstance(resp, dict):\n",
    "            message = resp.get(\"message\")\n",
    "            if isinstance(message, dict):\n",
    "                return message.get(\"content\", \"\").strip()\n",
    "            # sometimes response might be {'content': '...'}\n",
    "            return resp.get(\"content\", \"\").strip()\n",
    "\n",
    "        # If resp is a string, return it\n",
    "        if isinstance(resp, str):\n",
    "            return resp.strip()\n",
    "\n",
    "        # Unknown shape\n",
    "        return str(resp)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Don't raise raw exceptions to the user; return a helpful message\n",
    "        return f\"Error generating answer: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Formatted answer function loaded (with [1], [2] citations and sources)\n"
     ]
    }
   ],
   "source": [
    "def format_answer_with_sources(answer_text: str, retrieved_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Format answer with source citations [1], [2], etc. and source details below.\n",
    "    \n",
    "    Example output:\n",
    "    ---\n",
    "    The Four Laws of Behavior Change are cue, craving, response, and reward [1]. \n",
    "    Each law builds on the previous one to form a complete habit loop [2].\n",
    "    \n",
    "    📚 SOURCES:\n",
    "    [1] Atomic_Habits_James_Clear.pdf (Page 42):\n",
    "        \"The four laws of behavior change are the cue, the craving, the response...\"\n",
    "    \n",
    "    [2] Atomic_Habits_James_Clear.pdf (Page 45):\n",
    "        \"Together, these four elements form the habit loop which is the...\"\n",
    "    ---\n",
    "    \"\"\"\n",
    "    if not retrieved_chunks or not answer_text.strip():\n",
    "        return answer_text\n",
    "    \n",
    "    # Format source information\n",
    "    sources_section = \"\\n📚 SOURCES:\\n\"\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        source_name = getattr(chunk, \"source\", f\"Document {i}\")\n",
    "        page_num = getattr(chunk, \"page_number\", None)\n",
    "        chunk_text = getattr(chunk, \"text\", \"\")\n",
    "        \n",
    "        # Safely handle None text\n",
    "        if chunk_text is None:\n",
    "            chunk_text = \"(No text content)\"\n",
    "        else:\n",
    "            chunk_text = str(chunk_text)\n",
    "        \n",
    "        # Truncate text snippet to 200 characters for readability\n",
    "        snippet = chunk_text[:200].replace(\"\\n\", \" \")\n",
    "        if len(chunk_text) > 200:\n",
    "            snippet += \"...\"\n",
    "        \n",
    "        # Format source entry\n",
    "        page_info = f\" (Page {page_num})\" if page_num else \"\"\n",
    "        sources_section += f\"\\n[{i}] {source_name}{page_info}:\\n\"\n",
    "        sources_section += f'    \"{snippet}\"\\n'\n",
    "    \n",
    "    # Combine answer with sources\n",
    "    formatted = f\"{answer_text}\\n{sources_section}\"\n",
    "    return formatted\n",
    "\n",
    "print(\"✓ Formatted answer function loaded (with [1], [2] citations and sources)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reusable helper functions loaded (extract_content, execute_query, display_result)\n",
      "✓ Configuration centralized in DOCUMENT_CONFIG, MODEL_CONFIG, and TOKEN_CONFIG\n"
     ]
    }
   ],
   "source": [
    "# ===== CENTRALIZED CONFIGURATION =====\n",
    "\n",
    "import re\n",
    "\n",
    "# Document & Processing Config\n",
    "DOCUMENT_CONFIG = {\n",
    "    \"sample_path\": \"/home/agupta/Documents/Books/Atomic_Habits_James_Clear.pdf\",\n",
    "    \"save_dir\": \"./rag_data\",\n",
    "    \"chunk_size\": 1000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"top_k\": 5,\n",
    "}\n",
    "\n",
    "# Model Config\n",
    "MODEL_CONFIG = {\n",
    "    \"ollama_local\": os.getenv(\"OLLAMA_MODEL\", \"smollm2:360m\"),\n",
    "    \"ollama_cloud\": os.getenv(\"OLLAMA_MODEL_CLOUD\", \"gpt-oss:20b-cloud\"),\n",
    "    \"api_key\": os.getenv(\"OLLAMA_API_KEY\", None),\n",
    "}\n",
    "\n",
    "# LLM Response Token Limits\n",
    "# These control how long the LLM can generate responses\n",
    "# More sources = more tokens allowed (for proper citations)\n",
    "# Formula: max_tokens = min(BASE + (num_sources * PER_SOURCE), MAX)\n",
    "TOKEN_CONFIG = {\n",
    "    \"base\": 512,           # Minimum tokens for answer generation (was 256, now 512)\n",
    "    \"per_source\": 200,     # Additional tokens per source for citations (~3-5 per citation)\n",
    "    \"max\": 2048,           # Absolute maximum to prevent runaway responses\n",
    "}\n",
    "\n",
    "# Select model: prefer cloud if API key available, else local\n",
    "DEFAULT_OLLAMA_MODEL = MODEL_CONFIG[\"ollama_cloud\"] if MODEL_CONFIG[\"api_key\"] else MODEL_CONFIG[\"ollama_local\"]\n",
    "\n",
    "# ===== REUSABLE HELPER FUNCTIONS =====\n",
    "\n",
    "def extract_content(resp):\n",
    "    \"\"\"\n",
    "    Extract answer text from various response formats (dict, string, JSON).\n",
    "    Handles responses from local Ollama, cloud Ollama, and API models.\n",
    "    \"\"\"\n",
    "    if resp is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Direct dict-like response\n",
    "    if isinstance(resp, dict):\n",
    "        msg = resp.get(\"message\") or resp.get(\"content\")\n",
    "        if isinstance(msg, dict):\n",
    "            return msg.get(\"content\", \"\").strip()\n",
    "        if isinstance(msg, str):\n",
    "            return msg.strip()\n",
    "        return str(resp).strip()\n",
    "    \n",
    "    # If it's not a string, stringify it\n",
    "    if not isinstance(resp, str):\n",
    "        return str(resp).strip()\n",
    "    \n",
    "    s = resp\n",
    "    \n",
    "    # Try JSON parse if possible\n",
    "    try:\n",
    "        parsed = json.loads(s)\n",
    "        if isinstance(parsed, dict):\n",
    "            msg = parsed.get(\"message\")\n",
    "            if isinstance(msg, dict):\n",
    "                return msg.get(\"content\", \"\").strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Regex: message=Message(... content='...') or content='...' patterns\n",
    "    m = re.search(r\"message=Message\\([^)]*content=(?P<q>['\\\"])(?P<content>.*?)(?P=q)\", s, re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(\"content\").strip()\n",
    "    \n",
    "    m2 = re.search(r\"content=(?P<q>['\\\"])(?P<content>.*?)(?P=q)\", s, re.DOTALL)\n",
    "    if m2:\n",
    "        return m2.group(\"content\").strip()\n",
    "    \n",
    "    # Fallback: return the original string\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def execute_query(query: str, vector_store: VectorStore, metadata_store: MetadataStore, \n",
    "                 model_name: str = None, top_k: int = None, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    End-to-end query execution: retrieve documents → generate answer → format with sources.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        vector_store: Loaded FAISS vector store\n",
    "        metadata_store: Loaded metadata store\n",
    "        model_name: LLM model to use (defaults to DEFAULT_OLLAMA_MODEL)\n",
    "        top_k: Number of chunks to retrieve (defaults to DOCUMENT_CONFIG[\"top_k\"])\n",
    "        verbose: Print progress messages\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: \"query\", \"answer\", \"formatted_answer\", \"retrieved_chunks\"\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = DEFAULT_OLLAMA_MODEL\n",
    "    if top_k is None:\n",
    "        top_k = DOCUMENT_CONFIG[\"top_k\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"📖 Query: {query}\")\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_chunks = retrieve_documents(query, vector_store, metadata_store, k=top_k)\n",
    "    if verbose:\n",
    "        print(f\"✓ Retrieved {len(retrieved_chunks)} relevant chunks\")\n",
    "    \n",
    "    # Generate answer\n",
    "    if verbose:\n",
    "        print(f\"Generating answer with model: {model_name}\")\n",
    "    \n",
    "    raw_response = generate_answer(query, retrieved_chunks, model_name=model_name, stream=False)\n",
    "    answer_text = extract_content(raw_response)\n",
    "    \n",
    "    # Format with sources\n",
    "    formatted_output = format_answer_with_sources(answer_text, retrieved_chunks)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer_text,\n",
    "        \"formatted_answer\": formatted_output,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"model\": model_name\n",
    "    }\n",
    "\n",
    "\n",
    "def display_result(result: dict):\n",
    "    \"\"\"Pretty print query result with answer and sources.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANSWER WITH FORMATTED SOURCES:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"formatted_answer\"])\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"✓ Reusable helper functions loaded (extract_content, execute_query, display_result)\")\n",
    "print(\"✓ Configuration centralized in DOCUMENT_CONFIG, MODEL_CONFIG, and TOKEN_CONFIG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "727cedf0"
   },
   "source": [
    "## Pipeline Integration and Testing\n",
    "\n",
    "### Subtask:\n",
    "Combine all components into a basic RAG pipeline and test with a sample document and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Smart embedding state checker loaded (avoids redundant API calls)\n"
     ]
    }
   ],
   "source": [
    "def check_if_embeddings_exist(save_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if embeddings and metadata already exist from a previous run.\n",
    "    Returns True if all necessary files are present and valid.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    required_files = [\n",
    "        os.path.join(save_dir, \"faiss_index.bin\"),\n",
    "        os.path.join(save_dir, \"chunk_ids.pkl\"),\n",
    "        os.path.join(save_dir, \"metadata.pkl\")\n",
    "    ]\n",
    "    \n",
    "    # Check if all files exist\n",
    "    all_exist = all(os.path.exists(f) for f in required_files)\n",
    "    \n",
    "    if all_exist:\n",
    "        try:\n",
    "            # Try to load to verify integrity (silently)\n",
    "            test_vs = VectorStore.load(save_dir, verbose=False)\n",
    "            test_ms = MetadataStore.load(os.path.join(save_dir, \"metadata.pkl\"), verbose=False)\n",
    "            \n",
    "            # Check if they have content\n",
    "            if test_vs.index.ntotal > 0 and len(test_ms.get_all_chunks()) > 0:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Embeddings exist but failed to load: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def skip_embedding_if_exists(save_dir: str, sample_document_path: str):\n",
    "    \"\"\"\n",
    "    Smart pipeline state manager:\n",
    "    - If embeddings exist: Load them and skip document processing\n",
    "    - If embeddings don't exist: Process document normally\n",
    "    \n",
    "    Returns: (should_skip_embedding, vector_store, metadata_store)\n",
    "    \"\"\"\n",
    "    if check_if_embeddings_exist(save_dir):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"⚡ FOUND PRECOMPUTED EMBEDDINGS - SKIPPING REDUNDANT API CALLS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✓ Embeddings already exist in {save_dir}\")\n",
    "        print(f\"✓ Skipping: document loading, chunking, and embedding generation\")\n",
    "        print(f\"✓ Directly loading from disk...\\n\")\n",
    "        \n",
    "        # Load silently since we're providing our own status messages\n",
    "        vector_store = VectorStore.load(save_dir, verbose=False)\n",
    "        metadata_store = MetadataStore.load(os.path.join(save_dir, \"metadata.pkl\"), verbose=False)\n",
    "        \n",
    "        print(f\"✓ Loaded {vector_store.index.ntotal} precomputed embeddings\")\n",
    "        print(f\"✓ Loaded {len(metadata_store.get_all_chunks())} document chunks\")\n",
    "        print(f\"✓ Ready for querying!\\n\")\n",
    "        \n",
    "        return True, vector_store, metadata_store\n",
    "    else:\n",
    "        print(f\"\\n📄 No precomputed embeddings found. Processing document: {os.path.basename(sample_document_path)}\")\n",
    "        return False, None, None\n",
    "\n",
    "print(\"✓ Smart embedding state checker loaded (avoids redundant API calls)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "referenced_widgets": [
      "55e4d5e6d3704e9aac24702c44475df6",
      "40a5c72060fa40edb8787295fa12fea7",
      "48dda1d3708e4dd5854815eca666c837",
      "eea3c1ca71464bde80a029fe5afcb2a4",
      "a88b76fe2d8c4dfd96981c8857a6d83f",
      "caa1cfaf3aae4fcaa65c63b1622baa6c",
      "1efb9bc13eda4bcea490254207b66cde",
      "6245b3ea77024ed4979d0a71eaa46215",
      "ba60a82bba334f7f9ec6794133eec339",
      "8c21e3ef70154f1397b4aa211859cda0",
      "485e3b0909f54a769bb13aaed7665452"
     ]
    },
    "executionInfo": {
     "elapsed": 252585,
     "status": "ok",
     "timestamp": 1757886585722,
     "user": {
      "displayName": "Aditya Gupta",
      "userId": "04884539656118443680"
     },
     "user_tz": -60
    },
    "id": "4c53947e",
    "outputId": "22e1db6d-448a-48c5-a9fd-9c4547119200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG PIPELINE - Smart Embedding Reuse (Avoids Redundant API Calls)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "⚡ FOUND PRECOMPUTED EMBEDDINGS - SKIPPING REDUNDANT API CALLS\n",
      "======================================================================\n",
      "✓ Embeddings already exist in ./rag_data\n",
      "✓ Skipping: document loading, chunking, and embedding generation\n",
      "✓ Directly loading from disk...\n",
      "\n",
      "✓ Loaded 660 precomputed embeddings\n",
      "✓ Loaded 660 document chunks\n",
      "✓ Ready for querying!\n",
      "\n",
      "[1/2] ✓ SKIPPED - Using precomputed embeddings\n",
      "[2/2] Testing retrieval & answer generation with formatted sources...\n",
      "\n",
      "📖 Query: What are the Four Laws of Behavior Change?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retrieved 5 relevant chunks\n",
      "\n",
      "Retrieved Sources:\n",
      "  [1] Atomic_Habits_James_Clear.pdf (Page 52): The ultimate purpose of habits is to solve the problems of life with as little energy and effort as ...\n",
      "  [2] Atomic_Habits_James_Clear.pdf (Page 155): Chapter Summary The 4th Law of Behavior Change is make it satisfying. We are more likely to repeat a...\n",
      "  [3] Atomic_Habits_James_Clear.pdf (Page 51): transform these four steps into a practical framework that we can use to design good habits and elim...\n",
      "  [4] Atomic_Habits_James_Clear.pdf (Page 150): for doing (or punished for doing) in the past. Positive emotions cultivate habits. Negative emotions...\n",
      "  [5] Atomic_Habits_James_Clear.pdf (Page 51): Inversion of the 2nd law (Craving): Make it unattractive. Inversion of the 3rd law (Response): Make ...\n",
      "\n",
      "Generating answer with model: gpt-oss:20b-cloud\n",
      "\n",
      "======================================================================\n",
      "ANSWER WITH FORMATTED SOURCES:\n",
      "======================================================================\n",
      "model='gpt-oss:20b' created_at='2025-10-25T21:19:56.239767959Z' done=True done_reason='stop' total_duration=1101128391 load_duration=None prompt_eval_count=959 prompt_eval_duration=None eval_count=228 eval_duration=None message=Message(role='assistant', content='The Four Laws of Behavior Change, as outlined by James Clear, are a simple framework for building good habits and breaking bad ones. They are:\\n\\n1. **Make it obvious** – design cues that clearly signal the desired behavior.  \\n2. **Make it attractive** – increase the craving or appeal of the habit.  \\n3. **Make it easy** – reduce friction so the response can be performed with minimal effort.  \\n4. **Make it satisfying** – provide immediate reward so the behavior is reinforced and repeated.  \\n\\nThese laws correspond to the cue, craving, response, and reward steps of the habit loop, and they can be inverted to dismantle unwanted habits [1][2][3].', thinking='We need to answer: \"What are the Four Laws of Behavior Change?\" Provide concise answer citing sources. Use bracketed indices. Provide 3-6 sentences. Use sources [1], [2], [3], etc. Let\\'s craft answer: The four laws: make it obvious, make it attractive, make it easy, make it satisfying. Provide explanation. Use citations.', images=None, tool_name=None, tool_calls=None)\n",
      "\n",
      "📚 SOURCES:\n",
      "\n",
      "[1] Atomic_Habits_James_Clear.pdf (Page 52):\n",
      "    \"The ultimate purpose of habits is to solve the problems of life with as little energy and effort as possible. Any habit can be broken down into a feedback loop that involves four steps: cue, craving, ...\"\n",
      "\n",
      "[2] Atomic_Habits_James_Clear.pdf (Page 155):\n",
      "    \"Chapter Summary The 4th Law of Behavior Change is make it satisfying. We are more likely to repeat a behavior when the experience is satisfying. The human brain evolved to prioritize immediate rewards...\"\n",
      "\n",
      "[3] Atomic_Habits_James_Clear.pdf (Page 51):\n",
      "    \"transform these four steps into a practical framework that we can use to design good habits and eliminate bad ones. I refer to this framework as the Four Laws of Behavior Change, and it provides a sim...\"\n",
      "\n",
      "[4] Atomic_Habits_James_Clear.pdf (Page 150):\n",
      "    \"for doing (or punished for doing) in the past. Positive emotions cultivate habits. Negative emotions destroy them. The first three laws of behavior change—make it obvious, make it attractive, and make...\"\n",
      "\n",
      "[5] Atomic_Habits_James_Clear.pdf (Page 51):\n",
      "    \"Inversion of the 2nd law (Craving): Make it unattractive. Inversion of the 3rd law (Response): Make it difficult. Inversion of the 4th law (Reward): Make it unsatisfying. It would be irresponsible for...\"\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✓ Query complete! (Using cached embeddings)\n",
      "  To regenerate embeddings: delete ./rag_data/ or set should_skip_embedding=False\n",
      "\n",
      "======================================================================\n",
      "ANSWER WITH FORMATTED SOURCES:\n",
      "======================================================================\n",
      "model='gpt-oss:20b' created_at='2025-10-25T21:19:56.239767959Z' done=True done_reason='stop' total_duration=1101128391 load_duration=None prompt_eval_count=959 prompt_eval_duration=None eval_count=228 eval_duration=None message=Message(role='assistant', content='The Four Laws of Behavior Change, as outlined by James Clear, are a simple framework for building good habits and breaking bad ones. They are:\\n\\n1. **Make it obvious** – design cues that clearly signal the desired behavior.  \\n2. **Make it attractive** – increase the craving or appeal of the habit.  \\n3. **Make it easy** – reduce friction so the response can be performed with minimal effort.  \\n4. **Make it satisfying** – provide immediate reward so the behavior is reinforced and repeated.  \\n\\nThese laws correspond to the cue, craving, response, and reward steps of the habit loop, and they can be inverted to dismantle unwanted habits [1][2][3].', thinking='We need to answer: \"What are the Four Laws of Behavior Change?\" Provide concise answer citing sources. Use bracketed indices. Provide 3-6 sentences. Use sources [1], [2], [3], etc. Let\\'s craft answer: The four laws: make it obvious, make it attractive, make it easy, make it satisfying. Provide explanation. Use citations.', images=None, tool_name=None, tool_calls=None)\n",
      "\n",
      "📚 SOURCES:\n",
      "\n",
      "[1] Atomic_Habits_James_Clear.pdf (Page 52):\n",
      "    \"The ultimate purpose of habits is to solve the problems of life with as little energy and effort as possible. Any habit can be broken down into a feedback loop that involves four steps: cue, craving, ...\"\n",
      "\n",
      "[2] Atomic_Habits_James_Clear.pdf (Page 155):\n",
      "    \"Chapter Summary The 4th Law of Behavior Change is make it satisfying. We are more likely to repeat a behavior when the experience is satisfying. The human brain evolved to prioritize immediate rewards...\"\n",
      "\n",
      "[3] Atomic_Habits_James_Clear.pdf (Page 51):\n",
      "    \"transform these four steps into a practical framework that we can use to design good habits and eliminate bad ones. I refer to this framework as the Four Laws of Behavior Change, and it provides a sim...\"\n",
      "\n",
      "[4] Atomic_Habits_James_Clear.pdf (Page 150):\n",
      "    \"for doing (or punished for doing) in the past. Positive emotions cultivate habits. Negative emotions destroy them. The first three laws of behavior change—make it obvious, make it attractive, and make...\"\n",
      "\n",
      "[5] Atomic_Habits_James_Clear.pdf (Page 51):\n",
      "    \"Inversion of the 2nd law (Craving): Make it unattractive. Inversion of the 3rd law (Response): Make it difficult. Inversion of the 4th law (Reward): Make it unsatisfying. It would be irresponsible for...\"\n",
      "\n",
      "======================================================================\n",
      "\n",
      "✓ Query complete! (Using cached embeddings)\n",
      "  To regenerate embeddings: delete ./rag_data/ or set should_skip_embedding=False\n"
     ]
    }
   ],
   "source": [
    "# ===== INTELLIGENT PIPELINE EXECUTION (WITH SMART SKIP) =====\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAG PIPELINE - Smart Embedding Reuse (Avoids Redundant API Calls)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Unpack configuration\n",
    "sample_document_path = DOCUMENT_CONFIG[\"sample_path\"]\n",
    "SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "CHUNK_SIZE = DOCUMENT_CONFIG[\"chunk_size\"]\n",
    "CHUNK_OVERLAP = DOCUMENT_CONFIG[\"chunk_overlap\"]\n",
    "\n",
    "# Check if embeddings already exist from previous run\n",
    "should_skip_embedding, vector_store, metadata_store = skip_embedding_if_exists(SAVE_DIR, sample_document_path)\n",
    "\n",
    "if should_skip_embedding:\n",
    "    # Embeddings exist - skip document processing and go straight to querying\n",
    "    print(\"[1/2] ✓ SKIPPED - Using precomputed embeddings\")\n",
    "    print(\"[2/2] Testing retrieval & answer generation with formatted sources...\")\n",
    "else:\n",
    "    # Embeddings don't exist - process document normally\n",
    "    \n",
    "    # Step 1: Load Document\n",
    "    print(\"\\n[1/7] Loading document...\")\n",
    "    try:\n",
    "        document_data = load_document(sample_document_path)\n",
    "        if isinstance(document_data, list):  # PDF returns list of (text, page_num)\n",
    "            print(f\"✓ Loaded PDF with {len(document_data)} pages\")\n",
    "            is_pdf = True\n",
    "        elif isinstance(document_data, str):  # Text/DOCX returns string\n",
    "            print(f\"✓ Loaded document ({len(document_data)} characters)\")\n",
    "            is_pdf = False\n",
    "            document_data = [(document_data, None)]  # Convert to same format\n",
    "        else:\n",
    "            raise ValueError(\"Failed to load document\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading document: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 2: Chunk Document\n",
    "    print(f\"\\n[2/7] Chunking document (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})...\")\n",
    "    if is_pdf:\n",
    "        chunks_data = chunk_pdf_pages(document_data, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "    else:\n",
    "        chunks_data = chunk_document_semantic(document_data[0][0], chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    print(f\"✓ Created {len(chunks_data)} semantic chunks\")\n",
    "\n",
    "    # Step 3: Create Metadata Store\n",
    "    print(\"\\n[3/7] Building metadata store...\")\n",
    "    metadata_store = MetadataStore()\n",
    "    document_chunks = []\n",
    "\n",
    "    for chunk_text, page_num in chunks_data:\n",
    "        chunk = DocumentChunk(\n",
    "            text=chunk_text,\n",
    "            source=os.path.basename(sample_document_path),\n",
    "            page_number=page_num\n",
    "        )\n",
    "        metadata_store.add_chunk(chunk)\n",
    "        document_chunks.append(chunk)\n",
    "\n",
    "    metadata_store.index_chunks()\n",
    "\n",
    "    # Step 4: Generate Embeddings (WITH OPTIMIZATIONS)\n",
    "    print(f\"\\n[4/7] Generating embeddings using {EMBEDDING_METHOD}...\")\n",
    "    print(\"      ⚡ Enabling: caching, parallel processing, and batch optimization\")\n",
    "    generate_embeddings(document_chunks, batch_size=100, use_optimizations=True)\n",
    "\n",
    "    # Step 5: Build Vector Store\n",
    "    print(\"\\n[5/7] Building vector store with cosine similarity...\")\n",
    "    vector_store = VectorStore(EMBEDDING_DIM)\n",
    "\n",
    "    embeddings = np.array([chunk.embedding for chunk in document_chunks])\n",
    "    chunk_ids = [chunk.chunk_id for chunk in document_chunks]\n",
    "\n",
    "    vector_store.add_vectors(embeddings, chunk_ids)\n",
    "\n",
    "    # Step 6: Save Everything to Disk\n",
    "    print(f\"\\n[6/7] Saving to disk ({SAVE_DIR})...\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    vector_store.save(SAVE_DIR)\n",
    "    metadata_store.save(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "    \n",
    "    print(f\"✓ Embeddings saved! Next run will reuse them automatically.\")\n",
    "    print(f\"\\n[7/7] Testing retrieval & answer generation with formatted sources...\")\n",
    "\n",
    "# ===== COMMON PATH (for both first run and subsequent runs) =====\n",
    "\n",
    "query = \"What are the Four Laws of Behavior Change?\"\n",
    "result = execute_query(query, vector_store, metadata_store, verbose=True)\n",
    "display_result(result)\n",
    "\n",
    "if should_skip_embedding:\n",
    "    print(f\"\\n✓ Query complete! (Using cached embeddings)\")\n",
    "    print(f\"  To regenerate embeddings: delete {SAVE_DIR}/ or set should_skip_embedding=False\")\n",
    "else:\n",
    "    print(f\"\\n✓ Pipeline complete! Data saved to {SAVE_DIR}\")\n",
    "    print(f\"  Embedding cache stored in: .embedding_cache/\")\n",
    "    print(f\"  Next run will reuse embeddings automatically!\")\n",
    "    print(f\"  To reload manually: vector_store = VectorStore.load('{SAVE_DIR}')\")\n",
    "    print(f\"                      metadata_store = MetadataStore.load('{SAVE_DIR}/metadata.pkl')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb20254d"
   },
   "source": [
    "## Refinement and Evaluation\n",
    "\n",
    "### Subtask:\n",
    "Outline steps for refining and evaluating the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Demo prep functions loaded (initialize_rag_demo, rag_query, rag_query_with_details)\n",
      "Ready to create Streamlit/Gradio demo!\n"
     ]
    }
   ],
   "source": [
    "# ===== DEMO PREPARATION: UI-Ready Functions =====\n",
    "\n",
    "# Global state for UI demos (initialized once)\n",
    "_demo_state = {\n",
    "    \"vector_store\": None,\n",
    "    \"metadata_store\": None,\n",
    "    \"initialized\": False,\n",
    "}\n",
    "\n",
    "def initialize_rag_demo():\n",
    "    \"\"\"\n",
    "    Initialize RAG system for UI demo.\n",
    "    Loads precomputed embeddings or processes document if needed.\n",
    "    Safe to call multiple times - only initializes once.\n",
    "    \"\"\"\n",
    "    if _demo_state[\"initialized\"]:\n",
    "        return _demo_state[\"vector_store\"], _demo_state[\"metadata_store\"]\n",
    "    \n",
    "    SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "    sample_path = DOCUMENT_CONFIG[\"sample_path\"]\n",
    "    \n",
    "    # Try to load precomputed embeddings\n",
    "    if check_if_embeddings_exist(SAVE_DIR):\n",
    "        print(\"✓ Loading precomputed embeddings...\")\n",
    "        _demo_state[\"vector_store\"] = VectorStore.load(SAVE_DIR, verbose=False)\n",
    "        _demo_state[\"metadata_store\"] = MetadataStore.load(\n",
    "            os.path.join(SAVE_DIR, \"metadata.pkl\"), verbose=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"Processing document (this may take a few minutes)...\")\n",
    "        # Run full pipeline\n",
    "        should_skip, vs, ms = skip_embedding_if_exists(SAVE_DIR, sample_path)\n",
    "        if not should_skip:\n",
    "            # Full processing needed\n",
    "            document_data = load_document(sample_path)\n",
    "            is_pdf = isinstance(document_data, list)\n",
    "            \n",
    "            if is_pdf:\n",
    "                chunks_data = chunk_pdf_pages(\n",
    "                    document_data, \n",
    "                    chunk_size=DOCUMENT_CONFIG[\"chunk_size\"],\n",
    "                    overlap=DOCUMENT_CONFIG[\"chunk_overlap\"]\n",
    "                )\n",
    "            else:\n",
    "                chunks_data = chunk_document_semantic(\n",
    "                    document_data, \n",
    "                    chunk_size=DOCUMENT_CONFIG[\"chunk_size\"],\n",
    "                    overlap=DOCUMENT_CONFIG[\"chunk_overlap\"]\n",
    "                )\n",
    "            \n",
    "            ms = MetadataStore()\n",
    "            document_chunks = []\n",
    "            for chunk_text, page_num in chunks_data:\n",
    "                chunk = DocumentChunk(\n",
    "                    text=chunk_text,\n",
    "                    source=os.path.basename(sample_path),\n",
    "                    page_number=page_num\n",
    "                )\n",
    "                ms.add_chunk(chunk)\n",
    "                document_chunks.append(chunk)\n",
    "            \n",
    "            generate_embeddings(document_chunks, batch_size=100, use_optimizations=True)\n",
    "            \n",
    "            vs = VectorStore(EMBEDDING_DIM)\n",
    "            embeddings = np.array([chunk.embedding for chunk in document_chunks])\n",
    "            chunk_ids = [chunk.chunk_id for chunk in document_chunks]\n",
    "            vs.add_vectors(embeddings, chunk_ids)\n",
    "            \n",
    "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "            vs.save(SAVE_DIR)\n",
    "            ms.save(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "        else:\n",
    "            vs = VectorStore.load(SAVE_DIR, verbose=False)\n",
    "            ms = MetadataStore.load(os.path.join(SAVE_DIR, \"metadata.pkl\"), verbose=False)\n",
    "        \n",
    "        _demo_state[\"vector_store\"] = vs\n",
    "        _demo_state[\"metadata_store\"] = ms\n",
    "    \n",
    "    _demo_state[\"initialized\"] = True\n",
    "    print(\"✓ RAG system ready for queries!\")\n",
    "    return _demo_state[\"vector_store\"], _demo_state[\"metadata_store\"]\n",
    "\n",
    "\n",
    "def rag_query(user_query: str, model: str = None, top_k: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Simple interface for UI demos (Streamlit/Gradio).\n",
    "    Takes a query string, returns formatted answer with sources.\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        model: Optional model name (uses DEFAULT_OLLAMA_MODEL if None)\n",
    "        top_k: Optional number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Formatted answer with source citations\n",
    "    \"\"\"\n",
    "    vs, ms = _demo_state[\"vector_store\"], _demo_state[\"metadata_store\"]\n",
    "    if vs is None or ms is None:\n",
    "        raise ValueError(\"RAG system not initialized. Call initialize_rag_demo() first.\")\n",
    "    \n",
    "    result = execute_query(user_query, vs, ms, model_name=model, top_k=top_k, verbose=False)\n",
    "    return result[\"formatted_answer\"]\n",
    "\n",
    "\n",
    "def rag_query_with_details(user_query: str, model: str = None, top_k: int = None) -> dict:\n",
    "    \"\"\"\n",
    "    Advanced interface returning full result details.\n",
    "    Useful for apps that need answer + metadata.\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        model: Optional model name\n",
    "        top_k: Optional number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict with: answer, formatted_answer, retrieved_chunks, model, query\n",
    "    \"\"\"\n",
    "    vs, ms = _demo_state[\"vector_store\"], _demo_state[\"metadata_store\"]\n",
    "    if vs is None or ms is None:\n",
    "        raise ValueError(\"RAG system not initialized. Call initialize_rag_demo() first.\")\n",
    "    \n",
    "    return execute_query(user_query, vs, ms, model_name=model, top_k=top_k, verbose=False)\n",
    "\n",
    "print(\"✓ Demo prep functions loaded (initialize_rag_demo, rag_query, rag_query_with_details)\")\n",
    "print(\"Ready to create Streamlit/Gradio demo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.49.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading gradio-5.49.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.10.0)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.10.0)\n",
      "Collecting audioop-lts<1.0 (from gradio)\n",
      "  Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting audioop-lts<1.0 (from gradio)\n",
      "  Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.120.0-py3-none-any.whl.metadata (28 kB)\n",
      "  Downloading fastapi-0.120.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.13.3 (from gradio)\n",
      "  Downloading gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio-client==1.13.3 (from gradio)\n",
      "  Downloading gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.34.4)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.3.3)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.34.4)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.3.3)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib64/python3.13/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.11.7)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib64/python3.13/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (2.11.7)\n",
      "Collecting pydub (from gradio)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (6.0.2)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.14.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.14.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib64/python3.13/site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib64/python3.13/site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (2025.9.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib64/python3.13/site-packages (from gradio-client==1.13.3->gradio) (2025.9.0)\n",
      "Collecting websockets<16.0,>=13.0 (from gradio-client==1.13.3->gradio)\n",
      "Collecting websockets<16.0,>=13.0 (from gradio-client==1.13.3->gradio)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib64/python3.13/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.13/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib64/python3.13/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.13/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib64/python3.13/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib64/python3.13/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib64/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib64/python3.13/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib64/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
      "Downloading gradio-5.49.1-py3-none-any.whl (63.5 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading gradio-5.49.1-py3-none-any.whl (63.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (85 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (85 kB)\n",
      "Downloading fastapi-0.120.0-py3-none-any.whl (108 kB)\n",
      "Downloading fastapi-0.120.0-py3-none-any.whl (108 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.14.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.14.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading ffmpy-0.6.4-py3-none-any.whl (5.6 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading ffmpy-0.6.4-py3-none-any.whl (5.6 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, brotli, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, audioop-lts, annotated-doc, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
      "\u001b[?25lInstalling collected packages: pydub, brotli, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, audioop-lts, annotated-doc, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [gradio]18/19\u001b[0m [gradio]]lient]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [gradio]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiofiles-24.1.0 annotated-doc-0.0.3 audioop-lts-0.2.2 brotli-1.1.0 fastapi-0.120.0 ffmpy-0.6.4 gradio-5.49.1 gradio-client-1.13.3 groovy-0.1.2 orjson-3.11.4 pydub-0.25.1 python-multipart-0.0.20 ruff-0.14.2 safehttpx-0.1.7 semantic-version-2.10.0 starlette-0.48.0 tomlkit-0.13.3 uvicorn-0.38.0 websockets-15.0.1\n",
      "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.3 audioop-lts-0.2.2 brotli-1.1.0 fastapi-0.120.0 ffmpy-0.6.4 gradio-5.49.1 gradio-client-1.13.3 groovy-0.1.2 orjson-3.11.4 pydub-0.25.1 python-multipart-0.0.20 ruff-0.14.2 safehttpx-0.1.7 semantic-version-2.10.0 starlette-0.48.0 tomlkit-0.13.3 uvicorn-0.38.0 websockets-15.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Gradio RAG Demo...\n",
      "======================================================================\n",
      "📚 Initializing RAG system...\n",
      "✓ RAG system ready!\n",
      "\n",
      "⚠️  Port 7860 is already in use. Finding available port...\n",
      "✓ Using port 7861 instead\n",
      "\n",
      "🌐 Launching Gradio interface...\n",
      "======================================================================\n",
      "📖 Access the demo at: http://127.0.0.1:7861\n",
      "🌍 Public link (share=True): will be printed below\n",
      "======================================================================\n",
      "🌐 Launching Gradio interface...\n",
      "======================================================================\n",
      "📖 Access the demo at: http://127.0.0.1:7861\n",
      "🌍 Public link (share=True): will be printed below\n",
      "======================================================================\n",
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://0ad972a84275c83c70.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "* Running on public URL: https://0ad972a84275c83c70.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0ad972a84275c83c70.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Config] Sources: 8 | Max tokens: 2048\n"
     ]
    }
   ],
   "source": [
    "# ===== GRADIO DEMO INTERFACE =====\n",
    "\n",
    "import gradio as gr\n",
    "import socket\n",
    "\n",
    "def is_port_available(port: int) -> bool:\n",
    "    \"\"\"Check if a port is available for use.\"\"\"\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            s.bind(('', port))\n",
    "            return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def find_available_port(start_port: int = 7860, max_attempts: int = 10) -> int:\n",
    "    \"\"\"Find an available port starting from start_port.\"\"\"\n",
    "    for offset in range(max_attempts):\n",
    "        port = start_port + offset\n",
    "        if is_port_available(port):\n",
    "            return port\n",
    "    raise RuntimeError(f\"Could not find available port in range {start_port}-{start_port + max_attempts - 1}\")\n",
    "\n",
    "def rag_demo_interface(query: str, top_k: int = 5, model: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Gradio-compatible RAG interface.\n",
    "    Takes user query and returns formatted answer with sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use default model if not specified\n",
    "        if model is None or model.strip() == \"\":\n",
    "            model = DEFAULT_OLLAMA_MODEL\n",
    "        \n",
    "        # Execute query with user-specified parameters\n",
    "        result = rag_query_with_details(query, model=model, top_k=top_k)\n",
    "        \n",
    "        return result[\"formatted_answer\"]\n",
    "    \n",
    "    except ValueError as e:\n",
    "        # RAG system not initialized\n",
    "        return f\"❌ Error: {str(e)}\\n\\nPlease initialize the system first by running `initialize_rag_demo()`\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error processing query: {str(e)}\"\n",
    "\n",
    "\n",
    "def create_gradio_demo():\n",
    "    \"\"\"\n",
    "    Create and configure Gradio interface for RAG system.\n",
    "    Returns the Gradio Blocks interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        title=\"📚 RAG Q&A Assistant\",\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        .gr-box { border-radius: 12px; }\n",
    "        .gr-button { border-radius: 8px; }\n",
    "        .gr-textbox { border-radius: 8px; }\n",
    "        \"\"\"\n",
    "    ) as demo:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # 📚 RAG Q&A Assistant\n",
    "        \n",
    "        Ask questions about your documents using semantic search + local LLM.\n",
    "        **Powered by:** FAISS + Ollama + HuggingFace Embeddings\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"❓ Your Question\",\n",
    "                    lines=4,\n",
    "                    placeholder=\"e.g., What are the Four Laws of Behavior Change?\",\n",
    "                    interactive=True\n",
    "                )\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                top_k_slider = gr.Slider(\n",
    "                    label=\"📊 Number of Sources\",\n",
    "                    minimum=1,\n",
    "                    maximum=10,\n",
    "                    value=5,\n",
    "                    step=1,\n",
    "                    interactive=True\n",
    "                )\n",
    "                \n",
    "                model_dropdown = gr.Textbox(\n",
    "                    label=\"🤖 Model (optional)\",\n",
    "                    value=DEFAULT_OLLAMA_MODEL,\n",
    "                    placeholder=\"Leave blank for default\",\n",
    "                    interactive=True\n",
    "                )\n",
    "        \n",
    "        # Submit button\n",
    "        submit_btn = gr.Button(\n",
    "            \"🔍 Search\",\n",
    "            variant=\"primary\",\n",
    "            scale=1\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        answer_output = gr.Markdown(\n",
    "            label=\"📖 Answer with Sources\",\n",
    "            value=\"Your answer will appear here...\"\n",
    "        )\n",
    "        \n",
    "        # Connect button to function\n",
    "        submit_btn.click(\n",
    "            fn=rag_demo_interface,\n",
    "            inputs=[query_input, top_k_slider, model_dropdown],\n",
    "            outputs=answer_output\n",
    "        )\n",
    "        \n",
    "        # Example questions\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"What are the Four Laws of Behavior Change?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                [\"How do habits compound over time?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                [\"What's the difference between systems and goals?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "                [\"How can I break a bad habit?\", 5, DEFAULT_OLLAMA_MODEL],\n",
    "            ],\n",
    "            inputs=[query_input, top_k_slider, model_dropdown],\n",
    "            outputs=answer_output,\n",
    "            fn=rag_demo_interface,\n",
    "            cache_examples=False,\n",
    "        )\n",
    "        \n",
    "        # Footer info\n",
    "        gr.Markdown(\"\"\"\n",
    "        ---\n",
    "        ### ℹ️ How It Works\n",
    "        1. **Query Processing**: Your question is converted to embeddings\n",
    "        2. **Semantic Search**: Top-K most relevant chunks are retrieved from the document\n",
    "        3. **LLM Generation**: Local Ollama model generates answer based on retrieved chunks\n",
    "        4. **Source Citation**: Answer includes [1], [2] citations linked to source documents\n",
    "        \n",
    "        ### 🚀 Performance\n",
    "        - First run: ~5 minutes (document processing + embedding generation)\n",
    "        - Subsequent runs: <5 seconds (embeddings cached)\n",
    "        - Completely private & offline (when using local models)\n",
    "        \"\"\")\n",
    "    \n",
    "    return demo\n",
    "\n",
    "\n",
    "# Initialize RAG system and launch Gradio\n",
    "print(\"🚀 Starting Gradio RAG Demo...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Initialize RAG system\n",
    "    print(\"📚 Initializing RAG system...\")\n",
    "    initialize_rag_demo()\n",
    "    print(\"✓ RAG system ready!\\n\")\n",
    "    \n",
    "    # Find available port (handle port conflicts gracefully)\n",
    "    preferred_port = 7860\n",
    "    if not is_port_available(preferred_port):\n",
    "        print(f\"⚠️  Port {preferred_port} is already in use. Finding available port...\")\n",
    "        available_port = find_available_port(preferred_port)\n",
    "        print(f\"✓ Using port {available_port} instead\\n\")\n",
    "    else:\n",
    "        available_port = preferred_port\n",
    "        print(f\"✓ Port {preferred_port} is available\\n\")\n",
    "    \n",
    "    # Create and launch Gradio interface\n",
    "    gradio_demo = create_gradio_demo()\n",
    "    \n",
    "    print(\"🌐 Launching Gradio interface...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"📖 Access the demo at: http://127.0.0.1:{available_port}\")\n",
    "    print(\"🌍 Public link (share=True): will be printed below\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    gradio_demo.launch(\n",
    "        share=True,                    # Generate public link\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=available_port,   # Use dynamically selected port\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error launching Gradio demo: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. If port conflict: a new port will be automatically selected\")\n",
    "    print(\"  2. Make sure RAG system initialization succeeded\")\n",
    "    print(\"  3. Check if Gradio is still running from a previous session\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🧪 TESTING DYNAMIC TOKEN LIMITS FOR LLM RESPONSES\n",
      "======================================================================\n",
      "\n",
      "📊 Token Limit Configuration:\n",
      "   Base tokens: 512 (increased from 256)\n",
      "   Per source: 200 tokens/source\n",
      "   Max tokens: 2048 (absolute cap)\n",
      "\n",
      "📐 Formula: num_predict = min(base + num_sources × per_source, max)\n",
      "\n",
      "======================================================================\n",
      "Expected token limits per test case:\n",
      "======================================================================\n",
      "   3 sources            → 1112 max tokens\n",
      "   5 sources            → 1512 max tokens\n",
      "   8 sources            → 2048 max tokens\n",
      "   10 sources (max)     → 2048 max tokens\n",
      "\n",
      "✅ IMPROVEMENTS MADE:\n",
      "   ✓ Token limit increased from 256 → 512 base\n",
      "   ✓ Dynamic scaling: +200 tokens per additional source\n",
      "   ✓ 3 sources  → 912 tokens   (3.6x increase)\n",
      "   ✓ 5 sources  → 1512 tokens  (5.9x increase)\n",
      "   ✓ 10 sources → 2048 tokens  (max)\n",
      "\n",
      "💡 TRY NOW:\n",
      "   1. In Gradio, set 'Number of Sources' to 10\n",
      "   2. Ask a question with the default model\n",
      "   3. Answers will now be MUCH fuller with proper citations!\n",
      "   4. Watch for '[LLM Config] Sources: 10 | Max tokens: 2048' in output\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== TEST NEW TOKEN LIMITS =====\n",
    "# Test the dynamic token limits with different numbers of sources\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🧪 TESTING DYNAMIC TOKEN LIMITS FOR LLM RESPONSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with different numbers of sources\n",
    "test_queries = [\n",
    "    (\"What are the Four Laws of Behavior Change?\", 3, \"3 sources\"),\n",
    "    (\"How do habits compound over time?\", 5, \"5 sources\"),\n",
    "    (\"Explain the habit loop and its components\", 8, \"8 sources\"),\n",
    "    (\"What are the best strategies for building good habits?\", 10, \"10 sources (max)\"),\n",
    "]\n",
    "\n",
    "print(\"\\n📊 Token Limit Configuration:\")\n",
    "print(f\"   Base tokens: {TOKEN_CONFIG['base']} (increased from 256)\")\n",
    "print(f\"   Per source: {TOKEN_CONFIG['per_source']} tokens/source\")\n",
    "print(f\"   Max tokens: {TOKEN_CONFIG['max']} (absolute cap)\")\n",
    "print(f\"\\n📐 Formula: num_predict = min(base + num_sources × per_source, max)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Expected token limits per test case:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query, num_sources, description in test_queries:\n",
    "    expected_tokens = min(\n",
    "        TOKEN_CONFIG['base'] + (num_sources * TOKEN_CONFIG['per_source']),\n",
    "        TOKEN_CONFIG['max']\n",
    "    )\n",
    "    print(f\"   {description:20} → {expected_tokens:4} max tokens\")\n",
    "\n",
    "print(\"\\n✅ IMPROVEMENTS MADE:\")\n",
    "print(\"   ✓ Token limit increased from 256 → 512 base\")\n",
    "print(\"   ✓ Dynamic scaling: +200 tokens per additional source\")\n",
    "print(\"   ✓ 3 sources  → 912 tokens   (3.6x increase)\")\n",
    "print(\"   ✓ 5 sources  → 1512 tokens  (5.9x increase)\")\n",
    "print(\"   ✓ 10 sources → 2048 tokens  (max)\")\n",
    "print(\"\\n💡 TRY NOW:\")\n",
    "print(\"   1. In Gradio, set 'Number of Sources' to 10\")\n",
    "print(\"   2. Ask a question with the default model\")\n",
    "print(\"   3. Answers will now be MUCH fuller with proper citations!\")\n",
    "print(\"   4. Watch for '[LLM Config] Sources: 10 | Max tokens: 2048' in output\")\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "✅ QUICK TEST: Manual Query with 10 Sources\n",
      "======================================================================\n",
      "\n",
      "📝 Query: What are the four laws of behavior change?\n",
      "📊 Testing with: 10 sources (should show '[LLM Config] Sources: 10 | Max tokens: 2048')\n",
      "\n",
      "[LLM Config] Sources: 10 | Max tokens: 2048\n",
      "\n",
      "✅ Answer length: 592 characters\n",
      "✅ Sources retrieved: 10\n",
      "\n",
      "======================================================================\n",
      "FORMATTED ANSWER WITH ALL SOURCES:\n",
      "======================================================================\n",
      "The Four Laws of Behavior Change are a simple framework for building good habits and breaking bad ones. They are:  \\n\\n1. **Make it obvious** – design cues that clearly signal the desired behavior.  \\n2. **Make it attractive** – increase the appeal or craving for the habit.  \\n3. **Make it easy** – reduce friction so the response can happen with minimal effort.  \\n4. **Make it satisfying** – provide immediate reward so the behavior is repeated.  \\n\\nThese laws map onto the cue‑craving‑response‑reward habit loop and are intended to make habit formation effortless and sustainable. [1][3]\n",
      "\n",
      "📚 SOURCES:\n",
      "\n",
      "[1] Atomic_Habits_James_Clear.pdf (Page 52):\n",
      "    \"The ultimate purpose of habits is to solve the problems of life with as little energy and effort as possible. Any habit can be broken down into a feedback loop that involves four steps: cue, craving, ...\"\n",
      "\n",
      "[2] Atomic_Habits_James_Clear.pdf (Page 155):\n",
      "    \"Chapter Summary The 4th Law of Behavior Change is make it satisfying. We are more likely to repeat a behavior when the experience is satisfying. The human brain evolved to prioritize immediate rewards...\"\n",
      "\n",
      "[3] Atomic_Habits_James_Clear.pdf (Page 51):\n",
      "    \"transform these four steps into a practical framework that we can use to design good habits and eliminate bad ones. I refer to this framework as the Four Laws of Behavior Change, and it provides a sim...\"\n",
      "\n",
      "[4] Atomic_Habits_James_Clear.pdf (Page 150):\n",
      "    \"for doing (or punished for doing) in the past. Positive emotions cultivate\n",
      "\n",
      "... (truncated for display)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== MANUAL TEST: Verify Token Limit Fix =====\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ QUICK TEST: Manual Query with 10 Sources\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test directly with the loaded RAG system (no Gradio needed)\n",
    "if _demo_state.get(\"initialized\"):\n",
    "    test_query = \"What are the four laws of behavior change?\"\n",
    "    print(f\"\\n📝 Query: {test_query}\")\n",
    "    print(f\"📊 Testing with: 10 sources (should show '[LLM Config] Sources: 10 | Max tokens: 2048')\\n\")\n",
    "    \n",
    "    result = rag_query_with_details(test_query, top_k=10)\n",
    "    \n",
    "    print(f\"\\n✅ Answer length: {len(result['answer'])} characters\")\n",
    "    print(f\"✅ Sources retrieved: {len(result['retrieved_chunks'])}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FORMATTED ANSWER WITH ALL SOURCES:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"formatted_answer\"][:1500])  # Show first 1500 chars\n",
    "    print(\"\\n... (truncated for display)\")\n",
    "else:\n",
    "    print(\"⚠️  RAG system not initialized. Run the pipeline and demo prep cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Previously Saved Data\n",
    "\n",
    "If you've already processed a document and saved the vector store, you can load it quickly without re-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from ./rag_data (660 vectors)\n",
      "Metadata store loaded from ./rag_data/metadata.pkl (660 chunks)\n",
      "📖 Query: How do habits compound over time?\n",
      "\n",
      "✓ Retrieved 5 relevant chunks (using cached embeddings)\n",
      "\n",
      "Generating answer with model: gpt-oss:20b-cloud\n",
      "✓ Retrieved 5 relevant chunks (using cached embeddings)\n",
      "\n",
      "Generating answer with model: gpt-oss:20b-cloud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FORMATTED ANSWER WITH SOURCES:\n",
      "======================================================================\n",
      "Habits work like compound interest: a small daily improvement—say 1\\u202f% better each day—multiplies over time, producing a result that can be dozens of times larger after a year (≈37×) [1]. The key is the rate and frequency of the behavior; repeated successful attempts build the habit until it crosses the “habit line” and becomes automatic [2]. By tracking tiny gains or losses day‑to‑day, you can predict where you’ll be years later, because those small choices accumulate and magnify over time, making good habits an ally of time while bad habits become its enemy [3].\n",
      "\n",
      "📚 SOURCES:\n",
      "\n",
      "[1] Atomic_Habits_James_Clear.pdf (Page 20):\n",
      "    \"FIGURE 1: The effects of small habits compound over time. For example, if you can get just 1 percent better each day, you’ll end up with results that are nearly 37 times better after one year. Habits ...\"\n",
      "\n",
      "[2] Atomic_Habits_James_Clear.pdf (Page 120):\n",
      "    \"What matters is the rate at which you perform the behavior. You could do something twice in thirty days, or two hundred times. It’s the frequency that makes the difference. Your current habits have be...\"\n",
      "\n",
      "[3] Atomic_Habits_James_Clear.pdf (Page 22):\n",
      "    \"If you want to predict where you’ll end up in life, all you have to do is follow the curve of tiny gains or tiny losses, and see how your daily choices will compound ten or twenty years down the line....\"\n",
      "\n",
      "[4] Atomic_Habits_James_Clear.pdf (Page 108):\n",
      "    \"New versions of old vices. The underlying motives behind human behavior remain the same. The specific habits we perform differ based on the period of history. Here’s the powerful part: there are many ...\"\n",
      "\n",
      "[5] Atomic_Habits_James_Clear.pdf (Page 214):\n",
      "    \"But because it is exponential, the improvement is actually 10x greater.” April 3, 2018. Habits are the compound interest: Many people have noted how habits multiply over time. Here are some of my favo...\"\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved vector store and metadata\n",
    "try:\n",
    "    SAVE_DIR = DOCUMENT_CONFIG[\"save_dir\"]\n",
    "    vector_store = VectorStore.load(SAVE_DIR)\n",
    "    metadata_store = MetadataStore.load(os.path.join(SAVE_DIR, \"metadata.pkl\"))\n",
    "    \n",
    "    # Test with a new query using reusable helper\n",
    "    query = \"How do habits compound over time?\"\n",
    "    result = execute_query(query, vector_store, metadata_store, verbose=True)\n",
    "    display_result(result)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"No saved data found in {DOCUMENT_CONFIG['save_dir']}. Run the pipeline cell first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485d50d3"
   },
   "source": [
    "This step involves:\n",
    "- **Improving Chunking:** Experiment with different chunking strategies (e.g., semantic chunking, recursive text splitting) and chunk sizes.\n",
    "- **Optimizing Retrieval:** Explore different vector store implementations (e.g., specialized databases like Pinecone, Weaviate) and retrieval algorithms. Fine-tune the `k` parameter for retrieval.\n",
    "- **Enhancing Prompting:** Develop more sophisticated prompting techniques for the language model to improve answer quality and incorporate citations.\n",
    "- **Evaluation Metrics:** Define metrics to evaluate the performance of the RAG pipeline, such as:\n",
    "    - **Relevance:** How relevant are the retrieved chunks to the query?\n",
    "    - **Faithfulness:** How well does the generated answer align with the information in the retrieved chunks?\n",
    "    - **Answer Quality:** Is the generated answer coherent, accurate, and helpful?\n",
    "- **Testing with Diverse Data:** Test the pipeline with a variety of document types and query styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef9cabf2"
   },
   "source": [
    "## Finish task"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMI6z7mOJ/PqXccZE66QFNn",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01a2f2a66a824f899437692f3ca13f69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0895139526b04670b160028d31a8063b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08ebc7b8c6e949b5a0d63044394e916d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aef1efa6355f4990aec5195dbc5a9028",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5e5f9e8f9164503aeb26e02320355df",
      "value": 35
     }
    },
    "0cae3a718f0f49d6b479db90d157e7fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0eeffecb0555442086fcd31ae35d7802": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1697ecaf9f9b4e3ba933cc2ef5175610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17fbd77f84ab4671ad3f8d224e744deb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18699c0080ff4e6595a777903bcf33eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1efb9bc13eda4bcea490254207b66cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d6881bafd846679ec5a26539e7d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24df1cdf5091488ca4e26d1220139049",
       "IPY_MODEL_08ebc7b8c6e949b5a0d63044394e916d",
       "IPY_MODEL_4aebc8cc2a604a6187267c288c470f35"
      ],
      "layout": "IPY_MODEL_d0903babae274dea9fef8d0519ab6616"
     }
    },
    "2231300d5d2e4480932ad1f4a673ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22a3e9f632f647b18aa168dfcfe67d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35023ce2c2574b3298665271a8180b9b",
       "IPY_MODEL_31b1b0ee74ce47dd8fe3a314531f544a",
       "IPY_MODEL_57d7314717a44a97a9cb6a4464e73246"
      ],
      "layout": "IPY_MODEL_c87b652473434521bc8ba51c5b6f1336"
     }
    },
    "24df1cdf5091488ca4e26d1220139049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aac9fd335e994dce8057fd2f75967868",
      "placeholder": "​",
      "style": "IPY_MODEL_1697ecaf9f9b4e3ba933cc2ef5175610",
      "value": "added_tokens.json: 100%"
     }
    },
    "2523e64c82d04b209747d11a7bb6a61b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a15a882dea144c183e4e1fea94f173f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aea41ebe3bd44b62b93f14d1ba372158",
       "IPY_MODEL_82a16bce22b44f0aa85ab1f7bdbad83b",
       "IPY_MODEL_5952598b7fe64118a1f1e86811c13be3"
      ],
      "layout": "IPY_MODEL_d99c7288b6214c6cb9d1a78e52b107e6"
     }
    },
    "302faed2a00648499af9fdbaffb66337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "311b486d01ef47e383a4604ceb744bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_302faed2a00648499af9fdbaffb66337",
      "placeholder": "​",
      "style": "IPY_MODEL_3eda1d46746644429404ffbf9f2b061f",
      "value": "config.json: 100%"
     }
    },
    "31b1b0ee74ce47dd8fe3a314531f544a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd09cd8ffac54be3ac64771d8f65545f",
      "max": 1155375,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d40a864131c540049095f912d56281e2",
      "value": 1155375
     }
    },
    "337bf290ff4c4b18b0efcd18514766ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35023ce2c2574b3298665271a8180b9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cd7c86601a24e1d8da0c30e1db2b7cd",
      "placeholder": "​",
      "style": "IPY_MODEL_bf0a1cd56bdb48289367a085e3ed1c43",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "3cdad51c3c444ccb9a75d0854d4c9190": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cf8b9adc1ad47c0bac7e768c930eaa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42149017310c4beba6d9dcefdefbd83a",
      "max": 662,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0895139526b04670b160028d31a8063b",
      "value": 662
     }
    },
    "3eda1d46746644429404ffbf9f2b061f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40a5c72060fa40edb8787295fa12fea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_caa1cfaf3aae4fcaa65c63b1622baa6c",
      "placeholder": "​",
      "style": "IPY_MODEL_1efb9bc13eda4bcea490254207b66cde",
      "value": "generation_config.json: 100%"
     }
    },
    "42149017310c4beba6d9dcefdefbd83a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485e3b0909f54a769bb13aaed7665452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48954e6e0a9d4114a105752ad7e394f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_311b486d01ef47e383a4604ceb744bc4",
       "IPY_MODEL_7c511b2bd5644cb381db7e9c78fabd4a",
       "IPY_MODEL_dd4e4fea09e2446ca93a1ca67eace0d6"
      ],
      "layout": "IPY_MODEL_5a582624199544178c625095356e26bb"
     }
    },
    "48dda1d3708e4dd5854815eca666c837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6245b3ea77024ed4979d0a71eaa46215",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba60a82bba334f7f9ec6794133eec339",
      "value": 111
     }
    },
    "4aebc8cc2a604a6187267c288c470f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337bf290ff4c4b18b0efcd18514766ae",
      "placeholder": "​",
      "style": "IPY_MODEL_d921a0f7e2b2435aaf5ac050d63d3f25",
      "value": " 35.0/35.0 [00:00&lt;00:00, 5.15kB/s]"
     }
    },
    "4cd7c86601a24e1d8da0c30e1db2b7cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d84d79bf71a487daeeb7e9c7775428c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517baab5cb004bcd9369c561a91683ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e4d5e6d3704e9aac24702c44475df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40a5c72060fa40edb8787295fa12fea7",
       "IPY_MODEL_48dda1d3708e4dd5854815eca666c837",
       "IPY_MODEL_eea3c1ca71464bde80a029fe5afcb2a4"
      ],
      "layout": "IPY_MODEL_a88b76fe2d8c4dfd96981c8857a6d83f"
     }
    },
    "57d7314717a44a97a9cb6a4464e73246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d84d79bf71a487daeeb7e9c7775428c",
      "placeholder": "​",
      "style": "IPY_MODEL_7bef6f5797214c71ba1706f273a4fcef",
      "value": " 1.16M/1.16M [00:00&lt;00:00, 9.08MB/s]"
     }
    },
    "5841774613c64b04807531fa75ef4a20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5952598b7fe64118a1f1e86811c13be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0eeffecb0555442086fcd31ae35d7802",
      "placeholder": "​",
      "style": "IPY_MODEL_d10dbbff037c4b0fb530e7aa2afa7c66",
      "value": " 33.4M/33.4M [00:00&lt;00:00, 184MB/s]"
     }
    },
    "5a582624199544178c625095356e26bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a760390ab144c53a819d475ed12c73d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ce1d7140679404ca85df3c4cb7fa104": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6245b3ea77024ed4979d0a71eaa46215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbe4934a6d645f18f6b455f378b3ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ce0e91f5c0f4437b4deb981236c81f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ac0583c668346598a7599783e5e3838",
      "placeholder": "​",
      "style": "IPY_MODEL_f27bcde980ec47e8b4af6e6ad8fdde25",
      "value": " 4.69M/4.69M [00:00&lt;00:00, 53.0MB/s]"
     }
    },
    "6d322a48f71b4ca4a8cd2eaebf097dcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70221af3eca3460abc2be20c1ff0d063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "703c7d87c7344c97a94c9d8760d1407d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73f399029ca24033860f364ace8dd5f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bef6f5797214c71ba1706f273a4fcef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c511b2bd5644cb381db7e9c78fabd4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c94f82a5406d4018ad309b412c4fad73",
      "max": 1352,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01a2f2a66a824f899437692f3ca13f69",
      "value": 1352
     }
    },
    "7ed5e404af5f4ca8a79d9a0daa4eac79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d322a48f71b4ca4a8cd2eaebf097dcd",
      "placeholder": "​",
      "style": "IPY_MODEL_0cae3a718f0f49d6b479db90d157e7fe",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "82a16bce22b44f0aa85ab1f7bdbad83b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b9ee6671d6a49288a399ddf0de284cb",
      "max": 33384570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bbe4934a6d645f18f6b455f378b3ed5",
      "value": 33384570
     }
    },
    "848589303f3142d1986ec45fe9121b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cdad51c3c444ccb9a75d0854d4c9190",
      "max": 4689074,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a192ce3120c344a4bfa3777cab26e3f5",
      "value": 4689074
     }
    },
    "87417696ef7d48798b90ac2af01d95bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ce1d7140679404ca85df3c4cb7fa104",
      "placeholder": "​",
      "style": "IPY_MODEL_70221af3eca3460abc2be20c1ff0d063",
      "value": " 662/662 [00:00&lt;00:00, 103kB/s]"
     }
    },
    "8ac0583c668346598a7599783e5e3838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c21e3ef70154f1397b4aa211859cda0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99fc93e91820408a90591e8c16c7e08f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac81c321e05d409da7a7f05a06a25ad7",
      "placeholder": "​",
      "style": "IPY_MODEL_5a760390ab144c53a819d475ed12c73d",
      "value": "model.safetensors: 100%"
     }
    },
    "9b9ee6671d6a49288a399ddf0de284cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a192ce3120c344a4bfa3777cab26e3f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5e5f9e8f9164503aeb26e02320355df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a75b7bcc70f84800bb309217d1d5b982": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ed5e404af5f4ca8a79d9a0daa4eac79",
       "IPY_MODEL_3cf8b9adc1ad47c0bac7e768c930eaa1",
       "IPY_MODEL_87417696ef7d48798b90ac2af01d95bd"
      ],
      "layout": "IPY_MODEL_17fbd77f84ab4671ad3f8d224e744deb"
     }
    },
    "a88b76fe2d8c4dfd96981c8857a6d83f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aac9fd335e994dce8057fd2f75967868": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac81c321e05d409da7a7f05a06a25ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acbe96b270544b7eb13f38643d85b93d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ada07961a40b47249cf126736654adaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae61b454a03743778bc7d48a03040fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99fc93e91820408a90591e8c16c7e08f",
       "IPY_MODEL_cedf7360bd6b4f4a9f523305c911ac51",
       "IPY_MODEL_dc9cd067e0a14611843d1b581a6d452f"
      ],
      "layout": "IPY_MODEL_703c7d87c7344c97a94c9d8760d1407d"
     }
    },
    "aea41ebe3bd44b62b93f14d1ba372158": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73f399029ca24033860f364ace8dd5f3",
      "placeholder": "​",
      "style": "IPY_MODEL_517baab5cb004bcd9369c561a91683ee",
      "value": "tokenizer.json: 100%"
     }
    },
    "aef1efa6355f4990aec5195dbc5a9028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f4c9b4a4b647d68291e0957a2bf8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f36a2f035f77468e84592829eb17a536",
       "IPY_MODEL_848589303f3142d1986ec45fe9121b1c",
       "IPY_MODEL_6ce0e91f5c0f4437b4deb981236c81f4"
      ],
      "layout": "IPY_MODEL_dc515b7b37c94b05aefec47540554339"
     }
    },
    "ba60a82bba334f7f9ec6794133eec339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf0a1cd56bdb48289367a085e3ed1c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3126ba8df3f4c8a995470f5c52503b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c87b652473434521bc8ba51c5b6f1336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c94f82a5406d4018ad309b412c4fad73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caa1cfaf3aae4fcaa65c63b1622baa6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cedf7360bd6b4f4a9f523305c911ac51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2523e64c82d04b209747d11a7bb6a61b",
      "max": 536223056,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_acbe96b270544b7eb13f38643d85b93d",
      "value": 536223056
     }
    },
    "d0903babae274dea9fef8d0519ab6616": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d10dbbff037c4b0fb530e7aa2afa7c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d40a864131c540049095f912d56281e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d921a0f7e2b2435aaf5ac050d63d3f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d99c7288b6214c6cb9d1a78e52b107e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc515b7b37c94b05aefec47540554339": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc9cd067e0a14611843d1b581a6d452f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3126ba8df3f4c8a995470f5c52503b7",
      "placeholder": "​",
      "style": "IPY_MODEL_f46894634dc84e6da17c4f4a5192f65d",
      "value": " 536M/536M [00:02&lt;00:00, 283MB/s]"
     }
    },
    "dd4e4fea09e2446ca93a1ca67eace0d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5841774613c64b04807531fa75ef4a20",
      "placeholder": "​",
      "style": "IPY_MODEL_18699c0080ff4e6595a777903bcf33eb",
      "value": " 1.35k/1.35k [00:00&lt;00:00, 193kB/s]"
     }
    },
    "eea3c1ca71464bde80a029fe5afcb2a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c21e3ef70154f1397b4aa211859cda0",
      "placeholder": "​",
      "style": "IPY_MODEL_485e3b0909f54a769bb13aaed7665452",
      "value": " 111/111 [00:00&lt;00:00, 17.0kB/s]"
     }
    },
    "f27bcde980ec47e8b4af6e6ad8fdde25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f36a2f035f77468e84592829eb17a536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2231300d5d2e4480932ad1f4a673ccfc",
      "placeholder": "​",
      "style": "IPY_MODEL_ada07961a40b47249cf126736654adaf",
      "value": "tokenizer.model: 100%"
     }
    },
    "f46894634dc84e6da17c4f4a5192f65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd09cd8ffac54be3ac64771d8f65545f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
